# æ–‡ä»¶å: Run_AirCombatEnv_train_vectorized.py

import torch
import numpy as np
import random
import time
from gymnasium.vector import AsyncVectorEnv
from torch.utils.tensorboard import SummaryWriter

# --- å¯¼å…¥æ‚¨çš„è‡ªå®šä¹‰æ¨¡å— ---
# ç¡®ä¿è¿™é‡Œçš„è·¯å¾„æ˜¯æ­£ç¡®çš„
from Interference_code.PPO_model.æ—§æ–‡ä»¶.Hybrid_PPO_jsbsim_Vec import PPO_continuous
from Interference_code.PPO_model.æ—§æ–‡ä»¶.Config_Vec import AGENTPARA
from Interference_code.env.missile_evasion_environment_jsbsim.Vec_missile_evasion_environment_jsbsim import AirCombatEnv

# ========================= é…ç½®åŒº =========================
LOAD_ABLE = False  # æ˜¯å¦ä» 'save/' æ–‡ä»¶å¤¹åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
TACVIEW_ENABLED_DURING_TRAINING = False  # æ˜¯å¦ä¸ºç¬¬ä¸€ä¸ªå¹¶è¡Œç¯å¢ƒå¼€å¯Tacview
# åªæœ‰åœ¨è¯„ä¼°æ—¶æ‰å¯èƒ½å¼€å¯Tacviewï¼Œè®­ç»ƒæ—¶å…³é—­ä»¥æé«˜æ•ˆç‡
TACVIEW_ENABLED_DURING_EVAL = False
NUM_ENVS = 4  # <<<--- å¹¶è¡Œç¯å¢ƒçš„æ•°é‡ ---<<<
RANDOM_SEED = AGENTPARA.RANDOM_SEED
COLLECTION_EPISODES = 10  # æ¯æ”¶é›†10ä¸ªå›åˆçš„æ•°æ®åï¼Œè¿›è¡Œä¸€æ¬¡å­¦ä¹ 
EVALUATION_EPISODES = 1  # æ¯æ¬¡è¯„ä¼°æ—¶ï¼Œè¿è¡Œ5ä¸ªå›åˆæ¥è®¡ç®—å¹³å‡å¥–åŠ±
UPDATE_INTERVAL = 2048   # æ¯æ”¶é›† 2048 ä¸ª step åæ›´æ–°ä¸€æ¬¡ (å¯è°ƒ)

# ========================= è¾…åŠ©å‡½æ•° =========================
def set_seed(seed=RANDOM_SEED):
    """ è®¾ç½®æ‰€æœ‰ç›¸å…³åº“çš„éšæœºç§å­ """
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    # å¦‚æœä½¿ç”¨CUDAï¼Œä¹Ÿè®¾ç½®CUDAçš„ç§å­
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)


def make_env(rank: int, seed: int = 0, tacview_enabled: bool = False):
    """
    åˆ›å»ºå•ä¸ªç¯å¢ƒå®ä¾‹çš„è¾…åŠ©å‡½æ•°ï¼Œç”¨äºçŸ¢é‡åŒ–ã€‚
    """

    def _init():
        # åªä¸ºç¬¬ä¸€ä¸ªç¯å¢ƒ (rank=0) å¯ç”¨ Tacview ä»¥ä¾¿è§‚å¯Ÿ  # è®­ç»ƒæ—¶é€šå¸¸å…³é—­Tacviewä»¥è·å¾—æœ€å¤§é€Ÿåº¦
        is_tacview_on = tacview_enabled and (rank == 0)
        env = AirCombatEnv(tacview_enabled=is_tacview_on)
        # ä¸ºæ¯ä¸ªç¯å¢ƒè®¾ç½®ä¸åŒçš„éšæœºç§å­
        # æ³¨æ„: gym.Env çš„ reset ç°åœ¨æ¥å— seed å‚æ•°
        env.reset(seed=seed + rank)
        return env

    return _init


# ========================= ä¸»æ‰§è¡Œé€»è¾‘ =========================
if __name__ == "__main__":
    # --- 1. åˆå§‹åŒ– ---
    set_seed(RANDOM_SEED)

    # åˆå§‹åŒ– TensorBoard writer
    log_dir = f'log/Vec_seed{RANDOM_SEED}_time_{time.strftime("%m_%d_%H_%M_%S")}_load_{LOAD_ABLE}'
    writer = SummaryWriter(log_dir=log_dir)
    print(f"TensorBoard æ—¥å¿—å°†ä¿å­˜åœ¨: {log_dir}")

    # --- 2. åˆ›å»ºçŸ¢é‡åŒ–ç¯å¢ƒ ---
    print(f"æ­£åœ¨åˆ›å»º {NUM_ENVS} ä¸ªå¹¶è¡Œç¯å¢ƒ...")
    env_fns = [make_env(i, seed=RANDOM_SEED, tacview_enabled=TACVIEW_ENABLED_DURING_TRAINING) for i in range(NUM_ENVS)]
    vec_env = AsyncVectorEnv(env_fns)
    # vec_env = SyncVectorEnv(env_fns,autoreset_mode = AutoresetMode.DISABLED)

    print("çŸ¢é‡åŒ–ç¯å¢ƒåˆ›å»ºæˆåŠŸï¼")

    # --- 3. åˆå§‹åŒ–æ™ºèƒ½ä½“ ---
    agent = PPO_continuous(LOAD_ABLE)

    # --- 4. ä¸»è®­ç»ƒå¾ªç¯ ---
    global_step = 0
    episodes_collected = 0
    steps_collected = 0  # âœ… ç”¨æ­¥æ•°æ¥æ§åˆ¶æ›´æ–°
    total_episodes_trained = 0

    # ç”¨äºç»Ÿè®¡æˆåŠŸç‡
    success_num = 0
    total_completed_episodes = 0

    # åˆå§‹åŒ–è§‚æµ‹
    observations, infos = vec_env.reset()

    while total_episodes_trained < 100000:

        # --- 4.1 æ•°æ®æ”¶é›†é˜¶æ®µ ---
        agent.prep_eval_rl()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼è¿›è¡Œæ•°æ®æ”¶é›†

        # æ”¶é›†ç›´åˆ° Buffer æ»¡æˆ–è€…è¾¾åˆ°æŒ‡å®šçš„å›åˆæ•°
        # æ³¨æ„ï¼šåœ¨çŸ¢é‡åŒ–ç¯å¢ƒä¸­ï¼Œæˆ‘ä»¬é€šå¸¸æŒ‰æ­¥æ•°æ”¶é›†ï¼Œè€Œä¸æ˜¯å›åˆæ•°
        # ä½†ä¸ºäº†ä¿ç•™æ‚¨çš„é€»è¾‘ï¼Œæˆ‘ä»¬ä»ç„¶æŒ‰å›åˆæ•°æ¥è§¦å‘å­¦ä¹ 

        episode_rewards = [0] * NUM_ENVS  # è®°å½•æ¯ä¸ªå¹¶è¡Œç¯å¢ƒçš„å½“å‰å›åˆå¥–åŠ±

        # while episodes_collected < COLLECTION_EPISODES:
        # âœ… æŒç»­æ”¶é›†ï¼Œç›´åˆ°ç´¯è®¡çš„ç¯å¢ƒæ€»æ­¥æ•°è¾¾åˆ° UPDATE_INTERVAL
        while steps_collected < UPDATE_INTERVAL:
            with torch.no_grad():
                # Agent æ ¹æ®æ‰¹é‡è§‚æµ‹é€‰æ‹©åŠ¨ä½œ
                env_actions, actions_to_store, log_probs = agent.choose_action(observations, deterministic=False)
                # è·å–æ‰¹é‡ä»·å€¼ä¼°è®¡
                values = agent.get_value(observations).cpu().detach().numpy().flatten()

            # åœ¨çŸ¢é‡åŒ–ç¯å¢ƒä¸­æ‰§è¡ŒåŠ¨ä½œ
            next_observations, rewards, terminateds, truncateds, infos = vec_env.step(env_actions)
            # print("terminateds:", terminateds)
            # print("truncateds:", truncateds)
            # print("infos:", infos)

            # å­˜å‚¨ç»éªŒ
            for i in range(NUM_ENVS):
                agent.buffer.store_transition(
                    state=observations[i],
                    value=values[i],
                    action=actions_to_store[i],
                    probs=log_probs[i],
                    reward=rewards[i],
                    done=(terminateds[i] or truncateds[i])
                )
                episode_rewards[i] += rewards[i]

            global_step += NUM_ENVS  # æ¯æ¬¡ stepï¼Œæ€»æ­¥æ•°å¢åŠ  NUM_ENVS
            steps_collected += NUM_ENVS  # æœ¬è½®ç´¯è®¡æ­¥æ•°
            observations = next_observations

            # æ£€æŸ¥æ˜¯å¦æœ‰ç¯å¢ƒç»“æŸ
            # æ£€æŸ¥å“ªäº›ç¯å¢ƒç»“æŸ
            dones = terminateds | truncateds
            # print('dones:', dones)
            if np.any(dones):
                # # æ‰¾åˆ°ç»“æŸçš„ç¯å¢ƒç´¢å¼•
                # dones_idx = np.where(dones)[0]
                # # å‡è®¾ dones_idx æ˜¯éœ€è¦é‡ç½®çš„ç¯å¢ƒç´¢å¼•
                # reset_mask = np.zeros(vec_env.num_envs, dtype=bool)
                # reset_mask[dones_idx] = True

                for i in np.where(dones)[0]:
                    # é»˜è®¤ success=False
                    success_flag = False

                    # å®‰å…¨è¯»å– success
                    if 'success' in infos:
                        success_flag = infos['success'][i]

                    # ç´¯è®¡æˆåŠŸæ¬¡æ•°
                    if success_flag:
                        success_num += 1

                    episodes_collected += 1
                    total_episodes_trained += 1
                    total_completed_episodes += 1

                    print(f"Episode {total_episodes_trained} (Env {i}) finished. "
                          f"Reward: {episode_rewards[i]:.2f}  Success: {success_flag}")

                    # é‡ç½®å½“å‰ç¯å¢ƒå¥–åŠ±
                    episode_rewards[i] = 0
#                 # ğŸ”‘ æ‰‹åŠ¨ reset å·²ç»“æŸçš„ç¯å¢ƒï¼Œå¹¶æ›´æ–° observations
#                 reset_obs, reset_infos = vec_env.reset(options={"reset_mask": reset_mask}
# )
#                 # reset_obs shape: (len(dones_idx), obs_dim)
#                 # æ›´æ–°å¯¹åº”ä½ç½®çš„è§‚æµ‹
#                 observations[dones_idx] = reset_obs

        # --- 4.2 å­¦ä¹ é˜¶æ®µ ---
        # print(f"\n--- æ”¶é›†äº† {episodes_collected} ä¸ªå›åˆ, å¼€å§‹å­¦ä¹ . Global Step: {global_step} ---")
        # âœ… å›ºå®šæ­¥æ•°æ”¶é›†å®Œæˆ -> è®­ç»ƒä¸€æ¬¡
        print(f"\n--- æ”¶é›† {steps_collected} æ­¥, å¼€å§‹å­¦ä¹ . Global Step: {global_step} ---")
        agent.prep_training_rl()
        train_info = agent.learn()

        # è®°å½•è®­ç»ƒæ—¥å¿—
        for key, value in train_info.items():
            writer.add_scalar(f"train/{key}", value, global_step=global_step)

        episodes_collected = 0  # é‡ç½®æ”¶é›†è®¡æ•°å™¨
        steps_collected = 0  # é‡ç½®ç´¯è®¡æ­¥æ•°è®¡æ•°å™¨

        # --- 4.3 è¯„ä¼°é˜¶æ®µ (ä½¿ç”¨å•ä¸ªç¯å¢ƒ) ---
        print(f"--- å¼€å§‹è¯„ä¼° (å•ä¸ªå›åˆ) ---")

        # åˆ›å»ºä¸€ä¸ªä¸“ç”¨äºè¯„ä¼°çš„å•ç¯å¢ƒå®ä¾‹
        eval_env = AirCombatEnv(tacview_enabled=TACVIEW_ENABLED_DURING_EVAL)

        agent.prep_eval_rl()
        with torch.no_grad():
            eval_obs, _ = eval_env.reset(seed=RANDOM_SEED + 1000)  # ä½¿ç”¨ä¸åŒçš„ç§å­è¿›è¡Œè¯„ä¼°
            eval_done = False
            eval_reward_sum = 0.0

            # å•ç¯å¢ƒè¯„ä¼°å¾ªç¯
            while not eval_done:
                # choose_action ç°åœ¨æ¥æ”¶å•ä¸ªè§‚æµ‹ (éœ€è¦ unsqueeze æ·»åŠ  batch ç»´åº¦)
                # action è¿”å›çš„æ˜¯æ‰¹é‡åŠ¨ä½œï¼Œéœ€è¦å–ç¬¬ä¸€ä¸ª [0]
                eval_action, _, _ = agent.choose_action(eval_obs, deterministic=True)

                eval_obs, eval_reward, eval_terminated, eval_truncated, _ = eval_env.step(eval_action)

                eval_reward_sum += eval_reward
                eval_done = eval_terminated or eval_truncated

        writer.add_scalar('reward_sum', eval_reward_sum, global_step=global_step)
        print(f"--- è¯„ä¼°å®Œæˆ. å•å›åˆå¥–åŠ±: {eval_reward_sum:.2f} ---")

        # å…³é—­è¯„ä¼°ç¯å¢ƒï¼Œé‡Šæ”¾èµ„æº
        eval_env.close()


        # --- 4.4 æ£€æŸ¥æˆåŠŸç‡å’Œä¿å­˜æ¨¡å‹ ---
        if total_completed_episodes >= 100:
            success_rate = (success_num / total_completed_episodes) * 100
            print(f"--- è¿‡å» {total_completed_episodes} å›åˆçš„æˆåŠŸç‡: {success_rate:.2f}% (æˆåŠŸ {success_num} æ¬¡) ---")
            writer.add_scalar('success_num', success_rate, global_step=global_step)

            if success_rate >= 90:
                print(f"*** æˆåŠŸç‡è¾¾åˆ° {success_rate:.2f}%, ä¿å­˜æ¨¡å‹! ***")
                agent.save(f"success_{int(success_rate)}_ep{total_episodes_trained}")

            # é‡ç½®æˆåŠŸç‡è®¡æ•°å™¨
            success_num = 0
            total_completed_episodes = 0

    # --- 5. ç»“æŸè®­ç»ƒ ---
    vec_env.close()
    writer.close()
    print("è®­ç»ƒç»“æŸã€‚")





# # --- 4.3 è¯„ä¼°é˜¶æ®µ (ä¿®æ­£ç‰ˆ) ---
#         print(f"--- å¼€å§‹è¯„ä¼° (ç›®æ ‡: {EVALUATION_EPISODES * NUM_ENVS} ä¸ªå®Œæ•´å›åˆ) ---")
#         agent.prep_eval_rl()
#
#         # è¿™ä¸ªåˆ—è¡¨å°†å­˜å‚¨æ‰€æœ‰å®Œæˆçš„å›åˆçš„æ€»å¥–åŠ±
#         completed_episode_rewards = []
#
#         # ç›®æ ‡æ˜¯æ”¶é›†åˆ°è¶³å¤Ÿå¤šçš„å›åˆæ•°æ®
#         target_episodes = EVALUATION_EPISODES
#
#         with torch.no_grad():
#             eval_obs, _ = vec_env.reset()
#             # è®°å½•æ¯ä¸ªå¹¶è¡Œç¯å¢ƒå½“å‰æ­£åœ¨è¿›è¡Œçš„å›åˆçš„å¥–åŠ±
#             current_episode_rewards = np.zeros(NUM_ENVS)
#
#             # å¾ªç¯ç›´åˆ°æ”¶é›†åˆ°è¶³å¤Ÿçš„å›åˆ
#             while len(completed_episode_rewards) < target_episodes:
#                 # 1. é€‰æ‹©åŠ¨ä½œ
#                 eval_actions, _, _ = agent.choose_action(eval_obs, deterministic=True)
#
#                 # 2. ä¸ç¯å¢ƒäº¤äº’
#                 next_eval_obs, eval_rewards_step, eval_terminateds, eval_truncateds, infos = vec_env.step(eval_actions)
#
#                 # 3. ç´¯åŠ å½“å‰å›åˆçš„å¥–åŠ±
#                 current_episode_rewards += eval_rewards_step
#
#                 # 4. æ›´æ–°è§‚æµ‹
#                 eval_obs = next_eval_obs
#
#                 # 5. æ£€æŸ¥æ˜¯å¦æœ‰ç¯å¢ƒç»“æŸ
#                 dones = eval_terminateds | eval_truncateds
#                 if np.any(dones):
#                     for i in range(NUM_ENVS):
#                         if dones[i]:
#                             # å¦‚æœä¸€ä¸ªç¯å¢ƒç»“æŸäº†ï¼Œå°†å…¶æ€»å¥–åŠ±å­˜å…¥åˆ—è¡¨
#                             completed_episode_rewards.append(current_episode_rewards[i])
#                             # å¹¶ä¸”é‡ç½®è¿™ä¸ªç¯å¢ƒçš„å½“å‰å¥–åŠ±è®¡æ•°å™¨
#                             current_episode_rewards[i] = 0
#
#         # è®¡ç®—å¹³å‡å¥–åŠ±
#         if len(completed_episode_rewards) > 0:
#             avg_eval_reward = np.mean(completed_episode_rewards)
#         else:
#             avg_eval_reward = 0.0  # å¦‚æœä¸€ä¸ªå›åˆéƒ½æ²¡å®Œæˆï¼ˆä¸å¤ªå¯èƒ½ï¼‰
#
#         writer.add_scalar('eval/reward_sum', avg_eval_reward, global_step=global_step)
#         print(f"--- è¯„ä¼°å®Œæˆ. åœ¨ {len(completed_episode_rewards)} ä¸ªå›åˆä¸­, å¹³å‡å¥–åŠ±: {avg_eval_reward:.2f} ---")