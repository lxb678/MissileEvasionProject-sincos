# æ–‡ä»¶å: Hybrid_PPO_PostAttentionGRU.py
# æè¿°:
#      æ¶æ„ä¿®æ”¹ä¸º: [Missile/Air Encoder] -> [Cross Attention] -> [Feature Fusion] -> [Global GRU] -> [MLP]
#      GRU ç°åœ¨ä½äºæ³¨æ„åŠ›å±‚ä¹‹åï¼Œç”¨äºå¤„ç†èåˆäº†å¨èƒä¿¡æ¯çš„å…¨å±€ä¸Šä¸‹æ–‡åºåˆ—ã€‚

import torch
from torch import nn
from torch.nn import *
import torch.nn.functional as F
from torch.distributions import Bernoulli, Categorical, Normal
# å¯¼å…¥é…ç½®æ–‡ä»¶
from Interference_code.PPO_model.PPO_evasion_fuzaä¸¤ä¸ªå¯¼å¼¹.ConfigAttn import *
from Interference_code.PPO_model.PPO_evasion_fuzaä¸¤ä¸ªå¯¼å¼¹.BufferGRUAttnå®ä½“ import Buffer
from torch.optim import lr_scheduler
import numpy as np
import os
import time

# --- åŠ¨ä½œç©ºé—´é…ç½® (ä¿æŒä¸å˜) ---
CONTINUOUS_DIM = 4
CONTINUOUS_ACTION_KEYS = ['throttle', 'elevator', 'aileron', 'rudder']
DISCRETE_DIMS = {
    'flare_trigger': 1,
    'salvo_size': 3,
    'num_groups': 3,
    'inter_interval': 3,
}
TOTAL_DISCRETE_LOGITS = sum(DISCRETE_DIMS.values())
TOTAL_ACTION_DIM_BUFFER = CONTINUOUS_DIM + len(DISCRETE_DIMS)
DISCRETE_ACTION_MAP = {
    'salvo_size': [2, 3, 4],
    'num_groups': [2, 3, 4],
    'inter_interval': [0.2, 0.4, 0.6]
}
ACTION_RANGES = {
    'throttle': {'low': 0.0, 'high': 1.0},
    'elevator': {'low': -1.0, 'high': 1.0},
    'aileron': {'low': -1.0, 'high': 1.0},
    'rudder': {'low': -1.0, 'high': 1.0},
}

# --- å®ä½“æ³¨æ„åŠ›é…ç½® (ä¿æŒä¸å˜) ---
NUM_MISSILES = 2
MISSILE_FEAT_DIM = 5  # å¯¼å¼¹ç‰¹å¾æ”¹ä¸º 5 ç»´: [dist_min, dist_max, beta_sin, beta_cos, theta_L]
AIRCRAFT_FEAT_DIM = 7  # é£æœºç‰¹å¾æ”¹ä¸º 7 ç»´: [av, h, ae, am_sin, am_cos, ir, q]
FULL_OBS_DIM = (NUM_MISSILES * MISSILE_FEAT_DIM) + AIRCRAFT_FEAT_DIM

ENTITY_EMBED_DIM = 32 #64
ATTN_NUM_HEADS = 2 #4

assert ENTITY_EMBED_DIM % ATTN_NUM_HEADS == 0, "ENTITY_EMBED_DIM must be divisible by ATTN_NUM_HEADS"


# ==============================================================================
#           <<< æ ¸å¿ƒä¿®æ”¹ >>>: Post-Attention GRU æ¶æ„ (Actor)
# ==============================================================================

class Actor_PostAttentionGRU(Module):
    """
    Actor ç½‘ç»œ - [æ¶æ„: Encoders -> Attention -> Fusion -> GRU -> MLP]
    """

    def __init__(self, weight_decay=1e-4, rnn_hidden_dim=ENTITY_EMBED_DIM):
        super(Actor_PostAttentionGRU, self).__init__()
        self.input_dim = ACTOR_PARA.input_dim
        # # ======================================================================
        # # 1. åŠ¨ä½œæ ‡å‡†å·® (Std) è®¾ç½® - æ ¸å¿ƒä¼˜åŒ–éƒ¨åˆ†
        # # ======================================================================
        # # ä¸‹é™ 0.05: ä¿æŒ 5% çš„åº•å™ªï¼Œé˜²æ­¢ç­–ç•¥è¿‡æ—©å¡Œç¼©ä¸ºç¡®å®šæ€§ï¼Œç»´æŒé²æ£’æ€§
        # self.target_std_min = 0.05
        # # ä¸Šé™ 1.0: å…è®¸å°‘é‡åŒå³°åˆ†å¸ƒï¼ˆå¤§æœºåŠ¨æ¢ç´¢ï¼‰ï¼Œä½†é¿å… 1.5 å¸¦æ¥çš„è¿‡åº¦ Bang-Bang æ§åˆ¶
        # self.target_std_max = 1.0
        # # åˆå§‹ 0.6: ä½äº 0.7 ä¸´ç•Œç‚¹ä¹‹ä¸‹ï¼Œä¿è¯åˆæœŸä¸ºå•å³°åˆ†å¸ƒï¼Œé£æœºé£è¡Œå¹³ç¨³
        # self.target_init_std = 0.95

        self.target_std_min = 0.10 #0.20 #0.10 #0.20 #0.05  # ä¿è¯åº•å™ª
        self.target_std_max = 0.60 #0.80 #0.90 #0.70 #0.80  # é™ä½ä¸Šé™ï¼Œé¿å…å®Œå…¨éšæœº
        self.target_init_std = 0.60 #0.75 #0.85 #0.65 #0.75  # åˆå§‹å€¼è®¾ä¸ºä¸­é—´æ€ï¼Œä¸è¦è®¾ä¸º max

        # è½¬æ¢ä¸º Log ç©ºé—´è¾¹ç•Œ
        self.log_std_min = np.log(self.target_std_min)  # ln(0.05) â‰ˆ -2.99
        self.log_std_max = np.log(self.target_std_max)  # ln(1.0) = 0.0

        self.weight_decay = weight_decay

        # é…ç½®
        self.rnn_hidden_dim = 64 #128 #64 #128 #ENTITY_EMBED_DIM
        self.entity_embed_dim = ENTITY_EMBED_DIM
        self.encoder_hidden_dim = ENTITY_EMBED_DIM

        # 1. ç¼–ç å™¨ (Feature Extraction)
        # [ä¿®æ”¹] æ¢å¤é£æœºç¼–ç å™¨ï¼Œç¡®ä¿ Query å’Œ Key åœ¨åŒä¸€è¯­ä¹‰ç©ºé—´
        self.missile_encoder = Sequential(
            Linear(MISSILE_FEAT_DIM, self.encoder_hidden_dim),
        )
        # self.aircraft_encoder = Sequential(
        #     Linear(FULL_OBS_DIM, self.encoder_hidden_dim),  # ä½¿ç”¨å…¨è§‚æµ‹æˆ–ä»…é£æœºç‰¹å¾å‡å¯ï¼Œè¿™é‡Œç”¨FULLæ–¹ä¾¿
        # )

        # ä¿®æ”¹å
        self.aircraft_encoder = Sequential(
            Linear(AIRCRAFT_FEAT_DIM, self.encoder_hidden_dim),
        )

        # 2. äº¤å‰æ³¨æ„åŠ› (ä¿æŒä¸å˜)
        self.attention = MultiheadAttention(
            embed_dim=self.entity_embed_dim,
            num_heads=ATTN_NUM_HEADS,
            dropout=0.0,
            batch_first=True
        )

        # 3. GRU å±‚ (Global Memory)
        # [ä¿®æ”¹] GRU ç§»åˆ°è¿™é‡Œã€‚è¾“å…¥ç»´åº¦æ˜¯ é£æœºç‰¹å¾ + æ³¨æ„åŠ›ä¸Šä¸‹æ–‡
        self.global_gru = nn.GRU(
            input_size=self.entity_embed_dim * 2,  # Concat(Aircraft, Attn_Out)
            hidden_size=self.rnn_hidden_dim,
            batch_first=True
        )
        # [ä¿®æ”¹ 2] æ·»åŠ  Layer Normalization
        # GRUè¾“å…¥ç»´åº¦æ˜¯ entity_embed_dim * 2ï¼Œè¾“å‡ºæ˜¯ rnn_hidden_dim
        # æ®‹å·®è¿æ¥åçš„ç»´åº¦æ˜¯ input_size + hidden_size
        residual_dim = (self.entity_embed_dim * 2) + self.rnn_hidden_dim
        # self.layer_norm = nn.LayerNorm(residual_dim)

        # MLP å†³ç­–å±‚
        mlp_input_dim = residual_dim  # ä½¿ç”¨æ®‹å·®è¿æ¥åçš„ç»´åº¦
        # 4. MLP å†³ç­–å±‚
        # [ä¿®æ”¹] è¾“å…¥ç»´åº¦ç°åœ¨ç›´æ¥æ˜¯ GRU çš„ hidden_dim
        # mlp_input_dim = self.rnn_hidden_dim

        # gru_input_dim = self.entity_embed_dim * 2
        # mlp_input_dim = self.rnn_hidden_dim + gru_input_dim

        split_point = 2
        mlp_dims = ACTOR_PARA.model_layer_dim
        base_dims = mlp_dims[:split_point]
        tower_dims = mlp_dims[split_point:]

        self.shared_base_mlp = Sequential()
        base_input_dim = mlp_input_dim
        for i, dim in enumerate(base_dims):
            self.shared_base_mlp.add_module(f'base_fc_{i}', Linear(base_input_dim, dim))
            self.shared_base_mlp.add_module(f'base_leakyrelu_{i}', LeakyReLU())
            base_input_dim = dim
        base_output_dim = base_dims[-1] if base_dims else mlp_input_dim

        self.continuous_tower = Sequential()
        tower_input_dim = base_output_dim
        for i, dim in enumerate(tower_dims):
            self.continuous_tower.add_module(f'cont_tower_fc_{i}', Linear(tower_input_dim, dim))
            self.continuous_tower.add_module(f'cont_tower_leakyrelu_{i}', LeakyReLU())
            tower_input_dim = dim
        continuous_tower_output_dim = tower_dims[-1] if tower_dims else base_output_dim

        self.discrete_tower = Sequential()
        tower_input_dim = base_output_dim
        for i, dim in enumerate(tower_dims):
            self.discrete_tower.add_module(f'disc_tower_fc_{i}', Linear(tower_input_dim, dim))
            self.discrete_tower.add_module(f'disc_tower_leakyrelu_{i}', LeakyReLU())
            tower_input_dim = dim
        discrete_tower_output_dim = tower_dims[-1] if tower_dims else base_output_dim

        self.mu_head = Linear(continuous_tower_output_dim, CONTINUOUS_DIM)
        self.discrete_head = Linear(discrete_tower_output_dim, TOTAL_DISCRETE_LOGITS)

        # åˆå§‹åŒ–ä¸º -0.5 å·¦å³ (std â‰ˆ 0.6)ï¼Œæ¯” 1.0 ç¨³å¥ï¼Œåˆæ¯” 0.1 æœ‰æ¢ç´¢æ€§
        init_log_std = np.log(self.target_init_std)
        self.log_std_param = torch.nn.Parameter(torch.full((1, CONTINUOUS_DIM), init_log_std))
        # # =====================================================
        # # 2. è½¯é™åˆ¶å‚æ•°åˆå§‹åŒ–
        # # =====================================================
        # # è®¡ç®—éªŒè¯ï¼š
        # # Sigmoid(2.0) â‰ˆ 0.88
        # # LogStd â‰ˆ ln(0.05) + 0.88 * (ln(1.5) - ln(0.05)) â‰ˆ 0.0
        # # Std â‰ˆ 1.0 (å®Œç¾åˆå§‹å€¼)
        #
        # init_value = 2.5 #2.0
        # self.log_std_param = torch.nn.Parameter(torch.full((1, CONTINUOUS_DIM), init_value))

        # ä¼˜åŒ–å™¨
        attention_params, gru_params, other_params = [], [], []
        for name, param in self.named_parameters():
            if not param.requires_grad: continue
            name_lower = name.lower()
            if any(key in name_lower for key in ['attention', 'attn', 'layer_norm']):
                attention_params.append(param)
            elif 'gru' in name_lower:
                gru_params.append(param)
            else:
                other_params.append(param)
        param_groups = [
            {'params': attention_params, 'lr': ACTOR_PARA.attention_lr},
            {'params': gru_params, 'lr': ACTOR_PARA.gru_lr},
            {'params': other_params, 'lr': ACTOR_PARA.lr}
        ]
        self.optim = torch.optim.Adam(param_groups)
        self.actor_scheduler = lr_scheduler.LinearLR(
            self.optim, start_factor=1.0, end_factor=AGENTPARA.mini_lr / ACTOR_PARA.lr,
            total_iters=AGENTPARA.MAX_EXE_NUM
        )
        self.to(ACTOR_PARA.device)
        self._init_weights()  # å¿…é¡»è¿›è¡Œæƒé‡åˆå§‹åŒ–

    def _init_weights(self):
        for m in self.modules():
            # 1. çº¿æ€§å±‚é€šç”¨åˆå§‹åŒ–
            if isinstance(m, nn.Linear):
                nn.init.orthogonal_(m.weight, gain=np.sqrt(2))
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)

            # 2. GRU ç‰¹æ®Šåˆå§‹åŒ– (å…³é”®ï¼ä¸è¦æ¼æ‰)
            elif isinstance(m, nn.GRU):
                for name, param in m.named_parameters():
                    if 'weight_ih' in name:
                        nn.init.orthogonal_(param.data)
                    elif 'weight_hh' in name:
                        nn.init.orthogonal_(param.data)
                    elif 'bias' in name:
                        param.data.fill_(0)

            # 3. LayerNorm åˆå§‹åŒ–
            elif isinstance(m, nn.LayerNorm):
                nn.init.constant_(m.weight, 1.0)
                nn.init.constant_(m.bias, 0)

        # --- ç‰¹æ®Šå¤„ç†ï¼šç­–ç•¥è¾“å‡ºå¤´ (æœ€åè¦†ç›–å‰é¢çš„é€šç”¨åˆå§‹åŒ–) ---

        # è¿ç»­åŠ¨ä½œå¤´ï¼šç¡®ä¿å‡å€¼æ¥è¿‘ 0ï¼Œé¿å… Tanh é¥±å’Œ
        nn.init.orthogonal_(self.mu_head.weight, gain=0.01)
        nn.init.constant_(self.mu_head.bias, 0)

        # ç¦»æ•£åŠ¨ä½œå¤´ï¼šç¡®ä¿åˆå§‹æ¦‚ç‡å‡åŒ€ (Max Entropy)
        nn.init.orthogonal_(self.discrete_head.weight, gain=0.01)
        nn.init.constant_(self.discrete_head.bias, 0)

    def forward(self, obs, rnn_state=None):
        obs_tensor = check(obs).to(**ACTOR_PARA.tpdv)
        if obs_tensor.dim() == 2:
            obs_tensor = obs_tensor.unsqueeze(1)
        batch_size, seq_len, _ = obs_tensor.shape

        h_prev = rnn_state

        # --- æ•°æ®æå– ---
        obs_flat_raw = obs_tensor.view(-1, FULL_OBS_DIM)
        missile1_obs = obs_tensor[..., 0:MISSILE_FEAT_DIM]
        missile2_obs = obs_tensor[..., MISSILE_FEAT_DIM:2 * MISSILE_FEAT_DIM]
        aircraft_obs = obs_tensor[..., 2 * MISSILE_FEAT_DIM:]

        # 1. ç¼–ç  (Spatial Encoding)
        # å¯¼å¼¹
        missiles_raw = torch.cat([missile1_obs, missile2_obs], dim=0)
        missiles_embed_flat = self.missile_encoder(missiles_raw.view(-1, MISSILE_FEAT_DIM))
        missiles_embed_seq = missiles_embed_flat.view(batch_size * 2, seq_len, self.entity_embed_dim)
        m1_feat_seq, m2_feat_seq = torch.split(missiles_embed_seq, batch_size, dim=0)

        m1_feat_flat = m1_feat_seq.reshape(-1, self.entity_embed_dim)
        m2_feat_flat = m2_feat_seq.reshape(-1, self.entity_embed_dim)

        # é£æœº (ç°åœ¨é€šè¿‡Encoderï¼Œè€Œä¸æ˜¯ç›´æ¥è¿›GRU)
        # air_embed_seq = self.aircraft_encoder(obs_tensor)  # [B, Seq, Dim]
        air_embed_seq = self.aircraft_encoder(aircraft_obs)
        air_embed_flat = air_embed_seq.reshape(-1, self.entity_embed_dim)

        # 2. Attention (Spatial Relation)
        m1_raw = obs_flat_raw[..., 0:MISSILE_FEAT_DIM]
        m2_raw = obs_flat_raw[..., MISSILE_FEAT_DIM:2 * MISSILE_FEAT_DIM]
        # inactive_fingerprint = torch.tensor([1.0, 1.0, 0.0, 1.0, 0.0], device=obs_tensor.device)
        # <<< ä¿®æ”¹å¼€å§‹ï¼šæ›´æ–°æ— æ•ˆå¯¼å¼¹æŒ‡çº¹ >>>
        # åŸä»£ç å¯èƒ½æ˜¯ [1.0, 1.0, 0.0, 1.0, 0.0]ï¼Œè¿™æ˜¯æ­£ç¡®çš„ã€‚
        # å¯¹åº”ç¯å¢ƒä¸­çš„éæ¿€æ´»è§‚æµ‹å€¼: [dist_min=1, dist_max=1, sin=0, cos=1, theta=0]
        # ç¡®ä¿è¿™é‡Œçš„å€¼ä¸ç¯å¢ƒä»£ç ä¸­çš„å®Œå…¨ä¸€è‡´
        inactive_fingerprint = torch.tensor([1.0, 1.0, 0.0, 1.0, 0.0], device=obs_tensor.device)
        # <<< ä¿®æ”¹ç»“æŸ >>>
        is_m1_inactive = torch.all(torch.isclose(m1_raw, inactive_fingerprint), dim=-1)
        is_m2_inactive = torch.all(torch.isclose(m2_raw, inactive_fingerprint), dim=-1)
        attention_mask = torch.stack([is_m1_inactive, is_m2_inactive], dim=1)

        query = air_embed_flat.unsqueeze(1)
        keys = torch.stack([m1_feat_flat, m2_feat_flat], dim=1)

        attn_output, attn_weights = self.attention(query, keys, keys, key_padding_mask=attention_mask)
        if torch.isnan(attn_output).any():
            attn_output = torch.nan_to_num(attn_output, nan=0.0)

        # 3. ç‰¹å¾èåˆ (Fusion)
        # å°†é£æœºè‡ªèº«çš„ç†è§£ä¸å¯¹ç¯å¢ƒå¨èƒçš„ç†è§£æ‹¼æ¥
        # [Batch*Seq, 1, Dim] -> [Batch*Seq, Dim]
        fusion_features_flat = torch.cat([air_embed_flat, attn_output.squeeze(1)], dim=-1)

        # 4. GRU (Temporal Processing - Post Attention)
        # æ¢å¤åºåˆ—ç»´åº¦ä»¥è¿›å…¥GRU: [Batch, Seq, Dim*2]
        fusion_features_seq = fusion_features_flat.view(batch_size, seq_len, -1)

        gru_out, next_h = self.global_gru(fusion_features_seq, h_prev)

        # [ä¿®æ”¹ç‚¹ 2]ï¼šå®ç°æ®‹å·®/è·³è·ƒè¿æ¥
        # å°† GRU çš„è¾“å‡ºä¸ GRU çš„è¾“å…¥æ‹¼æ¥
        # gru_out shape: [Batch, Seq, Hidden]
        # fusion_features_seq shape: [Batch, Seq, Input_Dim]

        # 5. æ®‹å·®è¿æ¥ + LayerNorm (å…³é”®ä¿®æ”¹ç‚¹)
        # å°† GRU çš„è¾“å‡ºä¸ GRU çš„è¾“å…¥æ‹¼æ¥
        residual_features = torch.cat([fusion_features_seq, gru_out], dim=-1)
        # residual_features = gru_out

        # [æ–°å¢] å¯¹æ‹¼æ¥åçš„ç‰¹å¾è¿›è¡Œ LayerNorm
        # residual_features = self.layer_norm(residual_features)

        # å±•å¹³é€å…¥ MLP
        mlp_input = residual_features.reshape(-1, residual_features.shape[-1])

        # æ‹¼æ¥: [Batch, Seq, Hidden + Input_Dim]
        # residual_features = torch.cat([fusion_features_seq, gru_out], dim=-1)

        # å±•å¹³é€å…¥ MLP
        # mlp_input = residual_features.reshape(-1, residual_features.shape[-1])

        # å‡†å¤‡è¿›å…¥ MLP çš„æ•°æ®
        # mlp_input = gru_out.reshape(-1, self.rnn_hidden_dim)

        # 5. MLP å†³ç­–
        base_features = self.shared_base_mlp(mlp_input)
        continuous_features = self.continuous_tower(base_features)
        discrete_features = self.discrete_tower(base_features)

        mu = self.mu_head(continuous_features)

        # å¼ºè¡ŒæŠŠå‡å€¼é™åˆ¶åœ¨ [-2, 2] æˆ– [-3, 3] ä¹‹é—´
        # åªè¦ä¸è®©å®ƒè·‘åˆ° 10 è¿™ç§ç¦»è°±çš„å€¼å°±è¡Œ
        mu = torch.clamp(mu, -3.0, 3.0)

        all_disc_logits = self.discrete_head(discrete_features)

        # Masking å¤„ç† (ä¿æŒä¸å˜)
        split_sizes = list(DISCRETE_DIMS.values())
        logits_parts = torch.split(all_disc_logits, split_sizes, dim=-1)
        trigger_logits, salvo_size_logits, num_groups_logits, inter_interval_logits = logits_parts
        # <<< ä¿®æ”¹å¼€å§‹ï¼šæ›´æ–°è¯±é¥µå¼¹ä¿¡æ¯ç´¢å¼• >>>
        # é£æœºç‰¹å¾ç»“æ„: [av, h, ae, am_sin, am_cos, ir, q]
        # o_ir_norm æ˜¯ç¬¬ 6 ä¸ªå…ƒç´  (ç´¢å¼•ä¸º 5)
        # å…¨å±€ç´¢å¼• = å¯¼å¼¹éƒ¨åˆ†æ€»é•¿ + é£æœºå†…éƒ¨ç´¢å¼•
        flare_info_index = 2 * MISSILE_FEAT_DIM + 5
        has_flares_info = obs_flat_raw[..., flare_info_index]
        mask = (has_flares_info == 0).view(-1)

        NEG_INF = -1e8
        trigger_logits_masked = trigger_logits.clone()
        if torch.any(mask):
            mask_expanded = mask.unsqueeze(-1) if mask.dim() < trigger_logits_masked.dim() else mask
            trigger_logits_masked = torch.where(mask_expanded,
                                                torch.full_like(trigger_logits, NEG_INF),
                                                trigger_logits)

        trigger_probs = torch.sigmoid(trigger_logits_masked)
        no_trigger_mask = (trigger_probs < 0.5)

        salvo_size_logits_masked = salvo_size_logits.clone()
        forced_salvo = torch.full_like(salvo_size_logits_masked, NEG_INF)
        forced_salvo[..., 0] = 1.0
        salvo_size_logits_masked = torch.where(no_trigger_mask, forced_salvo, salvo_size_logits_masked)

        num_groups_logits_masked = num_groups_logits.clone()
        forced_groups = torch.full_like(num_groups_logits_masked, NEG_INF)
        forced_groups[..., 0] = 1.0
        num_groups_logits_masked = torch.where(no_trigger_mask, forced_groups, num_groups_logits_masked)

        inter_interval_logits_masked = inter_interval_logits.clone()
        forced_interval = torch.full_like(inter_interval_logits_masked, NEG_INF)
        forced_interval[..., 0] = 1.0
        inter_interval_logits_masked = torch.where(no_trigger_mask, forced_interval, inter_interval_logits_masked)

        # =========== ä¿®æ”¹: é™åˆ¶æ ‡å‡†å·®åº”ç”¨ ===========
        # ä½¿ç”¨ä¹‹å‰è®¡ç®—å¥½çš„ log ç•Œé™è¿›è¡Œæˆªæ–­
        # log_std_min = ln(0.01), log_std_max = ln(0.6)
        log_std = torch.clamp(self.log_std_param, self.log_std_min, self.log_std_max)
        std = torch.exp(log_std).expand_as(mu)

        # # =====================================================
        # # 3. è®¡ç®—åŠ¨æ€æ ‡å‡†å·® (Soft Mapping)
        # # =====================================================
        # # output = min + (max - min) * sigmoid(param)
        #
        # # 1. å°†æ— ç•Œå‚æ•°å‹ç¼©åˆ° (0, 1)
        # norm_val = torch.sigmoid(self.log_std_param)
        #
        # # 2. æ˜ å°„åˆ° log èŒƒå›´ [log_min, log_max]
        # log_std = self.log_std_min + norm_val * (self.log_std_max - self.log_std_min)
        #
        # # 3. è½¬å› std
        # std = torch.exp(log_std).expand_as(mu)
        # æ­¤æ—¶ std çš„å€¼ä¸€å®šåœ¨ [0.01, 0.6] ä¹‹é—´
        continuous_base_dist = Normal(mu, std)

        trigger_dist = Bernoulli(logits=trigger_logits_masked.squeeze(-1))
        salvo_size_dist = Categorical(logits=salvo_size_logits_masked)
        num_groups_dist = Categorical(logits=num_groups_logits_masked)
        inter_interval_dist = Categorical(logits=inter_interval_logits_masked)

        distributions = {
            'continuous': continuous_base_dist,
            'trigger': trigger_dist,
            'salvo_size': salvo_size_dist,
            'num_groups': num_groups_dist,
            'inter_interval': inter_interval_dist
        }

        attention_to_missiles = attn_weights.squeeze(1).view(batch_size, seq_len, 2)
        if seq_len == 1:
            attention_to_missiles = attention_to_missiles.squeeze(1)

        return distributions, attention_to_missiles, next_h


# ==============================================================================
#           <<< æ ¸å¿ƒä¿®æ”¹ >>>: Post-Attention GRU æ¶æ„ (Critic)
# ==============================================================================

class Critic_PostAttentionGRU(Module):
    """
    Critic ç½‘ç»œ - [æ¶æ„: Encoders -> Attention -> Fusion -> GRU -> MLP]
    """

    def __init__(self, weight_decay=1e-4, rnn_hidden_dim=ENTITY_EMBED_DIM):
        super(Critic_PostAttentionGRU, self).__init__()
        self.input_dim = CRITIC_PARA.input_dim
        self.output_dim = CRITIC_PARA.output_dim
        self.weight_decay = weight_decay

        self.rnn_hidden_dim = 64 #128 #64 #128 #ENTITY_EMBED_DIM
        self.entity_embed_dim = ENTITY_EMBED_DIM
        self.encoder_hidden_dim = ENTITY_EMBED_DIM

        # 1. Encoders
        self.missile_encoder = Sequential(
            Linear(MISSILE_FEAT_DIM, self.encoder_hidden_dim),
        )
        # self.aircraft_encoder = Sequential(
        #     Linear(FULL_OBS_DIM, self.encoder_hidden_dim),
        # )
        # ä¿®æ”¹å
        self.aircraft_encoder = Sequential(
            Linear(AIRCRAFT_FEAT_DIM, self.encoder_hidden_dim),
        )

        # 2. Attention
        self.attention = MultiheadAttention(
            embed_dim=self.entity_embed_dim,
            num_heads=ATTN_NUM_HEADS,
            batch_first=True
        )

        # 3. Post-Attention GRU
        self.global_gru = nn.GRU(
            input_size=self.entity_embed_dim * 2,  # Concat Input
            hidden_size=self.rnn_hidden_dim,
            batch_first=True
        )

        # [!!! ä¿®æ­£è¿™é‡Œ !!!] å®šä¹‰ LayerNorm
        residual_dim = (self.entity_embed_dim * 2) + self.rnn_hidden_dim
        # self.layer_norm = nn.LayerNorm(residual_dim)

        # 4. MLP
        mlp_dims = CRITIC_PARA.model_layer_dim
        self.mlp = Sequential()
        # input_dim = self.rnn_hidden_dim  # æ¥è‡ª GRU çš„è¾“å‡º
        gru_input_dim = self.entity_embed_dim * 2
        input_dim = self.rnn_hidden_dim + gru_input_dim
        for i, dim in enumerate(mlp_dims):
            self.mlp.add_module(f'fc_{i}', Linear(input_dim, dim))
            self.mlp.add_module(f'act_{i}', LeakyReLU())
            input_dim = dim
        self.fc_out = Linear(input_dim, self.output_dim)

        # Optimizer
        attn_params, gru_params, other_params = [], [], []
        for name, param in self.named_parameters():
            if not param.requires_grad: continue
            if 'attention' in name.lower():
                attn_params.append(param)
            elif 'gru' in name.lower():
                gru_params.append(param)
            else:
                other_params.append(param)
        self.optim = torch.optim.Adam([
            {'params': attn_params, 'lr': CRITIC_PARA.attention_lr},
            {'params': gru_params, 'lr': CRITIC_PARA.gru_lr},
            {'params': other_params, 'lr': CRITIC_PARA.lr}
        ])
        self.critic_scheduler = lr_scheduler.LinearLR(self.optim, start_factor=1.0,
                                                      end_factor=AGENTPARA.mini_lr / CRITIC_PARA.lr,
                                                      total_iters=AGENTPARA.MAX_EXE_NUM)
        self.to(CRITIC_PARA.device)
        self._init_weights()

    def _init_weights(self):
        # 1. éå†æ‰€æœ‰æ¨¡å—è¿›è¡Œé€šç”¨åˆå§‹åŒ–
        for m in self.modules():
            # çº¿æ€§å±‚ (Hidden Layers)ï¼šé…åˆ LeakyReLU/ReLU
            if isinstance(m, nn.Linear):
                nn.init.orthogonal_(m.weight, gain=np.sqrt(2))
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)

            # GRU å±‚ï¼šé˜²æ­¢æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸
            elif isinstance(m, nn.GRU):
                for name, param in m.named_parameters():
                    if 'weight_ih' in name:
                        nn.init.orthogonal_(param.data)
                    elif 'weight_hh' in name:
                        nn.init.orthogonal_(param.data)
                    elif 'bias' in name:
                        param.data.fill_(0)

            # LayerNorm å±‚ (å¦‚æœä½ åŠ äº†çš„è¯)
            elif isinstance(m, nn.LayerNorm):
                nn.init.constant_(m.weight, 1.0)
                nn.init.constant_(m.bias, 0)

        # 2. --- ç‰¹æ®Šå¤„ç†ï¼šCritic è¾“å‡ºå¤´ ---
        # è¦†ç›–æ‰ä¸Šé¢çš„é€šç”¨åˆå§‹åŒ–
        # å› ä¸º fc_out åé¢æ²¡æœ‰æ¿€æ´»å‡½æ•°ï¼Œæ‰€ä»¥ gain ä½¿ç”¨ 1.0 (çº¿æ€§å±‚çš„æ ‡å‡†å€¼)
        # è¿™æ ·åˆå§‹çš„ä»·å€¼ä¼°è®¡ V(s) ä¼šåœ¨ 0 é™„è¿‘æ³¢åŠ¨
        nn.init.orthogonal_(self.fc_out.weight, gain=1.0)
        nn.init.constant_(self.fc_out.bias, 0)

    def forward(self, obs, rnn_state=None):
        obs_tensor = check(obs).to(**ACTOR_PARA.tpdv)
        if obs_tensor.dim() == 2:
            obs_tensor = obs_tensor.unsqueeze(1)
        batch_size, seq_len, _ = obs_tensor.shape

        h_prev = rnn_state

        obs_flat_raw = obs_tensor.view(-1, FULL_OBS_DIM)
        missile1_obs = obs_tensor[..., 0:MISSILE_FEAT_DIM]
        missile2_obs = obs_tensor[..., MISSILE_FEAT_DIM:2 * MISSILE_FEAT_DIM]
        aircraft_obs = obs_tensor[..., 2 * MISSILE_FEAT_DIM:]

        # 1. Encoding
        # air_embed_seq = self.aircraft_encoder(obs_tensor)
        air_embed_seq = self.aircraft_encoder(aircraft_obs)
        air_embed_flat = air_embed_seq.reshape(-1, self.entity_embed_dim)

        missiles_raw = torch.cat([missile1_obs, missile2_obs], dim=0)
        missiles_embed_flat = self.missile_encoder(missiles_raw.view(-1, MISSILE_FEAT_DIM))
        missiles_embed_seq = missiles_embed_flat.view(batch_size * 2, seq_len, self.entity_embed_dim)
        m1_feat_seq, m2_feat_seq = torch.split(missiles_embed_seq, batch_size, dim=0)
        m1_feat_flat = m1_feat_seq.reshape(-1, self.entity_embed_dim)
        m2_feat_flat = m2_feat_seq.reshape(-1, self.entity_embed_dim)

        # 2. Attention
        m1_raw = obs_flat_raw[..., 0:MISSILE_FEAT_DIM]
        m2_raw = obs_flat_raw[..., MISSILE_FEAT_DIM:2 * MISSILE_FEAT_DIM]
        # <<< ä¿®æ”¹å¼€å§‹ï¼šæ›´æ–°æ— æ•ˆå¯¼å¼¹æŒ‡çº¹ (ä¸ Actor ä¿æŒä¸€è‡´) >>>
        # ç¡®ä¿è¿™é‡Œçš„å€¼ä¸ç¯å¢ƒä»£ç ä¸­çš„å®Œå…¨ä¸€è‡´: [1.0, 1.0, 0.0, 1.0, 0.0]
        inactive = torch.tensor([1.0, 1.0, 0.0, 1.0, 0.0], device=obs_tensor.device)
        # <<< ä¿®æ”¹ç»“æŸ >>>
        is_m1_in = torch.all(torch.isclose(m1_raw, inactive), dim=-1)
        is_m2_in = torch.all(torch.isclose(m2_raw, inactive), dim=-1)
        mask = torch.stack([is_m1_in, is_m2_in], dim=1)

        query = air_embed_flat.unsqueeze(1)
        keys = torch.stack([m1_feat_flat, m2_feat_flat], dim=1)

        attn_out, _ = self.attention(query, keys, keys, key_padding_mask=mask)
        if torch.isnan(attn_out).any(): attn_out = torch.nan_to_num(attn_out, nan=0.0)

        # 3. Fusion
        fusion_features_flat = torch.cat([air_embed_flat, attn_out.squeeze(1)], dim=-1)

        # 4. GRU
        fusion_features_seq = fusion_features_flat.view(batch_size, seq_len, -1)
        gru_out, next_h = self.global_gru(fusion_features_seq, h_prev)

        # [ä¿®æ”¹ç‚¹ 2]ï¼šæ®‹å·®æ‹¼æ¥
        residual_features = torch.cat([fusion_features_seq, gru_out], dim=-1)
        # residual_features = gru_out

        # åŠ ä¸Šè¿™ä¸€è¡Œï¼š
        # residual_features = self.layer_norm(residual_features)

        # 5. MLP
        mlp_input = residual_features.reshape(-1, residual_features.shape[-1])

        # 5. MLP
        # mlp_input = gru_out.reshape(-1, self.rnn_hidden_dim)
        val = self.fc_out(self.mlp(mlp_input))

        return val, next_h


class PPO_continuous(object):
    def __init__(self, load_able: bool, model_dir_path: str = None, use_rnn: bool = True):
        super(PPO_continuous, self).__init__()

        self.use_rnn = use_rnn  # True
        print(f"--- åˆå§‹åŒ– PPO Agent (Post-Attention GRU) use_rnn={self.use_rnn} ---")

        self.rnn_seq_len = 5 #15 #12 #15 #12 #20 #15 #10 #5 #15 #10 #15 #10 #5 #10
        self.rnn_batch_size = BUFFERPARA.BATCH_SIZE

        # åˆå§‹åŒ–æ¨¡å‹
        self.Actor = Actor_PostAttentionGRU()
        self.Critic = Critic_PostAttentionGRU()

        self.actor_rnn_state = None
        self.critic_rnn_state = None

        self.buffer = Buffer(use_rnn=self.use_rnn, use_attn=True)

        self.gamma = AGENTPARA.gamma
        self.gae_lambda = AGENTPARA.lamda
        self.ppo_epoch = AGENTPARA.ppo_epoch
        self.training_start_time = time.strftime("PPO_PostAttnGRU_%Y-%m-%d_%H-%M-%S")
        self.base_save_dir = "../../../../../save/save_evade_fuzaä¸¤ä¸ªå¯¼å¼¹"
        win_rate_subdir = "èƒœç‡æ¨¡å‹"
        self.run_save_dir = os.path.join(self.base_save_dir, self.training_start_time)
        self.win_rate_dir = os.path.join(self.run_save_dir, win_rate_subdir)
        if load_able:
            if model_dir_path:
                self.load_models_from_directory(model_dir_path)
            else:
                self.load_models_from_directory("../../../../test/test_evade")

    def reset_rnn_state(self):
        self.actor_rnn_state = None
        self.critic_rnn_state = None

    def load_models_from_directory(self, directory_path: str):
        if not os.path.isdir(directory_path):
            print(f"[é”™è¯¯] æ¨¡å‹åŠ è½½å¤±è´¥ï¼šæä¾›çš„è·¯å¾„ '{directory_path}' ä¸æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„æ–‡ä»¶å¤¹ã€‚")
            return
        files = os.listdir(directory_path)
        actor_files_with_prefix = [f for f in files if f.endswith("_Actor.pkl")]
        if len(actor_files_with_prefix) > 0:
            actor_filename = actor_files_with_prefix[0]
            prefix = actor_filename.replace("_Actor.pkl", "")
            critic_filename = f"{prefix}_Critic.pkl"
            print(f"  - æ£€æµ‹åˆ°å‰ç¼€ '{prefix}'ï¼Œå‡†å¤‡åŠ è½½æ¨¡å‹...")
            if critic_filename in files:
                actor_full_path = os.path.join(directory_path, actor_filename)
                critic_full_path = os.path.join(directory_path, critic_filename)
                try:
                    self.Actor.load_state_dict(torch.load(actor_full_path, map_location=ACTOR_PARA.device))
                    print(f"    - æˆåŠŸåŠ è½½ Actor: {actor_full_path}")
                    self.Critic.load_state_dict(torch.load(critic_full_path, map_location=CRITIC_PARA.device))
                    print(f"    - æˆåŠŸåŠ è½½ Critic: {critic_full_path}")
                    return
                except Exception as e:
                    print(f"    - [é”™è¯¯] åŠ è½½å¸¦å‰ç¼€çš„æ¨¡å‹æ—¶å¤±è´¥: {e}")
            else:
                print(f"    - [è­¦å‘Š] æ‰¾åˆ°äº† '{actor_filename}' ä½†æœªæ‰¾åˆ°å¯¹åº”çš„ '{critic_filename}'ã€‚")
        if "Actor.pkl" in files and "Critic.pkl" in files:
            print("  - æ£€æµ‹åˆ°æ— å‰ç¼€æ ¼å¼ï¼Œå‡†å¤‡åŠ è½½ 'Actor.pkl' å’Œ 'Critic.pkl'...")
            actor_full_path = os.path.join(directory_path, "Actor.pkl")
            critic_full_path = os.path.join(directory_path, "Critic.pkl")
            try:
                self.Actor.load_state_dict(torch.load(actor_full_path, map_location=ACTOR_PARA.device))
                print(f"    - æˆåŠŸåŠ è½½ Actor: {actor_full_path}")
                self.Critic.load_state_dict(torch.load(critic_full_path, map_location=CRITIC_PARA.device))
                print(f"    - æˆåŠŸåŠ è½½ Critic: {critic_full_path}")
                return
            except Exception as e:
                print(f"    - [é”™è¯¯] åŠ è½½æ— å‰ç¼€æ¨¡å‹æ—¶å¤±è´¥: {e}")
        print(f"[é”™è¯¯] æ¨¡å‹åŠ è½½å¤±è´¥ï¼šåœ¨æ–‡ä»¶å¤¹ '{directory_path}' ä¸­æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„ Actor/Critic æ¨¡å‹å¯¹ã€‚")

    def scale_action(self, action_cont_tanh):
        lows = torch.tensor([ACTION_RANGES[k]['low'] for k in CONTINUOUS_ACTION_KEYS], **ACTOR_PARA.tpdv)
        highs = torch.tensor([ACTION_RANGES[k]['high'] for k in CONTINUOUS_ACTION_KEYS], **ACTOR_PARA.tpdv)
        return lows + (action_cont_tanh + 1.0) * 0.5 * (highs - lows)

    def store_experience(self, state, action, probs, value, reward, done, attn_weights=None):
        self.buffer.store_transition(state, value, action, probs, reward, done,
                                     actor_hidden=self.temp_actor_h,
                                     critic_hidden=self.temp_critic_h,
                                     attn_weights=attn_weights)
        if done:
            self.reset_rnn_state()

    def choose_action(self, state, deterministic=False):
        state_tensor = torch.as_tensor(state, dtype=torch.float32, device=ACTOR_PARA.device)
        is_batch = state_tensor.dim() > 1
        if not is_batch:
            state_tensor = state_tensor.unsqueeze(0)

        current_actor_h = self.actor_rnn_state
        current_critic_h = self.critic_rnn_state

        with torch.no_grad():
            value, self.critic_rnn_state = self.Critic(state_tensor, self.critic_rnn_state)
            dists, attention_weights, self.actor_rnn_state = self.Actor(state_tensor, self.actor_rnn_state)

            attention_weights_for_reward = attention_weights

            continuous_base_dist = dists['continuous']
            u = continuous_base_dist.mean if deterministic else continuous_base_dist.rsample()
            action_cont_tanh = torch.tanh(u)
            # ================= [ä¿®æ”¹å¼€å§‹] =================
            # 1. è®¡ç®—åŸå§‹é«˜æ–¯åˆ†å¸ƒçš„ log_prob
            log_prob_u = continuous_base_dist.log_prob(u).sum(dim=-1)

            # 2. è®¡ç®—é›…å¯æ¯”ä¿®æ­£é¡¹ (ç¨³å®šå…¬å¼)
            # å…¬å¼: 2 * (log 2 - u - softplus(-2u))
            # æ³¨æ„: u æ˜¯ pre-tanh çš„å€¼
            correction = 2.0 * (np.log(2.0) - u - F.softplus(-2.0 * u)).sum(dim=-1)

            # 3. å¾—åˆ°æœ€ç»ˆåŠ¨ä½œ a = tanh(u) çš„ log_prob
            log_prob_cont = log_prob_u - correction
            # ================= [ä¿®æ”¹ç»“æŸ] =================
            # log_prob_cont = continuous_base_dist.log_prob(u).sum(dim=-1)

            sampled_actions_dict = {}
            for key, dist in dists.items():
                if key == 'continuous': continue
                if deterministic:
                    if isinstance(dist, Categorical):
                        sampled_actions_dict[key] = torch.argmax(dist.probs, dim=-1)
                    else:
                        sampled_actions_dict[key] = (dist.probs > 0.5).float()
                else:
                    sampled_actions_dict[key] = dist.sample()

            log_prob_disc = sum(dists[key].log_prob(act) for key, act in sampled_actions_dict.items())
            total_log_prob = log_prob_cont + log_prob_disc

            action_disc_to_store = torch.stack(list(sampled_actions_dict.values()), dim=-1).float()
            action_to_store = torch.cat([u, action_disc_to_store], dim=-1)
            env_action_cont = self.scale_action(action_cont_tanh)
            final_env_action_tensor = torch.cat([env_action_cont, action_disc_to_store], dim=-1)

            value_np = value.cpu().numpy()
            action_to_store_np = action_to_store.cpu().numpy()
            log_prob_to_store_np = total_log_prob.cpu().numpy()
            final_env_action_np = final_env_action_tensor.cpu().numpy()

            attention_weights_np = attention_weights_for_reward.cpu().numpy() if attention_weights_for_reward is not None else None

            if not is_batch:
                final_env_action_np = final_env_action_np[0]
                action_to_store_np = action_to_store_np[0]
                log_prob_to_store_np = log_prob_to_store_np[0]
                value_np = value_np[0]
                if attention_weights_np is not None:
                    attention_weights_np = attention_weights_np[0]

            # ======================================================================
            # åˆå§‹åŒ– temp_hidden: ç»´åº¦ä»…ä¸º 1 ä¸ª hidden_dim
            # ======================================================================
            if current_actor_h is None:
                batch_size = state_tensor.shape[0]
                total_hidden_dim = self.Actor.rnn_hidden_dim
                self.temp_actor_h = torch.zeros(1, batch_size, total_hidden_dim).to(ACTOR_PARA.device)
                self.temp_critic_h = torch.zeros(1, batch_size, total_hidden_dim).to(CRITIC_PARA.device)
            else:
                self.temp_actor_h = current_actor_h
                self.temp_critic_h = current_critic_h

        return final_env_action_np, action_to_store_np, log_prob_to_store_np, value_np, attention_weights_np

    def cal_gae(self, states, values, actions, probs, rewards, dones, next_value=0.0):
        advantage = np.zeros(len(rewards), dtype=np.float32)
        gae = 0
        for t in reversed(range(len(rewards))):
            if t == len(rewards) - 1:
                value_next_step = next_value
            else:
                value_next_step = values[t + 1]

            done_mask = 1.0 - int(dones[t])
            delta = rewards[t] + self.gamma * value_next_step * done_mask - values[t]
            gae = delta + self.gamma * self.gae_lambda * done_mask * gae
            advantage[t] = gae
        return advantage

    def learn(self, next_visual_value=0.0):
        """
        æ‰§è¡Œ PPO çš„å­¦ä¹ å’Œæ›´æ–°æ­¥éª¤ (Seq2One ä¿®æ”¹ç‰ˆ)ã€‚
        """
        # å¦‚æœ Buffer ä¸­çš„æ•°æ®ä¸è¶³ä¸€ä¸ªæ‰¹æ¬¡ï¼Œåˆ™è·³è¿‡å­¦ä¹ 
        if self.buffer.get_buffer_size() < BUFFERPARA.BATCH_SIZE:
            return None

        # 1. æå–æ‰€æœ‰æ•°æ®
        states, values, actions, old_probs, rewards, dones, _, _, attn_weights = self.buffer.get_all_data()

        # 2. è®¡ç®— GAE ä¼˜åŠ¿
        advantages = self.cal_gae(states, values, actions, old_probs, rewards, dones, next_value=next_visual_value)

        # ================= [å…¨å±€ä¼˜åŠ¿å½’ä¸€åŒ– & ç»´åº¦å¯¹é½] =================
        values = np.squeeze(values)
        if values.ndim == 1: values = values.reshape(-1, 1)
        if advantages.ndim == 1: advantages = advantages.reshape(-1, 1)

        returns = advantages + values

        adv_mean = np.mean(advantages)
        adv_std = np.std(advantages)
        advantages = (advantages - adv_mean) / (adv_std + 1e-8)
        # ===============================================================

        train_info = {'critic_loss': [], 'actor_loss': [], 'dist_entropy': [], 'entropy_cont': [], 'adv_targ': [],
                      'ratio': []}

        # 3. PPO æ›´æ–°å¾ªç¯
        for _ in range(self.ppo_epoch):
            if self.use_rnn:
                batch_generator = self.buffer.generate_sequence_batches(
                    self.rnn_seq_len, self.rnn_batch_size, advantages, returns
                )
            else:
                batch_generator = self.buffer.generate_batches()

            for batch_data in batch_generator:
                if self.use_rnn:
                    # [GRUæ¨¡å¼] è§£åŒ…æ•°æ®
                    (b_s, b_a, b_p, b_adv, b_ret, b_v, b_h_a, b_h_c, _) = batch_data

                    # è½¬ Tensor
                    state = torch.FloatTensor(b_s).to(**ACTOR_PARA.tpdv)
                    action_batch = torch.FloatTensor(b_a).to(**ACTOR_PARA.tpdv)
                    old_prob = torch.FloatTensor(b_p).to(**ACTOR_PARA.tpdv)  # [Batch, Seq]

                    # è¿™é‡Œçš„ b_adv å’Œ b_ret å·²ç»æ˜¯å½’ä¸€åŒ–åçš„ advantage å’ŒåŸå§‹ return
                    advantage = torch.FloatTensor(b_adv).to(**ACTOR_PARA.tpdv)  # [Batch, Seq, 1]
                    return_ = torch.FloatTensor(b_ret).to(**CRITIC_PARA.tpdv)  # [Batch, Seq, 1]

                    rnn_h_a = torch.FloatTensor(b_h_a).to(**ACTOR_PARA.tpdv)
                    rnn_h_c = torch.FloatTensor(b_h_c).to(**CRITIC_PARA.tpdv)

                    # =========================================================
                    # ğŸ”¥ [ä¿®æ”¹ç‚¹ 1] Seq2One: æˆªå– Target çš„æœ€åä¸€æ­¥
                    # =========================================================
                    # advantage: [Batch, Seq, 1] -> [Batch, 1]
                    target_advantage = advantage[:, -1, :]

                    # return_: [Batch, Seq, 1] -> [Batch, 1]
                    target_return = return_[:, -1, :]

                    # old_prob: [Batch, Seq] -> [Batch]
                    target_old_prob = old_prob[:, -1]

                    # å‰å‘ä¼ æ’­ (ä¾ç„¶è¾“å…¥å…¨åºåˆ—ï¼Œä¸ºäº† GRU Context)
                    new_dists, _, _ = self.Actor(state, rnn_h_a)
                    # new_value, _ = self.Critic(state, rnn_h_c) # ç§»åˆ°åé¢ï¼Œéœ€è¦æ—¶å†ç®—

                    # è§£æåŠ¨ä½œ (ä¾ç„¶è§£æå…¨åºåˆ—)
                    # ç»´åº¦è°ƒæ•´: action_batch [Batch, Seq, Dim] ä¿æŒ 3ç»´ ä»¥ä¾¿ log_prob è®¡ç®—
                    u_from_buffer = action_batch[..., :CONTINUOUS_DIM]
                    discrete_actions_from_buffer = {
                        'trigger': action_batch[..., CONTINUOUS_DIM],
                        'salvo_size': action_batch[..., CONTINUOUS_DIM + 1].long(),
                        'num_groups': action_batch[..., CONTINUOUS_DIM + 2].long(),
                        'inter_interval': action_batch[..., CONTINUOUS_DIM + 3].long(),
                    }

                else:
                    # [MLPæ¨¡å¼] (ä¿æŒåŸæœ‰é€»è¾‘)
                    batch_indices = batch_data
                    state = check(states[batch_indices]).to(**ACTOR_PARA.tpdv)
                    action_batch = check(actions[batch_indices]).to(**ACTOR_PARA.tpdv)

                    target_old_prob = check(old_probs[batch_indices]).to(**ACTOR_PARA.tpdv)  # MLPä¸€èˆ¬å­˜çš„æ˜¯å•æ­¥
                    target_advantage = check(advantages[batch_indices]).to(**ACTOR_PARA.tpdv).view(-1, 1)
                    target_return = check(returns[batch_indices]).to(**CRITIC_PARA.tpdv).view(-1, 1)

                    new_dists, _, _ = self.Actor(state)

                    u_from_buffer = action_batch[..., :CONTINUOUS_DIM]
                    discrete_actions_from_buffer = {
                        'trigger': action_batch[..., CONTINUOUS_DIM],
                        'salvo_size': action_batch[..., CONTINUOUS_DIM + 1].long(),
                        'num_groups': action_batch[..., CONTINUOUS_DIM + 2].long(),
                        'inter_interval': action_batch[..., CONTINUOUS_DIM + 3].long(),
                    }

                # ================= [é›…å¯æ¯”ä¿®æ­£ä¸ LogProb è®¡ç®—] =================
                # æ³¨æ„ï¼šæ­¤æ—¶å¦‚æœæ˜¯ RNN æ¨¡å¼ï¼Œè®¡ç®—å‡ºçš„ LogProb è¿˜æ˜¯ [Batch, Seq] ç»´åº¦çš„

                # --- A. è¿ç»­åŠ¨ä½œ Log Prob ---
                log_prob_u_buffer = new_dists['continuous'].log_prob(u_from_buffer).sum(dim=-1)
                correction_buffer = 2.0 * (np.log(2.0) - u_from_buffer - F.softplus(-2.0 * u_from_buffer)).sum(dim=-1)
                new_log_prob_cont = log_prob_u_buffer - correction_buffer

                # --- B. ç†µè®¡ç®— ---
                entropy_base = new_dists['continuous'].entropy().sum(dim=-1)
                u_curr_sample = new_dists['continuous'].rsample()
                correction_curr = 2.0 * (np.log(2.0) - u_curr_sample - F.softplus(-2.0 * u_curr_sample)).sum(dim=-1)
                entropy_cont = entropy_base + correction_curr

                # --- C. ç¦»æ•£åŠ¨ä½œ Log Prob ---
                new_log_prob_disc = sum(
                    new_dists[key].log_prob(discrete_actions_from_buffer[key])
                    for key in discrete_actions_from_buffer
                )
                entropy_disc = sum(
                    dist.entropy() for key, dist in new_dists.items() if key != 'continuous'
                )

                # åˆå¹¶ Log Prob
                new_prob_seq = new_log_prob_cont + new_log_prob_disc

                # ğŸ”¥ [ä¿®æ­£ Bug] ä¿æŒåºåˆ—ç»´åº¦ï¼Œä¸è¦åœ¨è¿™é‡Œ mean()
                total_entropy_seq = entropy_cont + entropy_disc

                # =========================================================
                # ğŸ”¥ [ä¿®æ”¹ç‚¹ 2] Seq2One: æˆªå– Prediction çš„æœ€åä¸€æ­¥
                # =========================================================
                if self.use_rnn:
                    # [Batch, Seq] -> [Batch]
                    current_prob = new_prob_seq[:, -1]
                    current_entropy = total_entropy_seq[:, -1]
                else:
                    current_prob = new_prob_seq
                    current_entropy = total_entropy_seq

                # è®¡ç®— Ratio
                log_ratio = current_prob - target_old_prob
                ratio = torch.exp(torch.clamp(log_ratio, -20.0, 20.0))

                # è®¡ç®— Actor Loss
                if target_advantage.dim() > ratio.dim():
                    target_advantage_squeezed = target_advantage.squeeze(-1)
                else:
                    target_advantage_squeezed = target_advantage

                surr1 = ratio * target_advantage_squeezed
                surr2 = torch.clamp(ratio, 1.0 - AGENTPARA.epsilon, 1.0 + AGENTPARA.epsilon) * target_advantage_squeezed

                # Loss = Policy Loss - Entropy Bonus (å¯¹ current_entropy æ±‚ mean)
                actor_loss = -torch.min(surr1, surr2).mean() - AGENTPARA.entropy * current_entropy.mean()

                # æ›´æ–° Actor
                self.Actor.optim.zero_grad()
                actor_loss.backward()
                torch.nn.utils.clip_grad_norm_(self.Actor.parameters(), max_norm=1.0)
                self.Actor.optim.step()

                # =========================================================
                # 5. Critic æ›´æ–° (Seq2One)
                # =========================================================
                if self.use_rnn:
                    # Critic è¾“å‡º [Batch, Seq, 1]
                    new_value_seq, _ = self.Critic(state, rnn_h_c)

                    # ğŸ”¥ [ä¿®æ”¹ç‚¹ 3] Seq2One: æˆªå– Value çš„æœ€åä¸€æ­¥
                    # [Batch, Seq, 1] -> [Batch, 1]
                    new_value = new_value_seq[:, -1, :]
                else:
                    new_value, _ = self.Critic(state)

                # ç»´åº¦æ£€æŸ¥
                if new_value.dim() > target_return.dim():
                    target_return = target_return.unsqueeze(-1)
                elif new_value.dim() < target_return.dim():
                    new_value = new_value.unsqueeze(-1)

                # Critic Loss
                critic_loss = torch.nn.functional.mse_loss(new_value, target_return)

                # æ›´æ–° Critic
                self.Critic.optim.zero_grad()
                critic_loss.backward()
                torch.nn.utils.clip_grad_norm_(self.Critic.parameters(), max_norm=1.0)
                self.Critic.optim.step()

                # è®°å½•ä¿¡æ¯ (ä½¿ç”¨æˆªå–åå€¼çš„ mean)
                train_info['critic_loss'].append(critic_loss.item())
                train_info['actor_loss'].append(actor_loss.item())
                train_info['dist_entropy'].append(current_entropy.mean().item())
                train_info['entropy_cont'].append(entropy_cont.mean().item())
                train_info['adv_targ'].append(target_advantage.mean().item())
                train_info['ratio'].append(ratio.mean().item())

        if not train_info['critic_loss']:
            print("  [Warning] No batches were generated for training.")
            self.buffer.clear_memory()
            return None

        # 4. æ¸…ç†ä¸ä¿å­˜
        self.buffer.clear_memory()
        for key in train_info:
            train_info[key] = np.mean(train_info[key])

        self.save()
        return train_info

    # def learn(self, next_visual_value=0.0):
    #     """
    #     æ‰§è¡Œ PPO çš„å­¦ä¹ å’Œæ›´æ–°æ­¥éª¤ã€‚
    #     é›†æˆç‰¹æ€§ï¼š
    #     1. Post-Attention GRU æ¶æ„
    #     2. å…¨å±€ä¼˜åŠ¿å½’ä¸€åŒ– (Global Advantage Normalization)
    #     3. é›…å¯æ¯”ä¿®æ­£ (Jacobian Correction)
    #     4. ç»´åº¦å¯¹é½é˜²é”™
    #     """
    #     # å¦‚æœ Buffer ä¸­çš„æ•°æ®ä¸è¶³ä¸€ä¸ªæ‰¹æ¬¡ï¼Œåˆ™è·³è¿‡å­¦ä¹ 
    #     if self.buffer.get_buffer_size() < BUFFERPARA.BATCH_SIZE:
    #         return None
    #
    #     # 1. æå–æ‰€æœ‰æ•°æ® (æ­¤æ—¶éƒ½åœ¨ CPU ä¸Šï¼Œä¸º Numpy æ•°ç»„)
    #     states, values, actions, old_probs, rewards, dones, _, _, attn_weights = self.buffer.get_all_data()
    #
    #     # 2. è®¡ç®— GAE ä¼˜åŠ¿ (ä½¿ç”¨ä¼ å…¥çš„ next_visual_value å¤„ç†æˆªæ–­)
    #     advantages = self.cal_gae(states, values, actions, old_probs, rewards, dones, next_value=next_visual_value)
    #
    #     # ================= [å…³é”®ä¿®æ”¹ï¼šå…¨å±€ä¼˜åŠ¿å½’ä¸€åŒ– & ç»´åº¦å¯¹é½] =================
    #
    #     # 1. ç»´åº¦å¼ºåˆ¶å¯¹é½ (N,) -> (N, 1)
    #     # é˜²æ­¢ (N,) + (N, 1) å¯¼è‡´ç”Ÿæˆ (N, N) çš„å·¨å¤§çŸ©é˜µ
    #     values = np.squeeze(values)  # ç¡®ä¿æ˜¯ (N,)
    #     if values.ndim == 1:
    #         values = values.reshape(-1, 1)
    #     if advantages.ndim == 1:
    #         advantages = advantages.reshape(-1, 1)
    #
    #     # 2. è®¡ç®— Critic çš„ç›®æ ‡ Returns (å¿…é¡»ä½¿ç”¨æœªå½’ä¸€åŒ–çš„åŸå§‹æ•°æ®)
    #     # Return = Advantage_raw + Value_old
    #     returns = advantages + values
    #
    #     # 3. å¯¹ Advantage è¿›è¡Œå…¨å±€å½’ä¸€åŒ– (ç”¨äº Actor æ›´æ–°)
    #     # åŸºäºæ•´ä¸ª buffer çš„ç»Ÿè®¡æ•°æ®è¿›è¡Œå½’ä¸€åŒ–ï¼Œæ¯” mini-batch å½’ä¸€åŒ–æ›´ç¨³å®š
    #     adv_mean = np.mean(advantages)
    #     adv_std = np.std(advantages)
    #     advantages = (advantages - adv_mean) / (adv_std + 1e-8)
    #
    #     # =======================================================================
    #
    #     train_info = {'critic_loss': [], 'actor_loss': [], 'dist_entropy': [], 'entropy_cont': [], 'adv_targ': [],
    #                   'ratio': []}
    #
    #     # 3. PPO æ›´æ–°å¾ªç¯
    #     for _ in range(self.ppo_epoch):
    #         # æ ¹æ®æ˜¯å¦ä½¿ç”¨ RNNï¼Œé€‰æ‹©ä¸åŒçš„æ‰¹æ¬¡ç”Ÿæˆå™¨
    #         if self.use_rnn:
    #             # [GRUæ¨¡å¼]
    #             # å°†å¤„ç†å¥½çš„å…¨å±€ advantages å’Œ returns ä¼ å…¥ç”Ÿæˆå™¨
    #             # ç”Ÿæˆå™¨å†…éƒ¨ä¼šæ ¹æ®åºåˆ—åˆ‡ç‰‡æå–å¯¹åº”çš„ç‰‡æ®µ
    #             batch_generator = self.buffer.generate_sequence_batches(
    #                 self.rnn_seq_len, self.rnn_batch_size, advantages, returns
    #             )
    #         else:
    #             # [MLPæ¨¡å¼]
    #             batch_generator = self.buffer.generate_batches()
    #
    #         for batch_data in batch_generator:
    #             if self.use_rnn:
    #                 # [GRUæ¨¡å¼] è§£åŒ…æ•°æ® (æ³¨æ„ï¼šreturn_ å’Œ advantage å·²ç»æ˜¯å¤„ç†è¿‡çš„äº†)
    #                 # è¿™é‡Œçš„è§£åŒ…éœ€è¦æ ¹æ®ä½  Buffer çš„å…·ä½“å®ç°æ¥ç¡®å®š
    #                 # å‡è®¾ Buffer è¿”å›çš„æ˜¯ (s, a, p, adv, ret, v, h_a, h_c, mask)
    #                 (b_s, b_a, b_p, b_adv, b_ret, b_v, b_h_a, b_h_c, _) = batch_data
    #
    #                 # è½¬ Tensor
    #                 state = torch.FloatTensor(b_s).to(**ACTOR_PARA.tpdv)
    #                 action_batch = torch.FloatTensor(b_a).to(**ACTOR_PARA.tpdv)
    #                 old_prob = torch.FloatTensor(b_p).to(**ACTOR_PARA.tpdv).view(-1)
    #
    #                 # è¿™é‡Œçš„ b_adv å’Œ b_ret å·²ç»æ˜¯å½’ä¸€åŒ–åçš„ advantage å’ŒåŸå§‹ return äº†
    #                 advantage = torch.FloatTensor(b_adv).to(**ACTOR_PARA.tpdv).view(-1, 1)
    #                 return_ = torch.FloatTensor(b_ret).to(**CRITIC_PARA.tpdv).view(-1, 1)
    #
    #                 # old_value ç”¨äº Value Clipping (å¯é€‰)
    #                 old_value = torch.FloatTensor(b_v).to(**CRITIC_PARA.tpdv).view(-1, 1)
    #
    #                 rnn_h_a = torch.FloatTensor(b_h_a).to(**ACTOR_PARA.tpdv)
    #                 rnn_h_c = torch.FloatTensor(b_h_c).to(**CRITIC_PARA.tpdv)
    #
    #                 # å‰å‘ä¼ æ’­
    #                 new_dists, _, _ = self.Actor(state, rnn_h_a)
    #                 new_value, _ = self.Critic(state, rnn_h_c)
    #
    #                 # ç»´åº¦è°ƒæ•´
    #                 new_value = new_value.view(-1, 1)
    #                 action_batch = action_batch.view(-1, action_batch.shape[-1])
    #
    #             else:
    #                 # [MLPæ¨¡å¼]
    #                 batch_indices = batch_data
    #                 state = check(states[batch_indices]).to(**ACTOR_PARA.tpdv)
    #                 action_batch = check(actions[batch_indices]).to(**ACTOR_PARA.tpdv)
    #                 old_prob = check(old_probs[batch_indices]).to(**ACTOR_PARA.tpdv)
    #
    #                 # ç›´æ¥æå–å·²ç»å…¨å±€å½’ä¸€åŒ–è¿‡çš„ Advantage
    #                 advantage = check(advantages[batch_indices]).to(**ACTOR_PARA.tpdv).view(-1, 1)
    #                 # ç›´æ¥æå–é¢„è®¡ç®—å¥½çš„ Return
    #                 return_ = check(returns[batch_indices]).to(**CRITIC_PARA.tpdv).view(-1, 1)
    #
    #                 old_value = check(values[batch_indices]).to(**CRITIC_PARA.tpdv).view(-1, 1)
    #
    #                 new_dists, _, _ = self.Actor(state)
    #                 new_value, _ = self.Critic(state)
    #
    #             # è§£æåŠ¨ä½œ
    #             u_from_buffer = action_batch[..., :CONTINUOUS_DIM]
    #             discrete_actions_from_buffer = {
    #                 'trigger': action_batch[..., CONTINUOUS_DIM],
    #                 'salvo_size': action_batch[..., CONTINUOUS_DIM + 1].long(),
    #                 'num_groups': action_batch[..., CONTINUOUS_DIM + 2].long(),
    #                 'inter_interval': action_batch[..., CONTINUOUS_DIM + 3].long(),
    #             }
    #
    #             # ================= [é›…å¯æ¯”ä¿®æ­£ä¸ LogProb è®¡ç®—] =================
    #
    #             # --- A. è¿ç»­åŠ¨ä½œ Log Prob ---
    #             # 1. è®¡ç®—æ—§åŠ¨ä½œåœ¨é«˜æ–¯åˆ†å¸ƒä¸‹çš„ log_prob
    #             log_prob_u_buffer = new_dists['continuous'].log_prob(u_from_buffer).sum(dim=-1)
    #
    #             # 2. è®¡ç®—é›…å¯æ¯”ä¿®æ­£é¡¹
    #             correction_buffer = 2.0 * (np.log(2.0) - u_from_buffer - F.softplus(-2.0 * u_from_buffer)).sum(dim=-1)
    #
    #             # 3. å¾—åˆ°æœ€ç»ˆ Log Prob (ç”¨äº Ratio)
    #             new_log_prob_cont = log_prob_u_buffer - correction_buffer
    #
    #             # --- B. ç†µè®¡ç®— (ä½¿ç”¨é‡é‡‡æ ·æŠ€å·§) ---
    #             # 1. åŸºç¡€é«˜æ–¯ç†µ
    #             entropy_base = new_dists['continuous'].entropy().sum(dim=-1)
    #
    #             # 2. é‡é‡‡æ ·å½“å‰ç­–ç•¥åŠ¨ä½œ
    #             u_curr_sample = new_dists['continuous'].rsample()
    #
    #             # 3. è®¡ç®—ä¿®æ­£æœŸæœ›
    #             correction_curr = 2.0 * (np.log(2.0) - u_curr_sample - F.softplus(-2.0 * u_curr_sample)).sum(dim=-1)
    #
    #             # 4. å¾—åˆ°æœ€ç»ˆç†µ
    #             entropy_cont = entropy_base + correction_curr
    #
    #             # --- C. ç¦»æ•£åŠ¨ä½œ Log Prob ---
    #             new_log_prob_disc = sum(
    #                 new_dists[key].log_prob(discrete_actions_from_buffer[key])
    #                 for key in discrete_actions_from_buffer
    #             )
    #
    #             # åˆå¹¶ Log Prob
    #             new_prob = new_log_prob_cont + new_log_prob_disc
    #             # ==========================================================
    #
    #             # è®¡ç®—æ€»ç†µ
    #             entropy_disc = sum(
    #                 dist.entropy() for key, dist in new_dists.items() if key != 'continuous'
    #             )
    #             total_entropy = (entropy_cont.mean() + entropy_disc.mean())
    #
    #             # è®¡ç®— Ratio
    #             log_ratio = new_prob - old_prob
    #             ratio = torch.exp(torch.clamp(log_ratio, -20.0, 20.0))
    #
    #             # è®¡ç®— Actor Loss (ä½¿ç”¨å½’ä¸€åŒ–åçš„ advantage)
    #             if advantage.dim() > ratio.dim():
    #                 advantage_squeezed = advantage.squeeze(-1)
    #             else:
    #                 advantage_squeezed = advantage
    #
    #             surr1 = ratio * advantage_squeezed
    #             surr2 = torch.clamp(ratio, 1.0 - AGENTPARA.epsilon, 1.0 + AGENTPARA.epsilon) * advantage_squeezed
    #             actor_loss = -torch.min(surr1, surr2).mean() - AGENTPARA.entropy * total_entropy
    #
    #             # æ›´æ–° Actor
    #             self.Actor.optim.zero_grad()
    #             actor_loss.backward()
    #             torch.nn.utils.clip_grad_norm_(self.Actor.parameters(), max_norm=1.0)
    #             self.Actor.optim.step()
    #
    #             # Critic Loss (ä½¿ç”¨åŸå§‹ return)
    #             critic_loss = torch.nn.functional.mse_loss(new_value, return_)
    #
    #             # æ›´æ–° Critic
    #             self.Critic.optim.zero_grad()
    #             critic_loss.backward()
    #             torch.nn.utils.clip_grad_norm_(self.Critic.parameters(), max_norm=1.0)
    #             self.Critic.optim.step()
    #
    #             # è®°å½•ä¿¡æ¯
    #             train_info['critic_loss'].append(critic_loss.item())
    #             train_info['actor_loss'].append(actor_loss.item())
    #             train_info['dist_entropy'].append(total_entropy.item())
    #             train_info['entropy_cont'].append(entropy_cont.mean().item())
    #             train_info['adv_targ'].append(advantage.mean().item())
    #             train_info['ratio'].append(ratio.mean().item())
    #
    #     if not train_info['critic_loss']:
    #         print("  [Warning] No batches were generated for training.")
    #         self.buffer.clear_memory()
    #         return None
    #
    #     # 4. æ¸…ç†ä¸ä¿å­˜
    #     self.buffer.clear_memory()
    #     for key in train_info:
    #         train_info[key] = np.mean(train_info[key])
    #
    #     self.save()
    #     return train_info

    def prep_training_rl(self):
        self.Actor.train()
        self.Critic.train()

    def prep_eval_rl(self):
        self.Actor.eval()
        self.Critic.eval()

    def save(self, prefix=""):
        try:
            os.makedirs(self.run_save_dir, exist_ok=True)
            os.makedirs(self.win_rate_dir, exist_ok=True)
        except Exception as e:
            print(f"åˆ›å»ºå­˜æ¡£ç›®å½•å¤±è´¥: {e}")

        target_dir = self.win_rate_dir if prefix else self.run_save_dir

        for net_name in ['Actor', 'Critic']:
            try:
                net_model = getattr(self, net_name)
                filename = f"{prefix}_{net_name}.pkl" if prefix else f"{net_name}.pkl"
                full_path = os.path.join(target_dir, filename)
                torch.save(net_model.state_dict(), full_path)
                print(f"  - {filename} ä¿å­˜æˆåŠŸäº {target_dir}ã€‚")
            except Exception as e:
                print(f"  - ä¿å­˜æ¨¡å‹ {net_name} æ—¶å‘ç”Ÿé”™è¯¯: {e}")