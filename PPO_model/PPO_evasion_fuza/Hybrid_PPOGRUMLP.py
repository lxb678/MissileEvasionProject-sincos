# --- START OF FILE Hybrid_PPO_jsbsim_SeparateHeads.py ---

import torch
from torch.nn import *
from torch.distributions import Bernoulli, Categorical
from torch.distributions import Normal
# å¯¼å…¥é…ç½®æ–‡ä»¶ï¼Œå…¶ä¸­åŒ…å«å„ç§è¶…å‚æ•°
from Interference_code.PPO_model.PPO_evasion_fuza.Config import *
# å¯¼å…¥æ”¯æŒ GRU çš„ç»éªŒå›æ”¾æ± 
from Interference_code.PPO_model.PPO_evasion_fuza.BufferGRU import *
from torch.optim import lr_scheduler
import numpy as np
import os
import re
import time
# å¯¼å…¥ PyTorch çš„è‡ªåŠ¨æ··åˆç²¾åº¦è®­ç»ƒå·¥å…·ï¼Œç”¨äºåŠ é€Ÿè®­ç»ƒå¹¶å‡å°‘æ˜¾å­˜å ç”¨
from torch.cuda.amp import GradScaler, autocast

# --- åŠ¨ä½œç©ºé—´é…ç½® (ä¸åŸç‰ˆç›¸åŒ) ---
# å®šä¹‰è¿ç»­åŠ¨ä½œçš„ç»´åº¦å’Œé”®å
CONTINUOUS_DIM = 4
CONTINUOUS_ACTION_KEYS = ['throttle', 'elevator', 'aileron', 'rudder']
# å®šä¹‰ç¦»æ•£åŠ¨ä½œçš„ç»´åº¦ã€‚æ¯ä¸ªé”®ä»£è¡¨ä¸€ä¸ªç¦»æ•£å†³ç­–ï¼Œå€¼ä»£è¡¨è¯¥å†³ç­–æœ‰å¤šå°‘ä¸ªé€‰é¡¹ã€‚
DISCRETE_DIMS = {
    'flare_trigger': 1,  # å¹²æ‰°å¼¹è§¦å‘ï¼Œä¼¯åŠªåˆ©åˆ†å¸ƒ (æ˜¯/å¦)ï¼Œæ‰€ä»¥æ˜¯1ä¸ª logit
    'salvo_size': 3,  # é½å°„æ•°é‡ï¼Œ3ä¸ªé€‰é¡¹
    'intra_interval': 3,  # ç»„å†…é—´éš”ï¼Œ3ä¸ªé€‰é¡¹
    'num_groups': 3,  # ç»„æ•°ï¼Œ3ä¸ªé€‰é¡¹
    'inter_interval': 3,  # ç»„é—´é—´éš”ï¼Œ3ä¸ªé€‰é¡¹
}
# è®¡ç®—æ‰€æœ‰ç¦»æ•£åŠ¨ä½œ logits çš„æ€»ç»´åº¦
TOTAL_DISCRETE_LOGITS = sum(DISCRETE_DIMS.values())
# è®¡ç®—å­˜å‚¨åœ¨ Buffer ä¸­çš„æ€»åŠ¨ä½œç»´åº¦ï¼ˆè¿ç»­åŠ¨ä½œ + ç¦»æ•£åŠ¨ä½œçš„ç±»åˆ«ç´¢å¼•ï¼‰
TOTAL_ACTION_DIM_BUFFER = CONTINUOUS_DIM + len(DISCRETE_DIMS)
# ç¦»æ•£åŠ¨ä½œçš„ç±»åˆ«ç´¢å¼•åˆ°å®é™…ç‰©ç†å€¼çš„æ˜ å°„
DISCRETE_ACTION_MAP = {
    # 'salvo_size': [2, 4, 6],
    # 'intra_interval': [0.02, 0.04, 0.1],
    # 'num_groups': [1, 2, 3],
    # 'inter_interval': [0.5, 1.0, 2.0]
    'salvo_size': [1, 2, 3],  # ä¿®æ”¹ä¸ºå‘å°„1ã€2ã€3æš
    # 'intra_interval': [0.05, 0.1, 0.15],
    'intra_interval': [0.02, 0.04, 0.08],
    'num_groups': [1, 2, 3],
    'inter_interval': [0.2, 0.5, 1.0]
}
# è¿ç»­åŠ¨ä½œçš„ç‰©ç†èŒƒå›´ï¼Œç”¨äºå°†ç½‘ç»œè¾“å‡º (-1, 1) ç¼©æ”¾åˆ°å®é™…èŒƒå›´
ACTION_RANGES = {
    'throttle': {'low': 0.0, 'high': 1.0},
    'elevator': {'low': -1.0, 'high': 1.0},
    'aileron': {'low': -1.0, 'high': 1.0},
    'rudder': {'low': -1.0, 'high': 1.0},
}

# <<< GRU/RNN ä¿®æ”¹ >>>: æ–°å¢ RNN é…ç½®
# è¿™äº›å‚æ•°æœ€å¥½ä¹Ÿç§»åˆ° Config.py ä¸­
RNN_HIDDEN_SIZE = 256  # GRU å±‚çš„éšè—å•å…ƒæ•°é‡
SEQUENCE_LENGTH = 10  # è®­ç»ƒæ—¶ä»ç»éªŒæ± ä¸­é‡‡æ ·çš„è¿ç»­è½¨è¿¹ç‰‡æ®µçš„é•¿åº¦


# ==============================================================================
# Original MLP-based Actor and Critic (ä¿ç•™åŸå§‹ç‰ˆæœ¬ä»¥ä¾›é€‰æ‹©)
# ==============================================================================

class Actor(Module):
    # ... åŸç‰ˆ Actor ä»£ç ä¿æŒä¸å˜ ...
    """
       Actor ç½‘ç»œ (ç­–ç•¥ç½‘ç»œ) - åŸºäº MLPï¼ˆå¤šå±‚æ„ŸçŸ¥æœºï¼‰çš„ç‰ˆæœ¬ã€‚
       è¯¥ç½‘ç»œè´Ÿè´£æ ¹æ®å½“å‰çŠ¶æ€å†³å®šè¦é‡‡å–çš„åŠ¨ä½œç­–ç•¥ã€‚
       å®ƒå…·æœ‰ä¸€ä¸ªå…±äº«çš„éª¨å¹²ç½‘ç»œï¼Œåæ¥ä¸¤ä¸ªç‹¬ç«‹çš„å¤´éƒ¨ï¼Œåˆ†åˆ«å¤„ç†è¿ç»­åŠ¨ä½œå’Œç¦»æ•£åŠ¨ä½œã€‚
        [ğŸ’¥ ä¿®æ”¹] è¿ç»­åŠ¨ä½œéƒ¨åˆ†ï¼šmu å¤´çŠ¶æ€ä¾èµ–ï¼Œlog_std ä¸ºå…¨å±€å¯å­¦ä¹ å‚æ•°ã€‚
       """

    def __init__(self):
        super(Actor, self).__init__()
        self.input_dim = ACTOR_PARA.input_dim
        # <<< æ›´æ”¹ >>> è¾“å‡ºç»´åº¦ç°åœ¨æ˜¯ (è¿ç»­*2) + æ–°çš„logitsæ€»æ•°
        # æ³¨æ„ï¼šè¿™é‡Œä¸å†éœ€è¦ä¸€ä¸ªæ€»çš„ output_dim
        # self.output_dim = (CONTINUOUS_DIM * 2) + TOTAL_DISCRETE_LOGITS
        self.log_std_min = -20.0 # é™åˆ¶å¯¹æ•°æ ‡å‡†å·®çš„æœ€å°å€¼ï¼Œé˜²æ­¢æ•°å€¼ä¸ç¨³å®š
        self.log_std_max = 2.0 # é™åˆ¶å¯¹æ•°æ ‡å‡†å·®çš„æœ€å¤§å€¼

        # å®šä¹‰å…±äº«éª¨å¹²ç½‘ç»œ
        # è´Ÿè´£ä»åŸå§‹çŠ¶æ€ä¸­æå–é«˜çº§ç‰¹å¾
        shared_layers_dims = [self.input_dim] + ACTOR_PARA.model_layer_dim
        self.shared_network = Sequential()
        for i in range(len(shared_layers_dims) - 1):
            # æ·»åŠ çº¿æ€§ï¼ˆå…¨è¿æ¥ï¼‰å±‚
            self.shared_network.add_module(f'fc_{i}', Linear(shared_layers_dims[i], shared_layers_dims[i + 1]))
            # --- åœ¨æ­¤å¤„æ·»åŠ  LayerNorm ---
            # LayerNorm çš„è¾“å…¥ç»´åº¦æ˜¯å‰ä¸€ä¸ªçº¿æ€§å±‚çš„è¾“å‡ºç»´åº¦
            # LayerNorm å¯¹æ¯ä¸ªæ ·æœ¬çš„ç‰¹å¾è¿›è¡Œå½’ä¸€åŒ–ï¼Œæœ‰åŠ©äºç¨³å®šè®­ç»ƒ
            self.shared_network.add_module(f'LayerNorm_{i}', LayerNorm(shared_layers_dims[i + 1]))
            # æ·»åŠ  LeakyReLU æ¿€æ´»å‡½æ•°ï¼Œä»¥é¿å…æ¢¯åº¦æ¶ˆå¤±é—®é¢˜
            self.shared_network.add_module(f'LeakyReLU_{i}', LeakyReLU())

        # éª¨å¹²ç½‘ç»œçš„è¾“å‡ºç»´åº¦ï¼Œå°†ä½œä¸ºå„ä¸ªå¤´éƒ¨çš„è¾“å…¥
        shared_output_dim = ACTOR_PARA.model_layer_dim[-1]
        # --- ç‹¬ç«‹å¤´éƒ¨ç½‘ç»œ ---
        # # 1. è¿ç»­åŠ¨ä½œå¤´éƒ¨ï¼šè¾“å‡ºé«˜æ–¯åˆ†å¸ƒçš„å‡å€¼ (mu) å’Œå¯¹æ•°æ ‡å‡†å·® (log_std)ï¼Œæ¯ä¸ªè¿ç»­åŠ¨ä½œç»´åº¦éƒ½éœ€è¦è¿™ä¸¤ä¸ªå‚æ•°
        # self.continuous_head = Linear(shared_output_dim, CONTINUOUS_DIM * 2)
        # --- ğŸ’¥ [ä¿®æ”¹] ç‹¬ç«‹å¤´éƒ¨ç½‘ç»œ ---
        # mu å¤´éƒ¨
        self.mu_head = Linear(shared_output_dim, CONTINUOUS_DIM)
        # log_std ä½œä¸ºç‹¬ç«‹çš„ã€ä¸çŠ¶æ€æ— å…³çš„å¯å­¦ä¹ å‚æ•°
        self.log_std_param = torch.nn.Parameter(torch.zeros(1, CONTINUOUS_DIM) * -0.5)

        # 2. ç¦»æ•£åŠ¨ä½œå¤´éƒ¨ï¼šè¾“å‡ºæ‰€æœ‰ç¦»æ•£å†³ç­–æ‰€éœ€çš„ logitsï¼ˆæœªç» Softmax çš„åŸå§‹åˆ†æ•°ï¼‰
        self.discrete_head = Linear(shared_output_dim, TOTAL_DISCRETE_LOGITS)

        # self.init_model() # (è¢«æ³¨é‡Šæ‰ï¼Œå› ä¸ºç½‘ç»œç»“æ„åœ¨ init ä¸­ç›´æ¥å®šä¹‰)
        # --- ä¼˜åŒ–å™¨å’Œè®¾å¤‡è®¾ç½® ---
        self.optim = torch.optim.Adam(self.parameters(), ACTOR_PARA.lr)
        self.actor_scheduler = lr_scheduler.LinearLR(
            self.optim,
            start_factor=1.0,# åˆå§‹å­¦ä¹ ç‡å› å­
            end_factor=AGENTPARA.mini_lr / ACTOR_PARA.lr,# æœ€ç»ˆå­¦ä¹ ç‡å› å­
            total_iters=AGENTPARA.MAX_EXE_NUM # è¾¾åˆ°æœ€ç»ˆå­¦ä¹ ç‡æ‰€éœ€çš„æ€»è¿­ä»£æ¬¡æ•°
        )
        self.to(ACTOR_PARA.device) # å°†æ¨¡å‹ç§»åŠ¨åˆ°æŒ‡å®šçš„è®¾å¤‡ (CPU æˆ– GPU)

    def forward(self, obs):
        """
        å‰å‘ä¼ æ’­æ–¹æ³•ï¼Œä¸ºæ¯ä¸ªåŠ¨ä½œç»´åº¦åˆ›å»ºå¹¶è¿”å›ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒã€‚
        """
        # ç¡®ä¿è¾“å…¥æ˜¯ PyTorch å¼ é‡å¹¶ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡
        obs_tensor = check(obs).to(**ACTOR_PARA.tpdv)
        # å¦‚æœè¾“å…¥æ˜¯ä¸€ç»´çš„ï¼ˆå•ä¸ªçŠ¶æ€ï¼‰ï¼Œå¢åŠ ä¸€ä¸ªæ‰¹æ¬¡ç»´åº¦
        if obs_tensor.dim() == 1:
            obs_tensor = obs_tensor.unsqueeze(0)

        # 1. é€šè¿‡å…±äº«éª¨å¹²ç½‘ç»œæå–é€šç”¨ç‰¹å¾
        shared_features = self.shared_network(obs_tensor)

        # 2. å°†å…±äº«ç‰¹å¾åˆ†åˆ«é€å…¥ä¸åŒçš„å¤´éƒ¨ç½‘ç»œ
        # cont_params = self.continuous_head(shared_features) # è·å–è¿ç»­åŠ¨ä½œçš„å‚æ•°
        # --- ğŸ’¥ [ä¿®æ”¹] ä»ä¸åŒå¤´è·å–å‚æ•° ---
        mu = self.mu_head(shared_features)
        all_disc_logits = self.discrete_head(shared_features) # è·å–æ‰€æœ‰ç¦»æ•£åŠ¨ä½œçš„ logits

        # ... åç»­é€»è¾‘ä¸åŸç‰ˆå®Œå…¨ç›¸åŒ ...
        # æ ¹æ® DISCRETE_DIMS çš„å®šä¹‰ï¼Œå°†æ€»çš„ logits åˆ‡åˆ†æˆå¯¹åº”æ¯ä¸ªç¦»æ•£åŠ¨ä½œçš„éƒ¨åˆ†
        split_sizes = list(DISCRETE_DIMS.values())
        logits_parts = torch.split(all_disc_logits, split_sizes, dim=1)
        trigger_logits, salvo_size_logits, intra_interval_logits, num_groups_logits, inter_interval_logits = logits_parts
        # è·å–çŠ¶æ€ä¸­å…³äºå¹²æ‰°å¼¹æ•°é‡çš„ä¿¡æ¯ï¼ˆç´¢å¼•ä¸º7ï¼‰
        has_flares_info = obs_tensor[:, 7]
        # åˆ›å»ºä¸€ä¸ªæ©ç ï¼Œå½“å¹²æ‰°å¼¹æ•°é‡ä¸º0æ—¶ä¸º True
        mask = (has_flares_info == 0)
        trigger_logits_masked = trigger_logits.clone()
        # å¦‚æœå­˜åœ¨å¹²æ‰°å¼¹æ•°é‡ä¸º0çš„æƒ…å†µï¼Œå°†å¯¹åº”çš„ trigger_logits è®¾ç½®ä¸ºè´Ÿæ— ç©·å¤§
        # è¿™æ ·åœ¨åº”ç”¨ sigmoid/softmax åï¼Œè§¦å‘æ¦‚ç‡ä¼šè¶‹è¿‘äº0ï¼Œå®ç°äº†åŠ¨ä½œå±è”½
        if torch.any(mask):
            trigger_logits_masked[mask] = torch.finfo(torch.float32).min
        # # ä»è¿ç»­åŠ¨ä½œå‚æ•°ä¸­åˆ†ç¦»å‡ºå‡å€¼å’Œå¯¹æ•°æ ‡å‡†å·®
        # mu, log_std = cont_params.chunk(2, dim=-1)
        # è£å‰ªå¯¹æ•°æ ‡å‡†å·®ä»¥ä¿è¯æ•°å€¼ç¨³å®šæ€§
        # log_std = torch.clamp(log_std, self.log_std_min, self.log_std_max)
        # # è®¡ç®—æ ‡å‡†å·®
        # std = torch.exp(log_std)

        # --- ğŸ’¥ [ä¿®æ”¹] åˆ›å»ºè¿ç»­åŠ¨ä½œåˆ†å¸ƒ ---
        log_std = torch.clamp(self.log_std_param, self.log_std_min, self.log_std_max)
        std = torch.exp(log_std).expand_as(mu)
        # åˆ›å»ºè¿ç»­åŠ¨ä½œçš„æ­£æ€åˆ†å¸ƒ
        continuous_base_dist = Normal(mu, std)
        # åˆ›å»ºç¦»æ•£åŠ¨ä½œçš„åˆ†å¸ƒ
        trigger_dist = Bernoulli(logits=trigger_logits_masked.squeeze(-1)) # ä¼¯åŠªåˆ©åˆ†å¸ƒ
        salvo_size_dist = Categorical(logits=salvo_size_logits) # åˆ†ç±»åˆ†å¸ƒ
        intra_interval_dist = Categorical(logits=intra_interval_logits)
        num_groups_dist = Categorical(logits=num_groups_logits)
        inter_interval_dist = Categorical(logits=inter_interval_logits)
        # å°†æ‰€æœ‰åˆ†å¸ƒæ‰“åŒ…æˆä¸€ä¸ªå­—å…¸è¿”å›
        distributions = {
            'continuous': continuous_base_dist,
            'trigger': trigger_dist,
            'salvo_size': salvo_size_dist,
            'intra_interval': intra_interval_dist,
            'num_groups': num_groups_dist,
            'inter_interval': inter_interval_dist
        }
        return distributions


class Critic(Module):
    # ... åŸç‰ˆ Critic ä»£ç ä¿æŒä¸å˜ ...
    """
       Critic ç½‘ç»œ (ä»·å€¼ç½‘ç»œ)ï¼Œè¯„ä¼°çŠ¶æ€çš„ä»·å€¼ V(s)ã€‚
       è¿™æ˜¯ä¸€ä¸ªæ ‡å‡†çš„ MLP æ¨¡å‹ï¼Œè´Ÿè´£é¢„æµ‹è¾“å…¥çŠ¶æ€çš„æœŸæœ›å›æŠ¥ã€‚
       """

    def __init__(self):
        super(Critic, self).__init__()
        self.input_dim = CRITIC_PARA.input_dim
        self.output_dim = CRITIC_PARA.output_dim
        self.init_model()
        self.optim = torch.optim.Adam(self.parameters(), CRITIC_PARA.lr)
        self.critic_scheduler = lr_scheduler.LinearLR(
            self.optim,
            start_factor=1.0,
            end_factor=AGENTPARA.mini_lr / CRITIC_PARA.lr,
            total_iters=AGENTPARA.MAX_EXE_NUM
        )
        self.to(CRITIC_PARA.device)

    def init_model(self):
        """åˆå§‹åŒ– Critic çš„ç½‘ç»œç»“æ„ã€‚"""
        self.network = Sequential()
        layers_dims = [self.input_dim] + CRITIC_PARA.model_layer_dim
        for i in range(len(layers_dims) - 1):
            self.network.add_module(f'fc_{i}', Linear(layers_dims[i], layers_dims[i + 1]))
            self.network.add_module(f'LayerNorm_{i}', LayerNorm(layers_dims[i + 1]))
            self.network.add_module(f'LeakyReLU_{i}', LeakyReLU())
        # è¾“å‡ºå±‚ï¼Œè¾“å‡ºä¸€ä¸ªæ ‡é‡å€¼ï¼Œå³çŠ¶æ€ä»·å€¼ V(s)
        self.network.add_module('fc_out', Linear(layers_dims[-1], self.output_dim))

    def forward(self, obs):
        """å‰å‘ä¼ æ’­ï¼Œè®¡ç®—çŠ¶æ€ä»·å€¼ã€‚"""
        obs = check(obs).to(**CRITIC_PARA.tpdv)
        value = self.network(obs)
        return value

# ==============================================================================
# <<< GRU/RNN ä¿®æ”¹ >>>: å®šä¹‰æ–°çš„åŸºäº GRU çš„ Actor å’Œ Critic
#                       [ğŸ’¥ æ–°ç»“æ„: GRU -> MLP -> Heads]
# ==============================================================================

def init_weights(m, gain=1.0):
    """
    ä¸€ä¸ªé€šç”¨çš„æƒé‡åˆå§‹åŒ–å‡½æ•°ã€‚
    :param m: PyTorch module
    :param gain: æ­£äº¤åˆå§‹åŒ–çš„å¢ç›Šå› å­
    """
    if isinstance(m, Linear):
        # å¯¹çº¿æ€§å±‚ä½¿ç”¨ Kaiming Normal åˆå§‹åŒ–ï¼Œé€‚ç”¨äº LeakyReLU
        torch.nn.init.kaiming_normal_(m.weight, a=0.01, mode='fan_in', nonlinearity='leaky_relu')
        if m.bias is not None:
            torch.nn.init.constant_(m.bias, 0)
    elif isinstance(m, GRU):
        # å¯¹ GRU çš„æƒé‡ä½¿ç”¨æ­£äº¤åˆå§‹åŒ–ï¼Œè¿™æ˜¯ RNN çš„æœ€ä½³å®è·µ
        for name, param in m.named_parameters():
            if 'weight_ih' in name:
                torch.nn.init.orthogonal_(param.data, gain=gain)
            elif 'weight_hh' in name:
                torch.nn.init.orthogonal_(param.data, gain=gain)
            elif 'bias' in name:
                param.data.fill_(0)
# ==============================================================================
# <<< GRU/RNN ä¿®æ”¹ >>>: å®šä¹‰æ–°çš„åŸºäº GRU çš„ Actor å’Œ Critic
#                       [ğŸ’¥ æ–°ç»“æ„: GRU -> MLP -> Heads]
# ==============================================================================

class Actor_GRU(Module):
    """
        Actor ç½‘ç»œ (ç­–ç•¥ç½‘ç»œ) - åŸºäº GRU çš„ç‰ˆæœ¬ã€‚
        ç»“æ„ä¸º: MLP ç‰¹å¾æå– -> GRU åºåˆ—å¤„ç† -> ç‹¬ç«‹åŠ¨ä½œå¤´ã€‚
        è¿™ç§ç»“æ„èƒ½å¤Ÿæ•æ‰çŠ¶æ€åºåˆ—ä¸­çš„æ—¶é—´ä¾èµ–å…³ç³»ã€‚
        [ğŸ’¥ ä¿®æ”¹] è¿ç»­åŠ¨ä½œéƒ¨åˆ†ï¼šmu å¤´çŠ¶æ€ä¾èµ–ï¼Œlog_std ä¸ºå…¨å±€å¯å­¦ä¹ å‚æ•°ã€‚
        """
    """
        Actor ç½‘ç»œ (ç­–ç•¥ç½‘ç»œ) - [æ–°ç»“æ„: GRU -> MLP]
        ç»“æ„ä¸º: GRU åºåˆ—å¤„ç† -> MLP ç‰¹å¾æå– -> ç‹¬ç«‹åŠ¨ä½œå¤´ã€‚
        """
    def __init__(self):
        super(Actor_GRU, self).__init__()
        self.input_dim = ACTOR_PARA.input_dim
        self.log_std_min = -20.0
        self.log_std_max = 2.0
        self.rnn_hidden_size = RNN_HIDDEN_SIZE

        # 1. GRU å±‚ä½œä¸ºç¬¬ä¸€å±‚ï¼Œç›´æ¥å¤„ç†åŸå§‹çŠ¶æ€è¾“å…¥
        self.gru = GRU(self.input_dim, self.rnn_hidden_size, batch_first=True)

        # 2. å…±äº«çš„ MLP éª¨å¹²ç½‘ç»œï¼Œæ¥æ”¶ GRU çš„è¾“å‡º
        # MLPçš„è¾“å…¥ç»´åº¦æ˜¯ GRU çš„éšè—å±‚å¤§å°
        # è¿™ä¸ª MLP è´Ÿè´£å¯¹æ¯ä¸ªæ—¶é—´æ­¥çš„çŠ¶æ€è¿›è¡Œç‹¬ç«‹çš„ç‰¹å¾æå–ã€‚
        shared_layers_dims = [self.rnn_hidden_size] + ACTOR_PARA.model_layer_dim
        self.shared_network = Sequential()
        for i in range(len(shared_layers_dims) - 1):
            self.shared_network.add_module(f'fc_{i}', Linear(shared_layers_dims[i], shared_layers_dims[i + 1]))
            self.shared_network.add_module(f'LayerNorm_{i}', LayerNorm(shared_layers_dims[i + 1]))
            self.shared_network.add_module(f'LeakyReLU_{i}', LeakyReLU())

        shared_output_dim = ACTOR_PARA.model_layer_dim[-1]

        # MLP çš„è¾“å‡ºç»´åº¦
        mlp_output_dim = ACTOR_PARA.model_layer_dim[-1]

        # 3. ç‹¬ç«‹å¤´éƒ¨ç½‘ç»œ ï¼Œæ¥æ”¶ MLP çš„è¾“å‡º
        # å¤´éƒ¨ç½‘ç»œæ¥æ”¶ GRU çš„è¾“å‡ºï¼Œå¹¶ç”Ÿæˆæœ€ç»ˆçš„åŠ¨ä½œåˆ†å¸ƒå‚æ•°ã€‚
        # self.continuous_head = Linear(self.rnn_hidden_size, CONTINUOUS_DIM * 2)

        # mu å¤´éƒ¨ï¼Œæ¥æ”¶ GRU çš„è¾“å‡º
        self.mu_head = Linear(mlp_output_dim, CONTINUOUS_DIM)
        # log_std ä½œä¸ºç‹¬ç«‹çš„ã€ä¸çŠ¶æ€æ— å…³çš„å¯å­¦ä¹ å‚æ•°
        self.log_std_param = torch.nn.Parameter(torch.zeros(1, CONTINUOUS_DIM) * -0.5)
        self.discrete_head = Linear(mlp_output_dim, TOTAL_DISCRETE_LOGITS)

        # --- [æ–°å¢] åº”ç”¨åˆå§‹åŒ– ---
        self.apply(init_weights)  # å¯¹æ‰€æœ‰å­æ¨¡å—åº”ç”¨é€šç”¨åˆå§‹åŒ–

        # --- [æ–°å¢] å¯¹è¾“å‡ºå±‚è¿›è¡Œç‰¹æ®Šåˆå§‹åŒ– ---
        # è¿™æ ·åšæ˜¯ä¸ºäº†åœ¨è®­ç»ƒå¼€å§‹æ—¶æœ‰æ›´ç¨³å®šã€æ›´å…·æ¢ç´¢æ€§çš„ç­–ç•¥
        init_range = 3e-3
        self.mu_head.weight.data.uniform_(-init_range, init_range)
        self.mu_head.bias.data.fill_(0)
        self.discrete_head.weight.data.uniform_(-init_range, init_range)
        self.discrete_head.bias.data.fill_(0)
        # --- åˆå§‹åŒ–ç»“æŸ ---

        # ä¼˜åŒ–å™¨ç­‰è®¾ç½®
        self.optim = torch.optim.Adam(self.parameters(), ACTOR_PARA.lr)
        self.actor_scheduler = lr_scheduler.LinearLR(self.optim, start_factor=1.0,
                                                     end_factor=AGENTPARA.mini_lr / ACTOR_PARA.lr,
                                                     total_iters=AGENTPARA.MAX_EXE_NUM)
        self.to(ACTOR_PARA.device)

    def forward(self, obs, hidden_state):
        """
        GRU Actor çš„å‰å‘ä¼ æ’­ã€‚
        è¿™ä¸ªæ–¹æ³•è¢«è®¾è®¡ä¸ºå¯ä»¥åŒæ—¶å¤„ç†å•ä¸ªæ—¶é—´æ­¥çš„è¾“å…¥ï¼ˆç”¨äºä¸ç¯å¢ƒäº¤äº’ï¼‰å’Œåºåˆ—è¾“å…¥ï¼ˆç”¨äºè®­ç»ƒï¼‰ã€‚
        Args:
            obs (Tensor): è§‚æµ‹å€¼ã€‚å½¢çŠ¶å¯ä»¥æ˜¯ (batch, features) ç”¨äºå•æ­¥ï¼Œæˆ– (batch, seq_len, features) ç”¨äºåºåˆ—ã€‚
            hidden_state (Tensor): GRU çš„éšè—çŠ¶æ€ã€‚å½¢çŠ¶æ˜¯ (num_layers=1, batch, rnn_hidden_size)ã€‚
        Returns:
            tuple: (åŒ…å«æ‰€æœ‰åŠ¨ä½œåˆ†å¸ƒçš„å­—å…¸, æ–°çš„éšè—çŠ¶æ€)
        """
        obs_tensor = check(obs).to(**ACTOR_PARA.tpdv)
        # æ£€æŸ¥è¾“å…¥æ˜¯å•ä¸ªæ—¶é—´æ­¥è¿˜æ˜¯åºåˆ—ï¼Œé€šè¿‡åˆ¤æ–­å¼ é‡çš„ç»´åº¦
        is_sequence = obs_tensor.dim() == 3

        # ç»Ÿä¸€å¤„ç†è¾“å…¥å½¢çŠ¶ï¼Œä½¿å…¶ç¬¦åˆ GRU çš„è¾“å…¥è¦æ±‚ (batch, seq_len, features)
        if not is_sequence:
            # å¦‚æœæ˜¯å•æ­¥ (batch_size, features)ï¼Œå¢åŠ ä¸€ä¸ª seq_len=1 çš„ç»´åº¦
            obs_tensor = obs_tensor.unsqueeze(1)  # -> (batch_size, 1, features)

        # # 1. é€šè¿‡å…±äº« MLP æå–ç‰¹å¾
        # # æ³¨æ„ï¼šMLP åªèƒ½å¤„ç† (N, *, H_in) å½¢çŠ¶ï¼Œæ‰€ä»¥å¦‚æœè¾“å…¥æ˜¯åºåˆ—ï¼Œå®ƒä¼šç‹¬ç«‹åœ°å¤„ç†æ¯ä¸ªæ—¶é—´æ­¥
        # shared_features = self.shared_network(obs_tensor)
        #
        # # 2. å°†ç‰¹å¾åºåˆ—å’Œéšè—çŠ¶æ€é€å…¥ GRU
        # # gru_out å½¢çŠ¶: (batch, seq_len, rnn_hidden_size)
        # # new_hidden å½¢çŠ¶: (1, batch, rnn_hidden_size)
        # gru_out, new_hidden = self.gru(shared_features, hidden_state)
        #
        # # å¦‚æœåŸå§‹è¾“å…¥æ˜¯å•æ­¥ï¼Œæˆ‘ä»¬ä¹Ÿå¸Œæœ›è¾“å‡ºæ˜¯å•æ­¥çš„
        # if not is_sequence:
        #     gru_out = gru_out.squeeze(1)  # -> (batch_size, rnn_hidden_size)
        #
        # # 3. å°† GRU çš„è¾“å‡ºé€å…¥å„ä¸ªå¤´
        # # cont_params = self.continuous_head(gru_out)
        # mu = self.mu_head(gru_out)
        # all_disc_logits = self.discrete_head(gru_out)

        # 1. [æ–°æµç¨‹] åŸå§‹çŠ¶æ€åºåˆ—é¦–å…ˆé€šè¿‡ GRU
        gru_out, new_hidden = self.gru(obs_tensor, hidden_state)

        # 2. [æ–°æµç¨‹] GRU çš„è¾“å‡ºï¼ˆè®°å¿†å‘é‡ï¼‰å†é€šè¿‡ MLP è¿›è¡Œç‰¹å¾æå–
        shared_features = self.shared_network(gru_out)

        # å¦‚æœåŸå§‹è¾“å…¥æ˜¯å•æ­¥ï¼Œæˆ‘ä»¬ä¹Ÿå¸Œæœ›æœ€ç»ˆç”¨äºå¤´éƒ¨çš„ç‰¹å¾æ˜¯å•æ­¥çš„
        if not is_sequence:
            shared_features = shared_features.squeeze(1)

        # 3. [æ–°æµç¨‹] å°† MLP çš„è¾“å‡ºé€å…¥å„ä¸ªå¤´
        mu = self.mu_head(shared_features)
        all_disc_logits = self.discrete_head(shared_features)

        # åç»­çš„åˆ†å¸ƒåˆ›å»ºé€»è¾‘ä¸åŸç‰ˆ Actor å®Œå…¨ç›¸åŒ
        split_sizes = list(DISCRETE_DIMS.values())
        logits_parts = torch.split(all_disc_logits, split_sizes, dim=-1)
        trigger_logits, salvo_size_logits, intra_interval_logits, num_groups_logits, inter_interval_logits = logits_parts
        # å…³é”®ï¼šåŠ¨ä½œæ©ç ä¾èµ–äºåŸå§‹è¾“å…¥ obs_tensorï¼Œè€Œä¸æ˜¯ GRU çš„è¾“å‡º
        # åŠ¨ä½œæ©ç é€»è¾‘ (éœ€è¦æ³¨æ„åœ¨åºåˆ—æƒ…å†µä¸‹æ­£ç¡®ç´¢å¼•)
        # obs_tensor æ­¤æ—¶å¯èƒ½æ˜¯ (batch, seq_len, features) æˆ– (batch, features)
        # ä½¿ç”¨ ... (Ellipsis) å¯ä»¥ä¼˜é›…åœ°å¤„ç†è¿™ä¸¤ç§æƒ…å†µï¼Œå®ƒä»£è¡¨ä»»æ„æ•°é‡çš„å‰å¯¼ç»´åº¦ã€‚
        has_flares_info = obs_tensor[..., 7]  # ä½¿ç”¨ ... æ¥å¤„ç†å•æ­¥å’Œåºåˆ—ä¸¤ç§æƒ…å†µ
        mask = (has_flares_info == 0)
        trigger_logits_masked = trigger_logits.clone()
        if torch.any(mask):
            # unsqueeze a dim to match the mask shape with trigger_logits_masked if they are different
            if mask.dim() < trigger_logits_masked.dim():
                mask = mask.unsqueeze(-1)
            trigger_logits_masked[mask] = torch.finfo(torch.float32).min

        # mu, log_std = cont_params.chunk(2, dim=-1)
        # log_std = torch.clamp(log_std, self.log_std_min, self.log_std_max)
        # std = torch.exp(log_std)
        # --- ğŸ’¥ [ä¿®æ”¹] 5. åˆ›å»ºè¿ç»­åŠ¨ä½œåˆ†å¸ƒ ---
        # ä½¿ç”¨å…¨å±€å¯å­¦ä¹ çš„ log_std å‚æ•°
        log_std = torch.clamp(self.log_std_param, self.log_std_min, self.log_std_max)
        # ä½¿ç”¨ .expand_as(mu) æ¥åŒ¹é…æ‰¹æ¬¡å’Œåºåˆ—ç»´åº¦
        std = torch.exp(log_std).expand_as(mu)
        continuous_base_dist = Normal(mu, std)

        # Bernoulli çš„ logits éœ€è¦ç§»é™¤æœ€åä¸€ä¸ªç»´åº¦ï¼ˆå¦‚æœå®ƒå­˜åœ¨ä¸”ä¸º1ï¼‰
        trigger_dist = Bernoulli(logits=trigger_logits_masked.squeeze(-1))
        salvo_size_dist = Categorical(logits=salvo_size_logits)
        intra_interval_dist = Categorical(logits=intra_interval_logits)
        num_groups_dist = Categorical(logits=num_groups_logits)
        inter_interval_dist = Categorical(logits=inter_interval_logits)

        distributions = {
            'continuous': continuous_base_dist,
            'trigger': trigger_dist,
            'salvo_size': salvo_size_dist,
            'intra_interval': intra_interval_dist,
            'num_groups': num_groups_dist,
            'inter_interval': inter_interval_dist
        }

        return distributions, new_hidden


class Critic_GRU(Module):
    """
    Critic ç½‘ç»œ (ä»·å€¼ç½‘ç»œ) - åŸºäº GRU çš„ç‰ˆæœ¬ã€‚
    ç»“æ„ä¸º: MLP ç‰¹å¾æå– -> GRU åºåˆ—å¤„ç† -> è¾“å‡ºå¤´ã€‚
    ç”¨äºè¯„ä¼°çŠ¶æ€åºåˆ—çš„ä»·å€¼ã€‚
    """
    """
       Critic ç½‘ç»œ (ä»·å€¼ç½‘ç»œ) - [æ–°ç»“æ„: GRU -> MLP]
       ç»“æ„ä¸º: GRU åºåˆ—å¤„ç† -> MLP ç‰¹å¾æå– -> è¾“å‡ºå¤´ã€‚
       """
    def __init__(self):
        super(Critic_GRU, self).__init__()
        self.input_dim = CRITIC_PARA.input_dim
        self.output_dim = CRITIC_PARA.output_dim
        self.rnn_hidden_size = RNN_HIDDEN_SIZE

        # # 1. MLP éª¨å¹²ç½‘ç»œ (ä¸åŸç‰ˆ Critic ç±»ä¼¼)
        # self.network_base = Sequential()
        # layers_dims = [self.input_dim] + CRITIC_PARA.model_layer_dim
        # for i in range(len(layers_dims) - 1):
        #     self.network_base.add_module(f'fc_{i}', Linear(layers_dims[i], layers_dims[i + 1]))
        #     self.network_base.add_module(f'LayerNorm_{i}', LayerNorm(layers_dims[i + 1]))
        #     self.network_base.add_module(f'LeakyReLU_{i}', LeakyReLU())
        #
        # base_output_dim = CRITIC_PARA.model_layer_dim[-1]
        #
        # # 2. GRU å±‚
        # self.gru = GRU(base_output_dim, self.rnn_hidden_size, batch_first=True)
        #
        # # 3. è¾“å‡ºå¤´ï¼Œå°† GRU çš„è¾“å‡ºæ˜ å°„åˆ°æœ€ç»ˆçš„ä»·å€¼ V(s)
        # self.fc_out = Linear(self.rnn_hidden_size, self.output_dim)

        # 1. GRU å±‚ä½œä¸ºç¬¬ä¸€å±‚
        self.gru = GRU(self.input_dim, self.rnn_hidden_size, batch_first=True)

        # 2. MLP éª¨å¹²ç½‘ç»œï¼Œæ¥æ”¶ GRU çš„è¾“å‡º
        # MLPçš„è¾“å…¥ç»´åº¦æ˜¯ GRU çš„éšè—å±‚å¤§å°
        layers_dims = [self.rnn_hidden_size] + CRITIC_PARA.model_layer_dim
        self.network_base = Sequential()
        for i in range(len(layers_dims) - 1):
            self.network_base.add_module(f'fc_{i}', Linear(layers_dims[i], layers_dims[i + 1]))
            self.network_base.add_module(f'LayerNorm_{i}', LayerNorm(layers_dims[i + 1]))
            self.network_base.add_module(f'LeakyReLU_{i}', LeakyReLU())

        # MLP çš„è¾“å‡ºç»´åº¦
        base_output_dim = CRITIC_PARA.model_layer_dim[-1]

        # 3. è¾“å‡ºå¤´ï¼Œæ¥æ”¶ MLP çš„è¾“å‡º
        self.fc_out = Linear(base_output_dim, self.output_dim)

        # --- [æ–°å¢] åº”ç”¨åˆå§‹åŒ– ---
        self.apply(init_weights)  # å¯¹æ‰€æœ‰å­æ¨¡å—åº”ç”¨é€šç”¨åˆå§‹åŒ–

        # --- [æ–°å¢] å¯¹è¾“å‡ºå±‚è¿›è¡Œç‰¹æ®Šåˆå§‹åŒ– ---
        # è¿™æ ·åšæ˜¯ä¸ºäº†åœ¨è®­ç»ƒå¼€å§‹æ—¶æœ‰æ›´ç¨³å®šçš„ä»·å€¼ä¼°è®¡
        init_range = 3e-3
        self.fc_out.weight.data.uniform_(-init_range, init_range)
        self.fc_out.bias.data.fill_(0)
        # --- åˆå§‹åŒ–ç»“æŸ ---

        # ä¼˜åŒ–å™¨ç­‰è®¾ç½®
        self.optim = torch.optim.Adam(self.parameters(), CRITIC_PARA.lr)
        self.critic_scheduler = lr_scheduler.LinearLR(self.optim, start_factor=1.0,
                                                      end_factor=AGENTPARA.mini_lr / CRITIC_PARA.lr,
                                                      total_iters=AGENTPARA.MAX_EXE_NUM)
        self.to(CRITIC_PARA.device)

    def forward(self, obs, hidden_state):
        """
        GRU Critic çš„å‰å‘ä¼ æ’­ã€‚
        åŒæ ·æ”¯æŒå•æ­¥å’Œåºåˆ—è¾“å…¥ã€‚
        """
        obs_tensor = check(obs).to(**CRITIC_PARA.tpdv)
        is_sequence = obs_tensor.dim() == 3

        if not is_sequence:
            obs_tensor = obs_tensor.unsqueeze(1)
        # # 1. MLP ç‰¹å¾æå–
        # base_features = self.network_base(obs_tensor)
        # # 2. GRU åºåˆ—å¤„ç†
        # gru_out, new_hidden = self.gru(base_features, hidden_state)
        #
        # if not is_sequence:
        #     gru_out = gru_out.squeeze(1)
        # # 3. è¾“å‡ºå¤´è®¡ç®—ä»·å€¼
        # value = self.fc_out(gru_out)

        # 1. [æ–°æµç¨‹] åŸå§‹çŠ¶æ€åºåˆ—é¦–å…ˆé€šè¿‡ GRU
        gru_out, new_hidden = self.gru(obs_tensor, hidden_state)

        # 2. [æ–°æµç¨‹] GRU çš„è¾“å‡ºå†é€šè¿‡ MLP
        base_features = self.network_base(gru_out)

        if not is_sequence:
            base_features = base_features.squeeze(1)

        # 3. [æ–°æµç¨‹] MLP çš„è¾“å‡ºé€å…¥è¾“å‡ºå¤´è®¡ç®—ä»·å€¼
        value = self.fc_out(base_features)
        return value, new_hidden


# ==============================================================================
# PPO Agent ä¸»ç±»
# ==============================================================================

class PPO_continuous(object):
    """
    PPO æ™ºèƒ½ä½“ä¸»ç±»ã€‚
    é€šè¿‡ `use_rnn` æ ‡å¿—æ¥å†³å®šæ˜¯ä½¿ç”¨ MLP æ¨¡å‹è¿˜æ˜¯ GRU æ¨¡å‹ã€‚
    """
    def __init__(self, load_able: bool, model_dir_path: str = None, use_rnn: bool = False):
        super(PPO_continuous, self).__init__()
        # æ ¹æ® use_rnn æ ‡å¿—ï¼Œå®ä¾‹åŒ–å¯¹åº”çš„ Actor å’Œ Critic ç½‘ç»œ
        self.use_rnn = use_rnn
        if self.use_rnn:
            print("--- åˆå§‹åŒ– PPO Agent (ä½¿ç”¨ GRU æ¨¡å‹) ---")
            self.Actor = Actor_GRU()
            self.Critic = Critic_GRU()
        else:
            print("--- åˆå§‹åŒ– PPO Agent (ä½¿ç”¨ MLP æ¨¡å‹) ---")
            self.Actor = Actor()
            self.Critic = Critic()
        # å®ä¾‹åŒ–ç»éªŒå›æ”¾æ± ï¼Œå¹¶å‘ŠçŸ¥å®ƒæ˜¯å¦éœ€è¦å¤„ç† RNN éšè—çŠ¶æ€
        self.buffer = Buffer(use_rnn=self.use_rnn)
        # ä»é…ç½®ä¸­åŠ è½½ PPO ç®—æ³•çš„è¶…å‚æ•°
        self.gamma = AGENTPARA.gamma
        self.gae_lambda = AGENTPARA.lamda
        self.ppo_epoch = AGENTPARA.ppo_epoch
        self.total_steps = 0
        self.training_start_time = time.strftime("PPOGRU_%Y-%m-%d_%H-%M-%S")
        self.base_save_dir = "../../save/save_evade_fuza"
        # ä¸ºæœ¬æ¬¡è¿è¡Œåˆ›å»ºä¸€ä¸ªå”¯ä¸€çš„å­˜æ¡£æ–‡ä»¶å¤¹
        self.run_save_dir = os.path.join(self.base_save_dir, self.training_start_time)
        # å¦‚æœéœ€è¦åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        if load_able:
            if model_dir_path:
                print(f"--- æ­£åœ¨ä»æŒ‡å®šæ–‡ä»¶å¤¹åŠ è½½æ¨¡å‹: {model_dir_path} ---")
                self.load_models_from_directory(model_dir_path)
            else:
                print("--- æœªæŒ‡å®šæ¨¡å‹æ–‡ä»¶å¤¹ï¼Œå°è¯•ä»é»˜è®¤æ–‡ä»¶å¤¹ 'test' åŠ è½½ ---")
                self.load_models_from_directory("../../test/test_evade")

    def load_models_from_directory(self, directory_path: str):
        # This function is correct, no changes needed.
        """ä»æŒ‡å®šç›®å½•åŠ è½½ Actor å’Œ Critic æ¨¡å‹çš„æƒé‡ã€‚"""
        if not os.path.isdir(directory_path):
            print(f"[é”™è¯¯] æ¨¡å‹åŠ è½½å¤±è´¥ï¼šæä¾›çš„è·¯å¾„ '{directory_path}' ä¸æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„æ–‡ä»¶å¤¹ã€‚")
            return
        files = os.listdir(directory_path)
        # ä¼˜å…ˆå°è¯•åŠ è½½å¸¦æœ‰å‰ç¼€çš„æ¨¡å‹æ–‡ä»¶ (ä¾‹å¦‚ "1000_Actor.pkl")
        actor_files_with_prefix = [f for f in files if f.endswith("_Actor.pkl")]
        if len(actor_files_with_prefix) > 0:
            actor_filename = actor_files_with_prefix[0]
            prefix = actor_filename.replace("_Actor.pkl", "")
            critic_filename = f"{prefix}_Critic.pkl"
            print(f"  - æ£€æµ‹åˆ°å‰ç¼€ '{prefix}'ï¼Œå‡†å¤‡åŠ è½½æ¨¡å‹...")
            if critic_filename in files:
                actor_full_path = os.path.join(directory_path, actor_filename)
                critic_full_path = os.path.join(directory_path, critic_filename)
                try:
                    self.Actor.load_state_dict(torch.load(actor_full_path, map_location=ACTOR_PARA.device))
                    print(f"    - æˆåŠŸåŠ è½½ Actor: {actor_full_path}")
                    self.Critic.load_state_dict(torch.load(critic_full_path, map_location=CRITIC_PARA.device))
                    print(f"    - æˆåŠŸåŠ è½½ Critic: {critic_full_path}")
                    return
                except Exception as e:
                    print(f"    - [é”™è¯¯] åŠ è½½å¸¦å‰ç¼€çš„æ¨¡å‹æ—¶å¤±è´¥: {e}")
            else:
                print(f"    - [è­¦å‘Š] æ‰¾åˆ°äº† '{actor_filename}' ä½†æœªæ‰¾åˆ°å¯¹åº”çš„ '{critic_filename}'ã€‚")
        # å¦‚æœæ²¡æœ‰å¸¦å‰ç¼€çš„æ–‡ä»¶ï¼Œåˆ™å°è¯•åŠ è½½é»˜è®¤çš„ "Actor.pkl" å’Œ "Critic.pkl"
        if "Actor.pkl" in files and "Critic.pkl" in files:
            print("  - æ£€æµ‹åˆ°æ— å‰ç¼€æ ¼å¼ï¼Œå‡†å¤‡åŠ è½½ 'Actor.pkl' å’Œ 'Critic.pkl'...")
            actor_full_path = os.path.join(directory_path, "Actor.pkl")
            critic_full_path = os.path.join(directory_path, "Critic.pkl")
            try:
                self.Actor.load_state_dict(torch.load(actor_full_path, map_location=ACTOR_PARA.device))
                print(f"    - æˆåŠŸåŠ è½½ Actor: {actor_full_path}")
                self.Critic.load_state_dict(torch.load(critic_full_path, map_location=CRITIC_PARA.device))
                print(f"    - æˆåŠŸåŠ è½½ Critic: {critic_full_path}")
                return
            except Exception as e:
                print(f"    - [é”™è¯¯] åŠ è½½æ— å‰ç¼€æ¨¡å‹æ—¶å¤±è´¥: {e}")
        print(f"[é”™è¯¯] æ¨¡å‹åŠ è½½å¤±è´¥ï¼šåœ¨æ–‡ä»¶å¤¹ '{directory_path}' ä¸­æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„ Actor/Critic æ¨¡å‹å¯¹ã€‚")

    def scale_action(self, action_cont_tanh):
        """å°† tanh è¾“å‡ºçš„è¿ç»­åŠ¨ä½œ (-1, 1) ç¼©æ”¾åˆ°ç¯å¢ƒå®šä¹‰çš„å®é™…èŒƒå›´ã€‚"""
        lows = torch.tensor([ACTION_RANGES[k]['low'] for k in CONTINUOUS_ACTION_KEYS], **ACTOR_PARA.tpdv)
        highs = torch.tensor([ACTION_RANGES[k]['high'] for k in CONTINUOUS_ACTION_KEYS], **ACTOR_PARA.tpdv)
        scaled_action = lows + (action_cont_tanh + 1.0) * 0.5 * (highs - lows)
        return scaled_action

    def get_initial_hidden_states(self, batch_size=1):
        """ä¸º GRU æ¨¡å‹ç”Ÿæˆåˆå§‹çš„é›¶éšè—çŠ¶æ€ã€‚"""
        if not self.use_rnn:
            return None, None
        actor_hidden = torch.zeros((1, batch_size, self.Actor.rnn_hidden_size), device=ACTOR_PARA.device)
        critic_hidden = torch.zeros((1, batch_size, self.Critic.rnn_hidden_size), device=CRITIC_PARA.device)
        return actor_hidden, critic_hidden

    def store_experience(self, state, action, probs, value, reward, done, actor_hidden=None, critic_hidden=None):
        """å°†å•æ­¥ç»éªŒå­˜å‚¨åˆ° Buffer ä¸­ã€‚"""
        # æ£€æŸ¥ log_prob æ˜¯å¦åŒ…å«æ— æ•ˆå€¼ï¼ˆNaN æˆ– Infï¼‰
        if not np.all(np.isfinite(probs)):
            raise ValueError("åœ¨ log_prob ä¸­æ£€æµ‹åˆ° NaN/Infï¼")
        # å¦‚æœä½¿ç”¨ RNNï¼Œå¿…é¡»æä¾›éšè—çŠ¶æ€
        if self.use_rnn and (actor_hidden is None or critic_hidden is None):
            raise ValueError("ä½¿ç”¨ RNN æ¨¡å‹æ—¶å¿…é¡»å­˜å‚¨éšè—çŠ¶æ€ï¼")
        self.buffer.store_transition(state, value, action, probs, reward, done, actor_hidden, critic_hidden)

    def choose_action(self, state, actor_hidden, critic_hidden, deterministic=False):
        """
        æ ¹æ®å½“å‰çŠ¶æ€é€‰æ‹©åŠ¨ä½œã€‚
        :param deterministic: å¦‚æœä¸º Trueï¼Œåˆ™é€‰æ‹©æœ€å¯èƒ½çš„åŠ¨ä½œï¼ˆç”¨äºè¯„ä¼°ï¼‰ï¼Œå¦åˆ™è¿›è¡Œéšæœºé‡‡æ ·ï¼ˆç”¨äºè®­ç»ƒï¼‰ã€‚
        :return: A tuple containing the action for the environment, action to store in buffer,
                 log probability, state value, and new hidden states.
        """
        state_tensor = torch.as_tensor(state, dtype=torch.float32, device=ACTOR_PARA.device)
        # æ£€æŸ¥è¾“å…¥æ˜¯å¦æ˜¯æ‰¹å¤„ç†æ•°æ®
        is_batch = state_tensor.dim() > 1
        if not is_batch:
            state_tensor = state_tensor.unsqueeze(0)

        with torch.no_grad():# åœ¨æ­¤å—ä¸­ä¸è®¡ç®—æ¢¯åº¦
            if self.use_rnn:
                # GRU æ¨¡å‹éœ€è¦ä¼ å…¥éšè—çŠ¶æ€
                value, new_critic_hidden = self.Critic(state_tensor, critic_hidden)
                dists, new_actor_hidden = self.Actor(state_tensor, actor_hidden)
            else:
                # MLP æ¨¡å‹ä¸éœ€è¦éšè—çŠ¶æ€
                value = self.Critic(state_tensor)
                dists = self.Actor(state_tensor)
                new_critic_hidden, new_actor_hidden = None, None
            # ä»è¿ç»­åŠ¨ä½œåˆ†å¸ƒä¸­é‡‡æ ·
            continuous_base_dist = dists['continuous']
            # u æ˜¯æ­£æ€åˆ†å¸ƒçš„åŸå§‹æ ·æœ¬ï¼Œtanh(u) æ˜¯ä¸ºäº†é™åˆ¶èŒƒå›´å¹¶å¼•å…¥éçº¿æ€§
            u = continuous_base_dist.mean if deterministic else continuous_base_dist.rsample()
            action_cont_tanh = torch.tanh(u)
            log_prob_cont = continuous_base_dist.log_prob(u).sum(dim=-1)
            # ä»æ‰€æœ‰ç¦»æ•£åŠ¨ä½œåˆ†å¸ƒä¸­é‡‡æ ·
            sampled_actions_dict = {}
            for key, dist in dists.items():
                if key == 'continuous': continue
                if deterministic:
                    if isinstance(dist, Categorical): # åˆ†ç±»åˆ†å¸ƒï¼šå–æ¦‚ç‡æœ€å¤§çš„ç±»åˆ«
                        sampled_actions_dict[key] = torch.argmax(dist.probs, dim=-1)
                    else: # ä¼¯åŠªåˆ©åˆ†å¸ƒï¼šå–æ¦‚ç‡å¤§äº0.5çš„
                        # sampled_actions_dict[key] = (dist.probs > 0.5).float()
                        sampled_actions_dict[key] = dist.sample()
                else:
                    sampled_actions_dict[key] = dist.sample()
            # è®¡ç®—æ€»çš„å¯¹æ•°æ¦‚ç‡
            log_prob_disc = sum(dists[key].log_prob(act) for key, act in sampled_actions_dict.items())
            total_log_prob = log_prob_cont + log_prob_disc
            # å‡†å¤‡è¦å­˜å‚¨åœ¨ Buffer ä¸­çš„åŠ¨ä½œï¼ˆè¿ç»­éƒ¨åˆ†æ˜¯åŸå§‹æ ·æœ¬ uï¼Œç¦»æ•£éƒ¨åˆ†æ˜¯ç±»åˆ«ç´¢å¼•ï¼‰
            action_disc_to_store = torch.stack(list(sampled_actions_dict.values()), dim=-1).float()
            action_to_store = torch.cat([u, action_disc_to_store], dim=-1)
            # å‡†å¤‡è¦å‘é€ç»™ç¯å¢ƒçš„åŠ¨ä½œï¼ˆè¿ç»­éƒ¨åˆ†æ˜¯ç¼©æ”¾åçš„åŠ¨ä½œï¼Œç¦»æ•£éƒ¨åˆ†æ˜¯ç±»åˆ«ç´¢å¼•ï¼‰
            env_action_cont = self.scale_action(action_cont_tanh)
            final_env_action_tensor = torch.cat([env_action_cont, action_disc_to_store], dim=-1)
            # å°†æ‰€æœ‰ç»“æœè½¬æ¢ä¸º numpy æ•°ç»„
            value_np = value.cpu().numpy()
            action_to_store_np = action_to_store.cpu().numpy()
            log_prob_to_store_np = total_log_prob.cpu().numpy()
            final_env_action_np = final_env_action_tensor.cpu().numpy()
            # å¦‚æœè¾“å…¥ä¸æ˜¯æ‰¹å¤„ç†ï¼Œåˆ™ç§»é™¤æ‰¹æ¬¡ç»´åº¦
            if not is_batch:
                final_env_action_np = final_env_action_np[0]
                action_to_store_np = action_to_store_np[0]
                log_prob_to_store_np = log_prob_to_store_np[0]
                value_np = value_np[0]

        return final_env_action_np, action_to_store_np, log_prob_to_store_np, value_np, new_actor_hidden, new_critic_hidden

    def cal_gae(self, states, values, actions, probs, rewards, dones):
        """è®¡ç®—å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡ (Generalized Advantage Estimation, GAE)ã€‚"""
        advantage = np.zeros(len(rewards), dtype=np.float32)
        gae = 0
        # ä»åå‘å‰éå†è½¨è¿¹
        for t in reversed(range(len(rewards))):
            # è·å–ä¸‹ä¸€ä¸ªçŠ¶æ€çš„ä»·å€¼ï¼Œå¦‚æœæ˜¯æœ€åä¸€æ­¥ï¼Œåˆ™ä¸º 0
            next_value = values[t + 1] if t < len(rewards) - 1 else 0.0
            # done_mask ç”¨äºåœ¨å›åˆç»“æŸæ—¶åˆ‡æ–­ä»·å€¼çš„ä¼ æ’­
            done_mask = 1.0 - int(dones[t])
            # è®¡ç®— TD-error (delta)
            delta = rewards[t] + self.gamma * next_value * done_mask - values[t]
            # é€’å½’è®¡ç®— GAE
            gae = delta + self.gamma * self.gae_lambda * done_mask * gae
            advantage[t] = gae
        return advantage

    def learn(self):
        """
        æ‰§è¡Œ PPO çš„å­¦ä¹ å’Œæ›´æ–°æ­¥éª¤ã€‚å·²é€‚é… RNN æ¨¡å¼å¹¶ä¿®æ­£æ‰€æœ‰é€»è¾‘å’Œç»´åº¦é”™è¯¯ã€‚
        """
        # å¦‚æœ Buffer ä¸­çš„æ•°æ®ä¸è¶³ä¸€ä¸ªæ‰¹æ¬¡ï¼Œåˆ™è·³è¿‡å­¦ä¹ 
        if self.buffer.get_buffer_size() < BUFFERPARA.BATCH_SIZE:
            print(
                f"  [Info] Buffer size ({self.buffer.get_buffer_size()}) < batch size ({BUFFERPARA.BATCH_SIZE}). Skipping.")
            return None
        # 1. æ•°æ®å‡†å¤‡
        states, values, actions, old_probs, rewards, dones, _, __ = self.buffer.get_all_data()
        advantages = self.cal_gae(states, values, actions, old_probs, rewards, dones)
        # âœ… ç¡®ä¿ values æ˜¯ä¸€ç»´æ•°ç»„ï¼Œä¸ advantages å¯¹é½
        values = np.squeeze(values)  # ç§»é™¤å¤šä½™ç»´åº¦ï¼Œæ¯”å¦‚ (N,1) â†’ (N,)

        # âœ… ä¿è¯ returns ä¸ values å½¢çŠ¶ä¸€è‡´
        # returns (å³ G_t) æ˜¯ä¼˜åŠ¿å‡½æ•°çš„ unbiased estimator
        returns = advantages + values
        # ç”¨äºè®°å½•è®­ç»ƒè¿‡ç¨‹ä¸­çš„å„ç§æŸå¤±å’ŒæŒ‡æ ‡
        train_info = {'critic_loss': [], 'actor_loss': [], 'dist_entropy': [],'entropy_cont': [], 'adv_targ': [], 'ratio': []}
        # 2. PPO è®­ç»ƒå¾ªç¯
        for _ in range(self.ppo_epoch):
            # æ ¹æ®æ˜¯å¦ä½¿ç”¨ RNNï¼Œé€‰æ‹©ä¸åŒçš„æ‰¹æ¬¡ç”Ÿæˆå™¨
            if self.use_rnn:
                # ä¸º RNN ç”Ÿæˆè¿ç»­çš„åºåˆ—æ‰¹æ¬¡
                batch_generator = self.buffer.generate_sequence_batches(
                    SEQUENCE_LENGTH, BUFFERPARA.BATCH_SIZE, advantages, returns
                )
            else:
                # ä¸º MLP ç”Ÿæˆéšæœºçš„æ‰¹æ¬¡ç´¢å¼•
                batch_generator = self.buffer.generate_batches()

            for batch_data in batch_generator:
                # 3. æ‰¹æ¬¡æ•°æ®å¤„ç†
                if self.use_rnn:
                    # è§£åŒ… RNN çš„åºåˆ—æ‰¹æ¬¡æ•°æ®
                    state, action_batch, old_prob, advantage, return_, initial_actor_h, initial_critic_h = batch_data
                    # å°† numpy æ•°ç»„è½¬æ¢ä¸º tensor å¹¶ç§»åŠ¨åˆ°è®¾å¤‡
                    state = check(state).to(**ACTOR_PARA.tpdv)
                    action_batch = check(action_batch).to(**ACTOR_PARA.tpdv)
                    old_prob = check(old_prob).to(**ACTOR_PARA.tpdv)
                    advantage = check(advantage).to(**ACTOR_PARA.tpdv)
                    return_ = check(return_).to(**CRITIC_PARA.tpdv)
                    initial_actor_h = check(initial_actor_h).to(**ACTOR_PARA.tpdv)
                    initial_critic_h = check(initial_critic_h).to(**CRITIC_PARA.tpdv)
                else:
                    # å¤„ç† MLP çš„æ‰¹æ¬¡ç´¢å¼•
                    batch_indices = batch_data
                    state = check(states[batch_indices]).to(**ACTOR_PARA.tpdv)
                    action_batch = check(actions[batch_indices]).to(**ACTOR_PARA.tpdv)
                    old_prob = check(old_probs[batch_indices]).to(**ACTOR_PARA.tpdv).view(-1)
                    advantage = check(advantages[batch_indices]).to(**ACTOR_PARA.tpdv).view(-1, 1)
                    return_ = check(returns[batch_indices]).to(**CRITIC_PARA.tpdv).view(-1, 1)
                # ä»æ‰¹æ¬¡åŠ¨ä½œä¸­è§£æå‡ºè¿ç»­å’Œç¦»æ•£éƒ¨åˆ†
                u_from_buffer = action_batch[..., :CONTINUOUS_DIM]
                discrete_actions_from_buffer = {
                    'trigger': action_batch[..., CONTINUOUS_DIM],
                    'salvo_size': action_batch[..., CONTINUOUS_DIM + 1].long(), # ç±»åˆ«ç´¢å¼•éœ€è¦æ˜¯ long ç±»å‹
                    'intra_interval': action_batch[..., CONTINUOUS_DIM + 2].long(),
                    'num_groups': action_batch[..., CONTINUOUS_DIM + 3].long(),
                    'inter_interval': action_batch[..., CONTINUOUS_DIM + 4].long(),
                }

                # 4. Actor (ç­–ç•¥) ç½‘ç»œè®­ç»ƒ
                if self.use_rnn:
                    new_dists, _ = self.Actor(state, initial_actor_h)
                else:
                    new_dists = self.Actor(state)
                # è®¡ç®—æ–°ç­–ç•¥ä¸‹ï¼Œæ—§åŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡
                new_log_prob_cont = new_dists['continuous'].log_prob(u_from_buffer).sum(dim=-1)
                new_log_prob_disc = sum(
                    new_dists[key].log_prob(discrete_actions_from_buffer[key])
                    for key in discrete_actions_from_buffer
                )
                new_prob = new_log_prob_cont + new_log_prob_disc
                # è®¡ç®—ç­–ç•¥çš„ç†µï¼Œä»¥é¼“åŠ±æ¢ç´¢
                entropy_cont = new_dists['continuous'].entropy().sum(dim=-1)
                total_entropy = (entropy_cont + sum(
                    dist.entropy() for key, dist in new_dists.items() if key != 'continuous'
                )).mean()
                # è®¡ç®—æ–°æ—§ç­–ç•¥çš„æ¯”ç‡ (importance sampling ratio)
                log_ratio = new_prob - old_prob
                ratio = torch.exp(torch.clamp(log_ratio, -20.0, 20.0)) # clamp é˜²æ­¢æ•°å€¼æº¢å‡º
                # PPO çš„æ ¸å¿ƒï¼šClipped Surrogate Objective
                advantage_squeezed = advantage.squeeze(-1) if advantage.dim() > ratio.dim() else advantage
                surr1 = ratio * advantage_squeezed
                surr2 = torch.clamp(ratio, 1.0 - AGENTPARA.epsilon, 1.0 + AGENTPARA.epsilon) * advantage_squeezed
                # Actor çš„æŸå¤±æ˜¯è£å‰ªåçš„ç›®æ ‡å‡½æ•°çš„è´Ÿå€¼ï¼ŒåŠ ä¸Šç†µçš„æ­£åˆ™é¡¹
                actor_loss = -torch.min(surr1, surr2).mean() - AGENTPARA.entropy * total_entropy
                # åå‘ä¼ æ’­å’Œä¼˜åŒ–
                self.Actor.optim.zero_grad()
                actor_loss.backward()
                torch.nn.utils.clip_grad_norm_(self.Actor.parameters(), max_norm=1.0)  # æ¢¯åº¦è£å‰ª
                self.Actor.optim.step()

                # 5. Critic (ä»·å€¼) ç½‘ç»œè®­ç»ƒ
                if self.use_rnn:
                    new_value, _ = self.Critic(state, initial_critic_h)
                else:
                    new_value = self.Critic(state)

                # ####################################################################
                # # <<< FINAL, DEFINITIVE FIX FOR THE BROADCASTING WARNING >>>
                # ####################################################################
                # We ensure the target `return_` tensor has the same number of dimensions
                # as the network's output `new_value`.
                # ç¡®ä¿ç›®æ ‡å€¼ `return_` å’Œç½‘ç»œè¾“å‡º `new_value` çš„ç»´åº¦ä¸€è‡´ï¼Œä»¥é¿å… PyTorch çš„å¹¿æ’­è­¦å‘Šã€‚
                # ä¾‹å¦‚ï¼Œ`new_value` å¯èƒ½æ˜¯ (B, S, 1)ï¼Œè€Œ `return_` æ˜¯ (B, S)ï¼Œè¿™ä¼šå¯¼è‡´ä¸æ˜ç¡®çš„å¹¿æ’­ã€‚
                # é€šè¿‡ unsqueeze(-1) å°† `return_` å˜ä¸º (B, S, 1)ï¼Œä½¿å…¶å½¢çŠ¶å®Œå…¨åŒ¹é…ã€‚
                if new_value.dim() > return_.dim():
                    return_ = return_.unsqueeze(-1)
                # ####################################################################
                # Critic çš„æŸå¤±æ˜¯é¢„æµ‹å€¼å’ŒçœŸå®å›æŠ¥ï¼ˆreturnsï¼‰ä¹‹é—´çš„å‡æ–¹è¯¯å·®
                critic_loss = torch.nn.functional.mse_loss(new_value, return_)
                # åå‘ä¼ æ’­å’Œä¼˜åŒ–
                self.Critic.optim.zero_grad()
                critic_loss.backward()
                torch.nn.utils.clip_grad_norm_(self.Critic.parameters(), max_norm=1.0)
                self.Critic.optim.step()
                # è®°å½•è¯¥æ‰¹æ¬¡çš„è®­ç»ƒä¿¡æ¯
                train_info['critic_loss'].append(critic_loss.item())
                train_info['actor_loss'].append(actor_loss.item())
                train_info['dist_entropy'].append(total_entropy.item())
                train_info['entropy_cont'].append(entropy_cont.mean().item())
                train_info['adv_targ'].append(advantage.mean().item())
                train_info['ratio'].append(ratio.mean().item())
        # 6. æ¸…ç†å’Œè¿”å›
        self.buffer.clear_memory() # å®Œæˆä¸€è½®å­¦ä¹ åæ¸…ç©º Buffer
        # è®¡ç®—æ•´ä¸ª epoch çš„å¹³å‡æŒ‡æ ‡
        for key in train_info:
            train_info[key] = np.mean(train_info[key])
        # è®°å½•å½“å‰å­¦ä¹ ç‡
        train_info['actor_lr'] = self.Actor.optim.param_groups[0]['lr']
        train_info['critic_lr'] = self.Critic.optim.param_groups[0]['lr']
        self.save() # ä¿å­˜æ¨¡å‹
        return train_info

    def prep_training_rl(self):
        """å°† Actor å’Œ Critic ç½‘ç»œè®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼ã€‚"""
        self.Actor.train()
        self.Critic.train()

    def prep_eval_rl(self):
        """å°† Actor å’Œ Critic ç½‘ç»œè®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ã€‚"""
        self.Actor.eval()
        self.Critic.eval()

    def save(self, prefix=""):
        """ä¿å­˜ Actor å’Œ Critic æ¨¡å‹çš„æƒé‡ã€‚"""
        try:
            # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
            os.makedirs(self.run_save_dir, exist_ok=True)
            print(f"æ¨¡å‹å°†è¢«ä¿å­˜è‡³: {self.run_save_dir}")
        except Exception as e:
            print(f"åˆ›å»ºæ¨¡å‹æ–‡ä»¶å¤¹ {self.run_save_dir} å¤±è´¥: {e}")
            return
        # åˆ†åˆ«ä¿å­˜ Actor å’Œ Critic
        for net in ['Actor', 'Critic']:
            try:
                filename = f"{prefix}_{net}.pkl" if prefix else f"{net}.pkl"
                full_path = os.path.join(self.run_save_dir, filename)
                torch.save(getattr(self, net).state_dict(), full_path)
                print(f"  - {filename} ä¿å­˜æˆåŠŸã€‚")
            except Exception as e:
                print(f"  - ä¿å­˜æ¨¡å‹ {net} åˆ° {full_path} æ—¶å‘ç”Ÿé”™è¯¯: {e}")