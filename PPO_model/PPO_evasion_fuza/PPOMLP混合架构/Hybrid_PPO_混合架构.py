import torch
from torch import nn
from torch.nn import *
# <<< æ›´æ”¹ >>> å¯¼å…¥ Categorical åˆ†å¸ƒç”¨äºå¤šåˆ†ç±»ç¦»æ•£åŠ¨ä½œ
from torch.distributions import Bernoulli, Categorical
from torch.distributions import Normal
from Interference_code.PPO_model.PPO_evasion_fuza.Config import *
from Interference_code.PPO_model.PPO_evasion_fuza.Buffer import *
from torch.optim import lr_scheduler
import numpy as np
import os
import re
import time
from torch.cuda.amp import GradScaler, autocast

# --- åŠ¨ä½œç©ºé—´é…ç½® ---
# å®šä¹‰è¿ç»­åŠ¨ä½œçš„ç»´åº¦
CONTINUOUS_DIM = 4  # æ²¹é—¨, å‡é™èˆµ, å‰¯ç¿¼, æ–¹å‘èˆµ
CONTINUOUS_ACTION_KEYS = ['throttle', 'elevator', 'aileron', 'rudder']

# <<< æ›´æ”¹ >>> æ ¹æ®æ–°è¦æ±‚é‡æ–°å®šä¹‰å¤šç¦»æ•£åŠ¨ä½œç©ºé—´
# è¿™ä¸ªå­—å…¸å®šä¹‰äº†ç­–ç•¥ç½‘ç»œéœ€è¦åšå‡ºçš„æ‰€æœ‰ç¦»æ•£å†³ç­–
DISCRETE_DIMS = {
    'flare_trigger': 1,  # æ˜¯å¦æŠ•æ”¾: 1ä¸ªlogit -> Bernoulli (æ˜¯/å¦)
    'salvo_size': 3,  # ä¸€ç»„çš„æ•°é‡: 3ä¸ªlogit -> Categorical (ä¾‹å¦‚: 2, 4, 6 å‘)
    # 'intra_interval': 3,  # ç»„å†…æ¯å‘é—´éš”: 3ä¸ªlogit -> Categorical (ä¾‹å¦‚: 0.1s, 0.2s, 0.5s)
    'num_groups': 3,  # æŠ•æ”¾ç»„æ•°: 3ä¸ªlogit -> Categorical (ä¾‹å¦‚: 1, 2, 3 ç»„)
    'inter_interval': 3,  # ç»„é—´éš”:   3ä¸ªlogit -> Categorical (ä¾‹å¦‚: 0.5s, 1.0s, 2.0s)
}
# <<< æ›´æ”¹ >>> é‡æ–°è®¡ç®—ç¦»æ•£éƒ¨åˆ†æ€»å…±éœ€è¦çš„ç½‘ç»œè¾“å‡ºæ•°é‡
TOTAL_DISCRETE_LOGITS = sum(DISCRETE_DIMS.values())  # 1 + 3 + 3 + 3 + 3 = 13

# <<< æ›´æ”¹ >>> é‡æ–°è®¡ç®—å­˜å‚¨åœ¨ Buffer ä¸­åŠ¨ä½œçš„æ€»ç»´åº¦
# å­˜å‚¨çš„åŠ¨ä½œåŒ…æ‹¬ï¼š4ä¸ªè¿ç»­åŠ¨ä½œçš„åŸå§‹å€¼(u) + 5ä¸ªç¦»æ•£åŠ¨ä½œçš„é‡‡æ ·ç´¢å¼•
TOTAL_ACTION_DIM_BUFFER = CONTINUOUS_DIM + len(DISCRETE_DIMS)  # 4 + 5 = 9

# <<< æ–°å¢ >>> ç¦»æ•£åŠ¨ä½œç´¢å¼•åˆ°ç‰©ç†å€¼çš„æ˜ å°„ (éå¸¸é‡è¦ï¼)
# è¿™ä¸ªæ˜ å°„è¡¨ç”¨äºåœ¨ç¯å¢ƒç«¯å°†æ™ºèƒ½ä½“è¾“å‡ºçš„ç´¢å¼•(0, 1, 2)è½¬æ¢ä¸ºJSBSimå¯ä»¥ç†è§£çš„å®é™…å‚æ•°ã€‚
# ä¾‹å¦‚ï¼Œå¦‚æœæ™ºèƒ½ä½“ä¸º'salvo_size'é€‰æ‹©äº†ç´¢å¼•1ï¼Œç¯å¢ƒä¼šå°†å…¶è§£é‡Šä¸ºæŠ•æ”¾4å‘ã€‚
# DISCRETE_ACTION_MAP = {
#     'salvo_size': [2, 4, 6],  # index 0 -> 2å‘, index 1 -> 4å‘, index 2 -> 6å‘
#     'intra_interval': [0.1, 0.2, 0.5],  # index 0 -> 0.1s é—´éš”, ...
#     'num_groups': [1, 2, 3],  # index 0 -> 1ç»„, ...
#     'inter_interval': [0.5, 1.0, 2.0]  # index 0 -> 0.5s ç»„é—´éš”, ...
# }
DISCRETE_ACTION_MAP = {
#     'salvo_size': [2, 4, 6],          # æŠ•æ”¾æ•°é‡ï¼šä½ã€ä¸­ã€é«˜å¼ºåº¦
#     'intra_interval': [0.02, 0.04, 0.1], # ç»„å†…é—´éš”(ç§’)ï¼šå¯†é›†(æ¬ºéª—)ã€æ ‡å‡†ã€ç¨ç–
# # 'intra_interval': [0.05, 0.1, 0.2], # ç»„å†…é—´éš”(ç§’)ï¼šå¯†é›†(æ¬ºéª—)ã€æ ‡å‡†ã€ç¨ç–
#     'num_groups': [1, 2, 3],          # æŠ•æ”¾ç»„æ•°ï¼šå•æ¬¡ååº”ã€æ ‡å‡†ç¨‹åºã€æŒç»­ç¨‹åº
#     'inter_interval': [0.5, 1.0, 2.0]  # ç»„é—´é—´éš”(ç§’)ï¼šç´§æ€¥è¿ç»­ã€æ ‡å‡†è¿ç»­ã€é¢„é˜²/é®è”½
#     'salvo_size': [1, 2, 3],  # ä¿®æ”¹ä¸ºå‘å°„1ã€2ã€3æš
#     # 'intra_interval': [0.05, 0.1, 0.15],
#     'intra_interval': [0.02, 0.04, 0.08],
#     'num_groups': [1, 2, 3],
#     'inter_interval': [0.2, 0.5, 1.0]
    'salvo_size': [2, 3, 4],  # ä¿®æ”¹ä¸ºå‘å°„2ã€3ã€4æš
    # 'intra_interval': [0.05, 0.1, 0.15],
    # 'intra_interval': [0.02, 0.04, 0.06],
    'num_groups': [2, 3, 4],
    'inter_interval': [0.2, 0.4, 0.6]
}
# <<< æ–°å¢ >>> å®šä¹‰å›ºå®šçš„ç»„å†…æŠ•æ”¾é—´éš”
# FIXED_INTRA_INTERVAL = 0.05

# 'salvo_size': [2, 3, 4],  # ä¿®æ”¹ä¸ºå‘å°„2ã€3ã€4æš
#     'intra_interval': [0.05],
#     'num_groups': [2, 3, 4],
#     'inter_interval': [0.2, 0.4, 0.6]

# --- åŠ¨ä½œèŒƒå›´å®šä¹‰ (ä»…ç”¨äºè¿ç»­åŠ¨ä½œç¼©æ”¾) ---
ACTION_RANGES = {
    'throttle': {'low': 0.0, 'high': 1.0},
    'elevator': {'low': -1.0, 'high': 1.0},
    'aileron': {'low': -1.0, 'high': 1.0},
    'rudder': {'low': -1.0, 'high': 1.0},
}

def init_weights(m):
    """
    ä¸€ä¸ªé€šç”¨çš„æƒé‡åˆå§‹åŒ–å‡½æ•°ã€‚
    """
    if isinstance(m, Linear):
        # å¯¹çº¿æ€§å±‚ä½¿ç”¨ Kaiming Normal åˆå§‹åŒ–ï¼Œé€‚ç”¨äº LeakyReLU
        torch.nn.init.kaiming_normal_(m.weight, a=0.01, mode='fan_in', nonlinearity='leaky_relu')
        if m.bias is not None:
            torch.nn.init.constant_(m.bias, 0)


class Actor(Module):
    """
    Actor ç½‘ç»œ (ç­–ç•¥ç½‘ç»œ) - å·²æ›´æ–°ä»¥æ”¯æŒå¤æ‚çš„äº”éƒ¨åˆ†ç¦»æ•£åŠ¨ä½œç©ºé—´
    """

    def __init__(self):
        super(Actor, self).__init__()
        self.input_dim = ACTOR_PARA.input_dim
        # <<< æ›´æ”¹ >>> è¾“å‡ºç»´åº¦ç°åœ¨æ˜¯ (è¿ç»­*2) + æ–°çš„logitsæ€»æ•°
        # æ³¨æ„ï¼šè¿™é‡Œä¸å†éœ€è¦ä¸€ä¸ªæ€»çš„ output_dim
        # self.output_dim = (CONTINUOUS_DIM * 2) + TOTAL_DISCRETE_LOGITS
        self.log_std_min = -20.0
        self.log_std_max = 2.0

        # --- æ··åˆæ¶æ„å®šä¹‰ ---

        # 1. ä¸ºæ¯ä¸ªéƒ¨åˆ†å®šä¹‰å±‚çš„ç»´åº¦
        #    !! é‡è¦ï¼šè¯·æ ¹æ®ä½ çš„ Config.py æ¥è°ƒæ•´è¿™é‡Œçš„åˆ‡ç‰‡ !!
        #    å‡è®¾ model_layer_dim = [256, 256ï¼Œ256]ï¼Œæˆ‘ä»¬åœ¨ç¬¬2å±‚åæ‹†åˆ†
        split_point = 2  # åœ¨ç¬¬2å±‚åæ‹†åˆ†
        base_dims = ACTOR_PARA.model_layer_dim[:split_point]  # ä¾‹å¦‚: [256ï¼Œ256]
        # tower_dims = ACTOR_PARA.model_layer_dim[split_point:]  # ä¾‹å¦‚: [256]
        continuous_tower_dims = ACTOR_PARA.model_layer_dim[split_point:] #[256]  # è¿ç»­åŠ¨ä½œå¡”æ¥¼çš„ç»´åº¦
        #ã€ä¿®æ­£åçš„ä»£ç ã€‘è®©ç¦»æ•£å¡”æ¥¼çš„æ¯ä¸€å±‚ç»´åº¦éƒ½æ˜¯è¿ç»­å¡”æ¥¼å¯¹åº”å±‚çš„ä¸€åŠ
        # ä½¿ç”¨åˆ—è¡¨æ¨å¯¼å¼æ¥å®ç°
        discrete_tower_dims = continuous_tower_dims
        # discrete_tower_dims = [dim // 2 for dim in continuous_tower_dims]  # ä¾‹å¦‚: [128, 64]

        # 2. æ„å»ºå…±äº«åŸºåº§ç½‘ç»œ (Shared Base)
        self.shared_base = Sequential()
        base_input_dim = self.input_dim
        for i, dim in enumerate(base_dims):
            self.shared_base.add_module(f'base_fc_{i}', Linear(base_input_dim, dim))
            self.shared_base.add_module(f'base_leakyrelu_{i}', LeakyReLU())
            base_input_dim = dim

        # å…±äº«åŸºåº§çš„è¾“å‡ºç»´åº¦
        base_output_dim = base_dims[-1]

        # 3. æ„å»ºè¿ç»­åŠ¨ä½œå¡”æ¥¼ (Continuous Tower)
        self.continuous_tower = Sequential()
        tower_input_dim = base_output_dim
        for i, dim in enumerate(continuous_tower_dims):
            self.continuous_tower.add_module(f'cont_tower_fc_{i}', Linear(tower_input_dim, dim))
            self.continuous_tower.add_module(f'cont_tower_leakyrelu_{i}', LeakyReLU())
            tower_input_dim = dim

        # 4. æ„å»ºç¦»æ•£åŠ¨ä½œå¡”æ¥¼ (Discrete Tower)
        self.discrete_tower = Sequential()
        tower_input_dim = base_output_dim  # è¾“å…¥åŒæ ·æ¥è‡ªå…±äº«åŸºåº§
        for i, dim in enumerate(discrete_tower_dims):
            self.discrete_tower.add_module(f'disc_tower_fc_{i}', Linear(tower_input_dim, dim))
            self.discrete_tower.add_module(f'disc_tower_leakyrelu_{i}', LeakyReLU())
            tower_input_dim = dim

        # 5. å®šä¹‰æœ€ç»ˆçš„è¾“å‡ºå¤´ (Heads)
        #    å¤´éƒ¨çš„è¾“å…¥ç»´åº¦ç°åœ¨æ˜¯å®ƒä»¬å„è‡ªå¡”æ¥¼çš„è¾“å‡ºç»´åº¦
        continuous_tower_output_dim = continuous_tower_dims[-1] if continuous_tower_dims else base_output_dim
        self.continuous_head = Linear(continuous_tower_output_dim, CONTINUOUS_DIM)
        discrete_tower_output_dim = discrete_tower_dims[-1] if discrete_tower_dims else base_output_dim
        self.discrete_head = Linear(discrete_tower_output_dim, TOTAL_DISCRETE_LOGITS)

        # 6. å®šä¹‰ä¸çŠ¶æ€æ— å…³çš„è¿ç»­åŠ¨ä½œ log_std
        # self.log_std_param = torch.nn.Parameter(torch.full((1, CONTINUOUS_DIM), -0.5))
        self.log_std_param = torch.nn.Parameter(torch.full((1, CONTINUOUS_DIM), 0.0))

        # --- ä¼˜åŒ–å™¨å’Œå…¶ä»–è®¾ç½® (ä¿æŒä¸å˜) ---
        self.optim = torch.optim.Adam(self.parameters(), ACTOR_PARA.lr)
        self.actor_scheduler = lr_scheduler.LinearLR(
            self.optim,
            start_factor=1.0,
            end_factor=AGENTPARA.mini_lr / ACTOR_PARA.lr,
            total_iters=AGENTPARA.MAX_EXE_NUM
        )
        self.to(ACTOR_PARA.device)
    def init_model(self):
        """åˆå§‹åŒ–ç¥ç»ç½‘ç»œç»“æ„"""
        self.network = Sequential()
        layers_dims = [self.input_dim] + ACTOR_PARA.model_layer_dim + [self.output_dim]
        for i in range(len(layers_dims) - 1):
            self.network.add_module(f'fc_{i}', Linear(layers_dims[i], layers_dims[i + 1]))
            if i < len(layers_dims) - 2:
                self.network.add_module(f'LeakyReLU_{i}', LeakyReLU())

    def forward(self, obs):
        """
        å‰å‘ä¼ æ’­æ–¹æ³•ï¼Œä¸ºæ¯ä¸ªåŠ¨ä½œç»´åº¦åˆ›å»ºå¹¶è¿”å›ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒã€‚
        """
        obs_tensor = check(obs).to(**ACTOR_PARA.tpdv)
        if obs_tensor.dim() == 1:
            obs_tensor = obs_tensor.unsqueeze(0)

        # 1. æ•°æ®é¦–å…ˆæµç»å…±äº«åŸºåº§
        base_features = self.shared_base(obs_tensor)

        # 2. å…±äº«ç‰¹å¾è¢«åˆ†åˆ«é€å…¥ä¸¤ä¸ªä¸“ç”¨å¡”æ¥¼
        continuous_features = self.continuous_tower(base_features)
        discrete_features = self.discrete_tower(base_features)

        # 3. æ¯ä¸ªå¤´éƒ¨æ¥æ”¶æ¥è‡ªå…¶ä¸“å±å¡”æ¥¼çš„ç‰¹å¾
        mu = self.continuous_head(continuous_features)
        all_disc_logits = self.discrete_head(discrete_features)

        # 3. æŒ‰ DISCRETE_DIMS ç»“æ„å°†æ‰€æœ‰logitsåˆ‡åˆ†ä¸º5ä¸ªéƒ¨åˆ†
        split_sizes = list(DISCRETE_DIMS.values())
        logits_parts = torch.split(all_disc_logits, split_sizes, dim=1)

        # # ä¸ºæ¯ä¸ªéƒ¨åˆ†åˆ†é…åˆç†çš„å˜é‡å
        # trigger_logits, salvo_size_logits, intra_interval_logits, num_groups_logits, inter_interval_logits = logits_parts
        # <<< ä¿®æ”¹ >>> å˜é‡è§£åŒ…ï¼Œç°åœ¨åªæœ‰4ä¸ªéƒ¨åˆ†
        trigger_logits, salvo_size_logits, num_groups_logits, inter_interval_logits = logits_parts

        # 4. ã€åŠ¨ä½œæ©ç ã€‘é€»è¾‘ (åªä½œç”¨äºè§¦å‘å™¨ï¼Œå¦‚æœæ²¡è¯±é¥µå¼¹åˆ™ä¸èƒ½æŠ•æ”¾)
        # å‡è®¾ obs_tensor çš„ç¬¬ 7 ä¸ªç‰¹å¾ (ç´¢å¼•ä¸º7) ä»£è¡¨çº¢å¤–è¯±é¥µå¼¹æ•°é‡
        has_flares_info = obs_tensor[:, 11]
        mask = (has_flares_info == 0)
        trigger_logits_masked = trigger_logits.clone()
        if torch.any(mask):
            # å°†æ²¡æœ‰è¯±é¥µå¼¹çš„æ ·æœ¬å¯¹åº”çš„ logit è®¾ç½®ä¸ºä¸€ä¸ªæå°å€¼ï¼Œé˜»æ­¢é€‰æ‹©è¯¥åŠ¨ä½œ
            trigger_logits_masked[mask] = torch.finfo(torch.float32).min
            # --- è¿™æ˜¯ä¿®æ”¹åçš„æ­£ç¡®ä»£ç  ---
            # è·å– trigger_logits_masked å¼ é‡è‡ªèº«çš„æ•°æ®ç±»å‹ (dtype)
            # ç„¶åè·å–è¯¥ dtype å¯¹åº”çš„æå°å€¼
            # fill_value = torch.finfo(trigger_logits_masked.dtype).min
            # trigger_logits_masked[mask] = fill_value
            # ä½¿ç”¨ä¸€ä¸ªåœ¨ FP16 è¡¨ç¤ºèŒƒå›´å†…ä¸”è¶³å¤Ÿå°çš„å®‰å…¨å€¼
            # trigger_logits_masked[mask] = -1e4  # -10000.0

        # ===============================================================
        # 6ï¸âƒ£ <<< æ–°å¢ï¼šå±‚çº§æ§åˆ¶é€»è¾‘ >>>
        # å½“â€œä¸æŠ•æ”¾â€æ—¶ï¼Œå±è”½å…¶ä»–ç¦»æ•£åŠ¨ä½œçš„ logits
        # ===============================================================
        # å…ˆæ ¹æ®è§¦å‘å™¨çš„ logits è®¡ç®—å‡ºå…¶æ¦‚ç‡
        trigger_probs = torch.sigmoid(trigger_logits_masked)  # shape: [B,1]

        # å¦‚æœ trigger_probs < 0.5ï¼Œè¯´æ˜æ¨¡å‹å€¾å‘äºâ€œä¸æŠ•æ”¾â€
        # æˆ‘ä»¬ç”¨è¿™ä¸ªæ¡ä»¶ç”Ÿæˆä¸€ä¸ª maskï¼ˆTrue=ä¸æŠ•æ”¾ï¼‰
        no_trigger_mask = (trigger_probs < 0.5).squeeze(-1)  # shape: [B]

        # åˆ›å»º logits çš„å‰¯æœ¬ï¼Œé¿å…åŸåœ°æ“ä½œæ±¡æŸ“æ¢¯åº¦
        salvo_size_logits_masked = salvo_size_logits.clone()
        # intra_interval_logits_masked = intra_interval_logits.clone()
        num_groups_logits_masked = num_groups_logits.clone()
        inter_interval_logits_masked = inter_interval_logits.clone()
        # ===============================================================
        # å½“ trigger ä¸æŠ•æ”¾æ—¶ï¼Œå°†å…¶ä»–ç¦»æ•£åŠ¨ä½œ logits å¼ºåˆ¶ä¸º index=0 (one-hot å½¢å¼)
        # ===============================================================
        if torch.any(no_trigger_mask):
            # print("<<<<<<<<<<<<<<<<<< è­¦å‘Šï¼šå¼ºåˆ¶Maské€»è¾‘è¢«è§¦å‘ï¼ >>>>>>>>>>>>>>>>>>")
            INF = 1e6
            NEG_INF = -1e6
            # éå†æ‰€æœ‰ä¾èµ–äºè§¦å‘å™¨çš„ logits å¼ é‡
            for logits_tensor in [
                salvo_size_logits_masked,
                # intra_interval_logits_masked,
                num_groups_logits_masked,
                inter_interval_logits_masked,
            ]:
                # é€‰å‡ºé‚£äº›éœ€è¦è¢«å±è”½çš„è¡Œ (æ ·æœ¬)
                logits_sub = logits_tensor[no_trigger_mask]
                # å¦‚æœç¡®å®æœ‰éœ€è¦å±è”½çš„è¡Œ
                if logits_sub.numel() > 0:
                    # å°†è¿™äº›è¡Œçš„æ‰€æœ‰ logits éƒ½è®¾ä¸ºæå°å€¼
                    logits_sub[:] = NEG_INF  # å…¨éƒ¨ç½®ä¸ºæå°å€¼
                    # ç„¶ååªæŠŠç¬¬ 0 åˆ— (å¯¹åº”ç´¢å¼• 0) çš„ logit è®¾ä¸ºæå¤§å€¼
                    logits_sub[:, 0] = INF  # ä»… index=0 ç½®ä¸ºæå¤§å€¼
                    # å°†ä¿®æ”¹åçš„ logits å†™å›åŸå¼ é‡
                    logits_tensor[no_trigger_mask] = logits_sub

        # 5. åˆ›å»ºæ‰€æœ‰åŠ¨ä½œåˆ†å¸ƒå¯¹è±¡
        # 5.1 è¿ç»­åŠ¨ä½œåˆ†å¸ƒ
        # ä½¿ç”¨å…¨å±€å¯å­¦ä¹ çš„ log_std å‚æ•°
        log_std = torch.clamp(self.log_std_param, self.log_std_min, self.log_std_max)
        std = torch.exp(log_std).expand_as(mu)
        continuous_base_dist = Normal(mu, std)

        # 5.2  åˆ›å»ºæ–°çš„5ä¸ªç¦»æ•£åˆ†å¸ƒ
        trigger_dist = Bernoulli(logits=trigger_logits_masked.squeeze(-1))
        salvo_size_dist = Categorical(logits=salvo_size_logits_masked)
        # intra_interval_dist = Categorical(logits=intra_interval_logits_masked)
        num_groups_dist = Categorical(logits=num_groups_logits_masked)
        inter_interval_dist = Categorical(logits=inter_interval_logits_masked)

        # 6.è¿”å›åŒ…å«æ‰€æœ‰æ–°åˆ†å¸ƒçš„å­—å…¸
        distributions = {
            'continuous': continuous_base_dist,
            'trigger': trigger_dist,
            'salvo_size': salvo_size_dist,
            # 'intra_interval': intra_interval_dist,
            'num_groups': num_groups_dist,
            'inter_interval': inter_interval_dist
        }
        return distributions


class Critic(Module):
    """
    Critic ç½‘ç»œ (ä»·å€¼ç½‘ç»œ)ï¼Œè¯„ä¼°çŠ¶æ€çš„ä»·å€¼ V(s)ã€‚
    è¿™ä¸ªç±»çš„ç»“æ„ä¸å—åŠ¨ä½œç©ºé—´å˜åŒ–çš„å½±å“ã€‚
    """

    def __init__(self):
        super(Critic, self).__init__()
        self.input_dim = CRITIC_PARA.input_dim
        self.output_dim = CRITIC_PARA.output_dim
        # 1. å®šä¹‰ç½‘ç»œç»“æ„
        self.network = Sequential()
        layers_dims = [self.input_dim] + CRITIC_PARA.model_layer_dim
        for i in range(len(layers_dims) - 1):
            self.network.add_module(f'fc_{i}', Linear(layers_dims[i], layers_dims[i + 1]))
            # self.network.add_module(f'LayerNorm_{i}', LayerNorm(layers_dims[i + 1]))
            self.network.add_module(f'LeakyReLU_{i}', LeakyReLU())
        # æ·»åŠ æœ€åçš„è¾“å‡ºå±‚
        self.network.add_module('fc_out', Linear(layers_dims[-1], self.output_dim))

        # # --- [æ–°å¢] åº”ç”¨å‚æ•°åˆå§‹åŒ– ---
        # # é¦–å…ˆå¯¹æ‰€æœ‰å±‚åº”ç”¨é€šç”¨çš„åˆå§‹åŒ–ç­–ç•¥
        # self.network.apply(init_weights)
        #
        # # ç„¶åå¯¹è¾“å‡ºå±‚(fc_out)è¿›è¡Œç‰¹æ®Šçš„ã€å°èŒƒå›´çš„åˆå§‹åŒ–ï¼Œä»¥ä¿è¯åˆå§‹ä»·å€¼ä¼°è®¡çš„ç¨³å®šæ€§
        # # self.network[-1] æŒ‡çš„æ˜¯ Sequential ä¸­çš„æœ€åä¸€ä¸ªæ¨¡å—ï¼Œå³ fc_out
        # init_range = 3e-3
        # self.network[-1].weight.data.uniform_(-init_range, init_range)
        # self.network[-1].bias.data.fill_(0)
        # # --- åˆå§‹åŒ–ç»“æŸ ---

        # 2. å®šä¹‰ä¼˜åŒ–å™¨
        self.optim = torch.optim.Adam(self.parameters(), CRITIC_PARA.lr)
        self.critic_scheduler = lr_scheduler.LinearLR(
            self.optim,
            start_factor=1.0,
            end_factor=AGENTPARA.mini_lr / CRITIC_PARA.lr,
            total_iters=AGENTPARA.MAX_EXE_NUM
        )
        self.to(CRITIC_PARA.device)

        # self.init_model()
        # self.optim = torch.optim.Adam(self.parameters(), CRITIC_PARA.lr)
        # self.critic_scheduler = lr_scheduler.LinearLR(
        #     self.optim,
        #     start_factor=1.0,
        #     end_factor=AGENTPARA.mini_lr / CRITIC_PARA.lr,
        #     total_iters=AGENTPARA.MAX_EXE_NUM
        # )
        # self.to(CRITIC_PARA.device)

    def init_model(self):
        """åˆå§‹åŒ– Critic ç½‘ç»œç»“æ„ï¼Œå¹¶åŠ å…¥ LayerNorm ä»¥å¢å¼ºç¨³å®šæ€§"""
        self.network = Sequential()

        # 1. å®šä¹‰æ‰€æœ‰å±‚çš„ç»´åº¦åˆ—è¡¨ï¼Œä»è¾“å…¥å±‚åˆ°æœ€åä¸€ä¸ªéšè—å±‚
        # ä¾‹å¦‚: input_dim=10, model_layer_dim=[256, 256]
        # -> layers_dims ä¼šæ˜¯ [10, 256, 256]
        layers_dims = [self.input_dim] + CRITIC_PARA.model_layer_dim

        # 2. å¾ªç¯æ„å»ºæ‰€æœ‰çš„éšè—å±‚
        # è¿™ä¸ªå¾ªç¯å°†æ„å»ºä» input->256 å’Œ 256->256 çš„éƒ¨åˆ†
        for i in range(len(layers_dims) - 1):
            # æ·»åŠ çº¿æ€§å±‚
            self.network.add_module(f'fc_{i}', Linear(layers_dims[i], layers_dims[i + 1]))

            # åœ¨çº¿æ€§å±‚ä¹‹åã€æ¿€æ´»å‡½æ•°ä¹‹å‰æ·»åŠ  LayerNorm
            # LayerNorm çš„ç»´åº¦æ˜¯å…¶å‰é¢çº¿æ€§å±‚çš„è¾“å‡ºç»´åº¦
            # self.network.add_module(f'LayerNorm_{i}', LayerNorm(layers_dims[i + 1]))

            # æ·»åŠ æ¿€æ´»å‡½æ•°
            self.network.add_module(f'LeakyReLU_{i}', LeakyReLU())

        # 3. å•ç‹¬æ·»åŠ æœ€åçš„è¾“å‡ºå±‚
        # å®ƒçš„è¾“å…¥æ˜¯æœ€åä¸€ä¸ªéšè—å±‚çš„ç»´åº¦ï¼Œè¾“å‡ºæ˜¯ self.output_dim (é€šå¸¸ä¸º1)
        # æœ€åçš„è¾“å‡ºå±‚é€šå¸¸ä¸éœ€è¦ LayerNorm æˆ–æ¿€æ´»å‡½æ•°
        self.network.add_module('fc_out', Linear(layers_dims[-1], self.output_dim))

    def forward(self, obs):
        """å‰å‘ä¼ æ’­ï¼Œè®¡ç®—çŠ¶æ€ä»·å€¼"""
        obs = check(obs).to(**CRITIC_PARA.tpdv)
        value = self.network(obs)
        return value


class PPO_continuous(object):
    """
    PPO æ™ºèƒ½ä½“ä¸»ç±»ï¼Œæ•´åˆ Actor å’Œ Critic å¹¶å®ç° PPO ç®—æ³•æ ¸å¿ƒé€»è¾‘ã€‚
    """

    def __init__(self, load_able: bool, model_dir_path: str = None):
        super(PPO_continuous, self).__init__()
        self.Actor = Actor()
        self.Critic = Critic()


        # åªéœ€è¦åœ¨ agent åˆå§‹åŒ–æ—¶åˆ›å»ºä¸€æ¬¡
        self.actor_scaler = GradScaler()
        self.critic_scaler = GradScaler()

        self.buffer = Buffer()
        self.gamma = AGENTPARA.gamma
        self.gae_lambda = AGENTPARA.lamda
        self.ppo_epoch = AGENTPARA.ppo_epoch
        self.total_steps = 0
        self.training_start_time = time.strftime("PPO_%Y-%m-%d_%H-%M-%S")
        self.base_save_dir = "../../../save/save_evade_fuza"
        win_rate_subdir = "èƒœç‡æ¨¡å‹"
        # æ‹¼æ¥æˆå®Œæ•´çš„å­˜æ¡£è·¯å¾„
        self.run_save_dir = os.path.join(self.base_save_dir, self.training_start_time)
        self.win_rate_dir = os.path.join(self.run_save_dir, win_rate_subdir)

        if load_able:
            if model_dir_path:
                print(f"--- æ­£åœ¨ä»æŒ‡å®šæ–‡ä»¶å¤¹åŠ è½½æ¨¡å‹: {model_dir_path} ---")
                self.load_models_from_directory(model_dir_path)
            else:
                print("--- æœªæŒ‡å®šæ¨¡å‹æ–‡ä»¶å¤¹ï¼Œå°è¯•ä»é»˜è®¤æ–‡ä»¶å¤¹ 'test' åŠ è½½ ---")
                self.load_models_from_directory("../../../test/test_evade")

    def load_models_from_directory(self, directory_path: str):
        """ä»æŒ‡å®šçš„æ–‡ä»¶å¤¹è·¯å¾„åŠ è½½æ¨¡å‹ï¼Œèƒ½è‡ªåŠ¨è¯†åˆ«å¤šç§å‘½åæ ¼å¼ã€‚"""
        if not os.path.isdir(directory_path):
            print(f"[é”™è¯¯] æ¨¡å‹åŠ è½½å¤±è´¥ï¼šæä¾›çš„è·¯å¾„ '{directory_path}' ä¸æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„æ–‡ä»¶å¤¹ã€‚")
            return
        files = os.listdir(directory_path)
        actor_files_with_prefix = [f for f in files if f.endswith("_Actor.pkl")]
        if len(actor_files_with_prefix) > 0:
            actor_filename = actor_files_with_prefix[0]
            prefix = actor_filename.replace("_Actor.pkl", "")
            critic_filename = f"{prefix}_Critic.pkl"
            print(f"  - æ£€æµ‹åˆ°å‰ç¼€ '{prefix}'ï¼Œå‡†å¤‡åŠ è½½æ¨¡å‹...")
            if critic_filename in files:
                actor_full_path = os.path.join(directory_path, actor_filename)
                critic_full_path = os.path.join(directory_path, critic_filename)
                try:
                    self.Actor.load_state_dict(torch.load(actor_full_path, map_location=ACTOR_PARA.device))
                    print(f"    - æˆåŠŸåŠ è½½ Actor: {actor_full_path}")
                    self.Critic.load_state_dict(torch.load(critic_full_path, map_location=CRITIC_PARA.device))
                    print(f"    - æˆåŠŸåŠ è½½ Critic: {critic_full_path}")
                    return
                except Exception as e:
                    print(f"    - [é”™è¯¯] åŠ è½½å¸¦å‰ç¼€çš„æ¨¡å‹æ—¶å¤±è´¥: {e}")
            else:
                print(f"    - [è­¦å‘Š] æ‰¾åˆ°äº† '{actor_filename}' ä½†æœªæ‰¾åˆ°å¯¹åº”çš„ '{critic_filename}'ã€‚")
        if "Actor.pkl" in files and "Critic.pkl" in files:
            print("  - æ£€æµ‹åˆ°æ— å‰ç¼€æ ¼å¼ï¼Œå‡†å¤‡åŠ è½½ 'Actor.pkl' å’Œ 'Critic.pkl'...")
            actor_full_path = os.path.join(directory_path, "Actor.pkl")
            critic_full_path = os.path.join(directory_path, "Critic.pkl")
            try:
                self.Actor.load_state_dict(torch.load(actor_full_path, map_location=ACTOR_PARA.device))
                print(f"    - æˆåŠŸåŠ è½½ Actor: {actor_full_path}")
                self.Critic.load_state_dict(torch.load(critic_full_path, map_location=CRITIC_PARA.device))
                print(f"    - æˆåŠŸåŠ è½½ Critic: {critic_full_path}")
                return
            except Exception as e:
                print(f"    - [é”™è¯¯] åŠ è½½æ— å‰ç¼€æ¨¡å‹æ—¶å¤±è´¥: {e}")
        print(f"[é”™è¯¯] æ¨¡å‹åŠ è½½å¤±è´¥ï¼šåœ¨æ–‡ä»¶å¤¹ '{directory_path}' ä¸­æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„ Actor/Critic æ¨¡å‹å¯¹ã€‚")

    def store_experience(self, state, action, probs, value, reward, done):
        """å­˜å‚¨ç»éªŒåˆ° Bufferï¼Œå¹¶åœ¨å­˜å‚¨å‰è¿›è¡Œæ•°å€¼æ£€æŸ¥ã€‚"""
        if not np.all(np.isfinite(probs)):
            print("=" * 50)
            print(f"!!! ä¸¥é‡é”™è¯¯: åœ¨ log_prob ä¸­æ£€æµ‹åˆ°éæœ‰é™å€¼ (NaN/Inf) !!!")
            print(f"Log_prob å€¼: {probs}")
            print(f"å¯¼è‡´é”™è¯¯çš„çŠ¶æ€: {state}")
            print(f"å¯¼è‡´é”™è¯¯çš„åŠ¨ä½œ: {action}")
            print("=" * 50)
            raise ValueError("åœ¨ log_prob ä¸­æ£€æµ‹åˆ° NaN/Infï¼")
        self.buffer.store_transition(state, value, action, probs, reward, done)

    def scale_action(self, action_cont_tanh):
        """å°†tanhå‹ç¼©åçš„è¿ç»­åŠ¨ä½œ [-1, 1] ç¼©æ”¾åˆ°ç¯å¢ƒçš„å®é™…ç‰©ç†èŒƒå›´ã€‚"""
        lows = torch.tensor([ACTION_RANGES[k]['low'] for k in CONTINUOUS_ACTION_KEYS], **ACTOR_PARA.tpdv)
        highs = torch.tensor([ACTION_RANGES[k]['high'] for k in CONTINUOUS_ACTION_KEYS], **ACTOR_PARA.tpdv)
        scaled_action = lows + (action_cont_tanh + 1.0) * 0.5 * (highs - lows)
        return scaled_action

    def check_numerics(self, name, tensor, state=None, action=None, threshold=1e4):
        """æ£€æŸ¥ tensor æ˜¯å¦å­˜åœ¨ NaNã€Inf æˆ–å¼‚å¸¸å¤§å€¼ï¼Œç”¨äºè°ƒè¯•ã€‚"""
        arr = tensor.detach().cpu().numpy()
        if not np.all(np.isfinite(arr)):
            print(f"[æ•°å€¼é”™è¯¯] {name} å‡ºç° NaN/Inf. å€¼: {arr}")
            if state is not None: print(f"å¯¹åº” state: {state}")
            if action is not None: print(f"å¯¹åº” action: {action}")
            raise ValueError(f"NaN/Inf detected in {name}")
        if np.any(np.abs(arr) > threshold):
            print(f"[è­¦å‘Š] {name} æ•°å€¼è¿‡å¤§ (> {threshold}). å€¼: {arr.max()}, {arr.min()}")

    def map_discrete_actions(self, discrete_actions_indices):
        """
        å°†ç¦»æ•£åŠ¨ä½œçš„ç´¢å¼•å¼ é‡æ˜ å°„åˆ°å…¶ç‰©ç†å€¼ã€‚
        Args:
            discrete_actions_indices (dict): ä¸€ä¸ªåŒ…å«å„ç¦»æ•£åŠ¨ä½œç´¢å¼•å¼ é‡çš„å­—å…¸ã€‚
        Returns:
            np.ndarray: ä¸€ä¸ªåŒ…å«æ‰€æœ‰ç¦»æ•£åŠ¨ä½œç‰©ç†å€¼çš„ numpy æ•°ç»„ã€‚
        """
        # 0. è·å–è§¦å‘å™¨åŠ¨ä½œ (å½¢çŠ¶: [B])
        trigger_action = discrete_actions_indices['trigger'].cpu().numpy()

        # 1. åˆå§‹åŒ–ä¸€ä¸ªç”¨äºå­˜æ”¾ç‰©ç†å€¼çš„æ•°ç»„ (å½¢çŠ¶: [B, 4])
        # å¯¹åº” salvo_size, intra_interval, num_groups, inter_interval
        batch_size = trigger_action.shape[0]
        physical_actions = np.zeros((batch_size, 4), dtype=np.float32)

        # 2. éå†æ¯ä¸ª Categorical åŠ¨ä½œ
        action_keys = ['salvo_size', 'intra_interval', 'num_groups', 'inter_interval']
        for i, key in enumerate(action_keys):
            # è·å–è¯¥åŠ¨ä½œçš„ç´¢å¼• (å½¢çŠ¶: [B])
            indices = discrete_actions_indices[key].cpu().numpy()
            # è·å–æ˜ å°„è¡¨
            mapping = DISCRETE_ACTION_MAP[key]
            # ä½¿ç”¨ç´¢å¼•ä»æ˜ å°„è¡¨ä¸­æŸ¥æ‰¾ç‰©ç†å€¼
            physical_actions[:, i] = np.array([mapping[idx] for idx in indices])

        # 3. åº”ç”¨è§¦å‘å™¨é€»è¾‘ï¼šå¦‚æœ trigger=0ï¼Œåˆ™æ‰€æœ‰å…¶ä»–å‚æ•°æ— æ•ˆ (å¯ä»¥è®¾ä¸º0)
        # è§¦å‘å™¨ä¸º0çš„ä½ç½®çš„æ©ç  (broadcastable to physical_actions)
        trigger_mask = (trigger_action == 0)[:, np.newaxis]
        physical_actions[np.repeat(trigger_mask, 4, axis=1)] = 0.0

        # 4. æœ€ç»ˆå°† trigger (0/1) å’Œå…¶ä»–ç‰©ç†å€¼ç»„åˆèµ·æ¥
        # æœ€ç»ˆè¾“å‡ºçš„ç¦»æ•£åŠ¨ä½œæ•°ç»„ (å½¢çŠ¶: [B, 5])
        final_discrete_env_actions = np.hstack([
            trigger_action[:, np.newaxis],
            physical_actions
        ])

        return final_discrete_env_actions

    def choose_action(self, state, deterministic=False):
        """
        æ ¹æ®å½“å‰çŠ¶æ€é€‰æ‹©åŠ¨ä½œï¼Œè¿™æ˜¯ä¸ç¯å¢ƒäº¤äº’çš„æ ¸å¿ƒã€‚
        - æ”¯æŒç¡®å®šæ€§/éšæœºæ€§é‡‡æ ·ã€‚
        - æ”¯æŒæ‰¹å¤„ç†ã€‚
        - å½“ä¸æŠ•æ”¾è¯±é¥µå¼¹æ—¶ï¼Œæ™ºèƒ½åœ°å°†ç›¸å…³å‚æ•°ç½®é›¶ã€‚
        """
        state_tensor = torch.as_tensor(state, dtype=torch.float32, device=ACTOR_PARA.device)
        is_batch = state_tensor.dim() > 1
        if not is_batch:
            state_tensor = state_tensor.unsqueeze(0)

        with torch.no_grad():
            # 1. ä»Actorè·å–åŒ…å«æ‰€æœ‰åŠ¨ä½œåˆ†å¸ƒçš„å­—å…¸
            dists = self.Actor(state_tensor)

            # 2. å¤„ç†è¿ç»­åŠ¨ä½œ
            continuous_base_dist = dists['continuous']
            # é‡‡æ ·åŸå§‹åŠ¨ä½œ u (pre-tanh)
            u = continuous_base_dist.mean if deterministic else continuous_base_dist.rsample()
            # è®¡ç®— squashed åçš„åŠ¨ä½œ a (post-tanh)
            action_cont_tanh = torch.tanh(u)
            log_prob_cont = continuous_base_dist.log_prob(u).sum(dim=-1)

            # # --- ğŸ’¥ æ–°çš„ã€æ­£ç¡®çš„ log_prob è®¡ç®— (å¸¦é›…å¯æ¯”ä¿®æ­£) ---
            # # åŸºç¡€çš„é«˜æ–¯ log_prob: log p(u)
            # log_prob_u = continuous_base_dist.log_prob(u)
            #
            # # ç¨³å®šçš„é›…å¯æ¯”ä¿®æ­£é¡¹: log(1 - tanh(u)^2)
            # # ä½¿ç”¨å…¬å¼: 2 * (log(2) - u - softplus(-2*u))
            # # torch.nn.functional.softplus(x) = log(1 + exp(x))
            # log_prob_correction = 2 * (np.log(2.0) - u - torch.nn.functional.softplus(-2 * u))
            #
            # # æœ€ç»ˆçš„ log_prob: log p(a) = log p(u) - log(1 - a^2)
            # log_prob_cont = (log_prob_u - log_prob_correction).sum(dim=-1)

            # # --- ä¸ºäº†è°ƒè¯•ï¼Œæˆ‘ä»¬æå‰è·å– mean å’Œ log_std ---
            # action_mean, log_std = continuous_base_dist.mean, continuous_base_dist.scale.log()
            #
            # u = action_mean if deterministic else continuous_base_dist.rsample()
            # action_cont_tanh = torch.tanh(u)
            # log_prob_cont = continuous_base_dist.log_prob(u).sum(dim=-1)

            # 3. ä»æ‰€æœ‰5ä¸ªç¦»æ•£åˆ†å¸ƒä¸­é‡‡æ ·æˆ–é€‰æ‹©æœ€ä¼˜åŠ¨ä½œ
            sampled_actions_dict = {}
            for key, dist in dists.items():
                if key == 'continuous':
                    continue
                if deterministic:
                    if isinstance(dist, Categorical):
                        # é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„åŠ¨ä½œç´¢å¼•
                        sampled_actions_dict[key] = torch.argmax(dist.probs, dim=-1)
                        # sampled_actions_dict[key] = dist.sample()
                    elif isinstance(dist, Bernoulli):
                        # é€‰æ‹©æ¦‚ç‡å¤§äº0.5çš„åŠ¨ä½œ (0æˆ–1)
                        sampled_actions_dict[key] = (dist.probs > 0.5).float()
                        # æŒ‰æ¦‚ç‡é‡‡æ · (0æˆ–1)
                        # sampled_actions_dict[key] = dist.sample()
                else:
                    # éšæœºé‡‡æ ·
                    sampled_actions_dict[key] = dist.sample()

            # ä¸ºäº†æ–¹ä¾¿åç»­ä½¿ç”¨ï¼Œå°†å­—å…¸ä¸­çš„åŠ¨ä½œè§£åŒ…åˆ°å•ç‹¬çš„å˜é‡
            trigger_action = sampled_actions_dict['trigger']
            salvo_size_action = sampled_actions_dict['salvo_size']
            # intra_interval_action = sampled_actions_dict['intra_interval']
            num_groups_action = sampled_actions_dict['num_groups']
            inter_interval_action = sampled_actions_dict['inter_interval']

            # 4. è®¡ç®—å¹¶åŠ æ€»æ‰€æœ‰5ä¸ªç¦»æ•£åŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡
            log_prob_disc = (dists['trigger'].log_prob(trigger_action) +
                             dists['salvo_size'].log_prob(salvo_size_action) +
                             # dists['intra_interval'].log_prob(intra_interval_action) +
                             dists['num_groups'].log_prob(num_groups_action) +
                             dists['inter_interval'].log_prob(inter_interval_action))

            # 5. è®¡ç®—æ€»çš„å¯¹æ•°æ¦‚ç‡
            total_log_prob = log_prob_cont + log_prob_disc

            # 6. å‡†å¤‡è¦å­˜å…¥Bufferçš„åŠ¨ä½œå‘é‡ (å­˜å‚¨åŸå§‹å€¼uå’ŒåŸå§‹é‡‡æ ·ç´¢å¼•)
            action_disc_to_store = torch.stack([
                trigger_action, salvo_size_action, #intra_interval_action,
                num_groups_action, inter_interval_action
            ], dim=-1).float()
            action_to_store = torch.cat([u, action_disc_to_store], dim=-1)

            # 7. å‡†å¤‡å‘é€åˆ°ç¯å¢ƒçš„æœ€ç»ˆåŠ¨ä½œå‘é‡ (åº”ç”¨ç½®é›¶é€»è¾‘)
            env_action_cont = self.scale_action(action_cont_tanh)

            # åˆ›å»ºä¸€ä¸ªæ©ç ï¼Œç”¨äºè¯†åˆ«å“ªäº›æ ·æœ¬çš„ trigger_action ä¸º 0
            zero_mask = (trigger_action == 0)

            # ä½¿ç”¨ clone() é¿å…åŸåœ°ä¿®æ”¹å½±å“ buffer ä¸­å­˜å‚¨çš„åŠ¨ä½œ
            env_salvo_size_action = salvo_size_action.clone()
            # env_intra_interval_action = intra_interval_action.clone()
            env_num_groups_action = num_groups_action.clone()
            env_inter_interval_action = inter_interval_action.clone()

            # <<< æ–°å¢ >>> åˆ›å»ºä¸€ä¸ªä¸å…¶å®ƒåŠ¨ä½œå½¢çŠ¶ç›¸åŒã€å€¼ä¸ºå›ºå®šå€¼çš„å¼ é‡
            # fixed_intra_interval_action = torch.full_like(trigger_action, FIXED_INTRA_INTERVAL, dtype=torch.float32)

            # print(fixed_intra_interval_action)

            # åº”ç”¨æ©ç ï¼Œå°†ä¸æŠ•æ”¾çš„æ ·æœ¬çš„å‚æ•°ç½®é›¶
            env_salvo_size_action[zero_mask] = 0
            # env_intra_interval_action[zero_mask] = 0
            env_num_groups_action[zero_mask] = 0
            env_inter_interval_action[zero_mask] = 0

            # æ‹¼æ¥æˆæœ€ç»ˆå‘é€ç»™ç¯å¢ƒçš„ç¦»æ•£åŠ¨ä½œéƒ¨åˆ†
            final_env_action_disc = torch.stack([
                trigger_action, env_salvo_size_action, #fixed_intra_interval_action, #env_intra_interval_action,
                env_num_groups_action, env_inter_interval_action
            ], dim=-1).float()

            # æ‹¼æ¥è¿ç»­å’Œç¦»æ•£éƒ¨åˆ†
            final_env_action_tensor = torch.cat([env_action_cont, final_env_action_disc], dim=-1)

        # 8. å°†æ‰€æœ‰ç»“æœè½¬æ¢ä¸ºNumpyæ•°ç»„
        action_to_store_np = action_to_store.cpu().numpy()
        log_prob_to_store_np = total_log_prob.cpu().numpy()
        final_env_action_np = final_env_action_tensor.cpu().numpy()

    #     # ###############################################################
    #     # #                  åœ¨è¿™é‡Œæ’å…¥æˆ‘ä»¬çš„è¯Šæ–­ä»£ç                      #
    #     # ###############################################################
    #
    #     print("\n" + "=" * 20 + " ACTION DEBUG " + "=" * 20)
    #     # 1) æŸ¥çœ‹ mean/std åˆ†å¸ƒ
    #     print("mean (mu): mean {:.3f}, std {:.3f}, min {:.3f}, max {:.3f}".format(
    #         action_mean.mean().item(), action_mean.std().item(), action_mean.min().item(), action_mean.max().item()
    #     ))
    #     print("log_std:   mean {:.3f}, std {:.3f}, min {:.3f}, max {:.3f}".format(
    #         log_std.mean().item(), log_std.std().item(), log_std.min().item(), log_std.max().item()
    #     ))
    #
    #     # 2) æŸ¥çœ‹ u çš„èŒƒå›´ï¼ˆæœª squashï¼‰
    #     u_np = u.detach().cpu().numpy().ravel()
    #     print("u (pre-tanh): min {:.3f}, max {:.3f}, mean_abs {:.3f}".format(
    #         u_np.min(), u_np.max(), np.abs(u_np).mean()
    #     ))
    #
    #     # 3) æŸ¥çœ‹ tanh ååŠ¨ä½œä»¥åŠæ”¾å¤§åèŒƒå›´
    #     act_tanh_np = action_cont_tanh.detach().cpu().numpy().ravel()
    #     print("action (post-tanh): min {:.3f}, max {:.3f}".format(
    #         act_tanh_np.min(), act_tanh_np.max()
    #     ))
    #
    #     # ç”±äºä½ çš„ scale_action æ˜¯åœ¨ agent å†…éƒ¨å®ç°çš„ï¼Œè¿™é‡Œæˆ‘ä»¬ç›´æ¥æ‰“å°ç¼©æ”¾åçš„åŠ¨ä½œ
    #     scaled_act_np = env_action_cont.detach().cpu().numpy().ravel()
    #     print("action (scaled for env): min {:.3f}, max {:.3f}".format(
    #         scaled_act_np.min(), scaled_act_np.max()
    #     ))
    #
    #     # 4) ç»Ÿè®¡è¢«é¥±å’Œçš„æ¯”ä¾‹ï¼ˆæ¥è¿‘ Â±1ï¼‰
    #     saturated_count = np.sum(np.abs(act_tanh_np) > 0.98)
    #     saturated_ratio = saturated_count / act_tanh_np.size
    #     print("Saturated ratio (>0.98): {:.2f}% ({}/{})".format(
    #         saturated_ratio * 100, saturated_count, act_tanh_np.size
    #     ))
    #     print("=" * 54 + "\n")
    # # ###############################################################

        # å¦‚æœè¾“å…¥ä¸æ˜¯æ‰¹å¤„ç†ï¼Œåˆ™ç§»é™¤æ‰¹æ¬¡ç»´åº¦
        if not is_batch:
            final_env_action_np = final_env_action_np[0]
            action_to_store_np = action_to_store_np[0]
            log_prob_to_store_np = log_prob_to_store_np[0]

        return final_env_action_np, action_to_store_np, log_prob_to_store_np

    def get_value(self, state):
        """ä½¿ç”¨ Critic ç½‘ç»œè·å–ç»™å®šçŠ¶æ€çš„ä»·å€¼ã€‚"""
        with torch.no_grad():
            value = self.Critic(state)
        return value

    def cal_gae(self, states, values, actions, probs, rewards, dones):
        """è®¡ç®—å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡ (GAE)ã€‚"""
        advantage = np.zeros(len(rewards), dtype=np.float32)
        gae = 0
        for t in reversed(range(len(rewards))):
            next_value = values[t + 1] if t < len(rewards) - 1 else 0.0
            done_mask = 1.0 - int(dones[t])
            delta = rewards[t] + self.gamma * next_value * done_mask - values[t]
            gae = delta + self.gamma * self.gae_lambda * done_mask * gae
            advantage[t] = gae
        return advantage

    def learn(self):
        """
        æ‰§è¡Œ PPO çš„å­¦ä¹ å’Œæ›´æ–°æ­¥éª¤ï¼Œå·²é€‚é…å¤šéƒ¨åˆ†ç¦»æ•£åŠ¨ä½œç©ºé—´ã€‚
        """
        torch.autograd.set_detect_anomaly(True)
        states, values, actions, old_probs, rewards, dones = self.buffer.sample()
        advantages = self.cal_gae(states, values, actions, old_probs, rewards, dones)

        train_info = {'critic_loss': [], 'actor_loss': [], 'dist_entropy': [],'entropy_cont': [], 'adv_targ': [], 'ratio': []}

        for _ in range(self.ppo_epoch):
            for batch in self.buffer.generate_batches():
                state = check(states[batch]).to(**ACTOR_PARA.tpdv)
                action_batch = check(actions[batch]).to(**ACTOR_PARA.tpdv)
                old_prob = check(old_probs[batch]).to(**ACTOR_PARA.tpdv).view(-1)
                advantage = check(advantages[batch]).to(**ACTOR_PARA.tpdv)
                # advantage = (advantage - advantage.mean()) / (advantage.std() + 1e-8)  # Mini-batch advantage normalization
                advantage = advantage.view(-1, 1)

                # --- 1. ä» Buffer ä¸­åˆ†ç¦»å‡ºè¿ç»­å’Œç¦»æ•£åŠ¨ä½œ ---
                u_from_buffer = action_batch[:, :CONTINUOUS_DIM]
                # ç¦»æ•£åŠ¨ä½œçš„ç´¢å¼•ä»ç¬¬ CONTINUOUS_DIM åˆ—å¼€å§‹
                discrete_actions_from_buffer = {
                    'trigger': action_batch[:, CONTINUOUS_DIM],
                    'salvo_size': action_batch[:, CONTINUOUS_DIM + 1].long(),
                    # 'intra_interval': action_batch[:, CONTINUOUS_DIM + 2].long(),
                    'num_groups': action_batch[:, CONTINUOUS_DIM + 2].long(),# ç´¢å¼•ä» +3 æ”¹ä¸º +2
                    'inter_interval': action_batch[:, CONTINUOUS_DIM + 3].long(), # ç´¢å¼•ä» +4 æ”¹ä¸º +3
                }

                # ######################### Actor è®­ç»ƒ #########################
                # # 1. å°†å‰å‘ä¼ æ’­åŒ…è£¹åœ¨ autocast ä¸­
                # with autocast():
                #     # 2. ä½¿ç”¨å½“å‰ç­–ç•¥é‡æ–°è¯„ä¼°æ—§åŠ¨ä½œçš„æ¦‚ç‡
                #     new_dists = self.Actor(state)
                #
                #     # 3. é‡æ–°è®¡ç®—æ–°ç­–ç•¥ä¸‹ï¼Œæ—§åŠ¨ä½œçš„ç»„åˆ log_prob
                #     new_log_prob_cont = new_dists['continuous'].log_prob(u_from_buffer).sum(dim=-1)
                #     new_log_prob_disc = sum(
                #         new_dists[key].log_prob(discrete_actions_from_buffer[key])
                #         for key in discrete_actions_from_buffer
                #     )
                #     new_prob = new_log_prob_cont + new_log_prob_disc
                #
                #     # 4. è®¡ç®—ç»„åˆç­–ç•¥ç†µ
                #     entropy_cont = new_dists['continuous'].entropy().sum(dim=-1)
                #     entropy_disc = sum(
                #         dist.entropy() for key, dist in new_dists.items() if key != 'continuous'
                #     )
                #     total_entropy = (entropy_cont + entropy_disc).mean()
                #
                #     # 5. è®¡ç®—é‡è¦æ€§é‡‡æ ·æ¯”ç‡å’ŒPPO clipped loss (åç»­é€»è¾‘ä¸å˜)
                #     log_ratio = new_prob - old_prob
                #     ratio = torch.exp(torch.clamp(log_ratio, -20.0, 20.0))
                #     surr1 = ratio * advantage.squeeze(-1)
                #     surr2 = torch.clamp(ratio, 1.0 - AGENTPARA.epsilon, 1.0 + AGENTPARA.epsilon) * advantage.squeeze(-1)
                #     actor_loss = -torch.min(surr1, surr2).mean() - AGENTPARA.entropy * total_entropy
                #
                # # 6. Actoræ¢¯åº¦æ›´æ–°, ä½¿ç”¨ scaler
                # self.Actor.optim.zero_grad(set_to_none=True) # ä½¿ç”¨ set_to_none=True ç•¥å¾®æå‡æ€§èƒ½
                # # ç”¨ scaler.scale æ¥ç¼©æ”¾ loss
                # self.actor_scaler.scale(actor_loss).backward()
                # # --- æ­£ç¡®çš„æ¢¯åº¦è£å‰ªæµç¨‹ ---
                # # 1. Unscale æ¢¯åº¦
                # self.actor_scaler.unscale_(self.Actor.optim)
                # # 2. åœ¨ unscaled çš„æ¢¯åº¦ä¸Šè¿›è¡Œè£å‰ª
                # torch.nn.utils.clip_grad_norm_(self.Actor.parameters(), max_norm=1.0)
                # # 3. æ‰§è¡Œä¼˜åŒ–å™¨æ­¥éª¤å’Œ scaler æ›´æ–°
                # self.actor_scaler.step(self.Actor.optim)
                # self.actor_scaler.update()
                #
                #
                # ######################### Critic è®­ç»ƒ #########################
                # # 3. å°† Critic çš„å‰å‘ä¼ æ’­ä¹ŸåŒ…è£¹åœ¨ autocast ä¸­
                # with autocast():
                #     # 7. è®¡ç®—ä»·å€¼ç›®æ ‡å¹¶æ›´æ–°Critic (é€»è¾‘ä¸å˜)
                #     old_value_from_buffer = check(values[batch]).to(**CRITIC_PARA.tpdv).view(-1, 1)
                #     return_ = advantage + old_value_from_buffer
                #     new_value = self.Critic(state)
                #     critic_loss = torch.nn.functional.mse_loss(new_value, return_)
                # # 4. Critic æ¢¯åº¦æ›´æ–°ï¼Œä½¿ç”¨å¦ä¸€ä¸ª scaler
                # self.Critic.optim.zero_grad(set_to_none=True)
                # self.critic_scaler.scale(critic_loss).backward()
                # # --- åŒæ ·åœ°ï¼Œå¯¹ Critic ä¹Ÿæ‰§è¡Œæ­£ç¡®çš„è£å‰ªæµç¨‹ ---
                # # 1. Unscale æ¢¯åº¦
                # self.critic_scaler.unscale_(self.Critic.optim)
                # # 2. åœ¨ unscaled çš„æ¢¯åº¦ä¸Šè¿›è¡Œè£å‰ª
                # torch.nn.utils.clip_grad_norm_(self.Critic.parameters(), max_norm=1.0)
                # # 3. æ‰§è¡Œä¼˜åŒ–å™¨æ­¥éª¤å’Œ scaler æ›´æ–°
                # self.critic_scaler.step(self.Critic.optim)
                # self.critic_scaler.update()

                ######################### Actor è®­ç»ƒ #########################
                # 2. ä½¿ç”¨å½“å‰ç­–ç•¥é‡æ–°è¯„ä¼°æ—§åŠ¨ä½œçš„æ¦‚ç‡
                new_dists = self.Actor(state)

                # 3. é‡æ–°è®¡ç®—æ–°ç­–ç•¥ä¸‹ï¼Œæ—§åŠ¨ä½œçš„ç»„åˆ log_prob
                new_log_prob_cont = new_dists['continuous'].log_prob(u_from_buffer).sum(dim=-1)

                # # --- ğŸ’¥ é‡æ–°è®¡ç®—æ–°ç­–ç•¥ä¸‹ï¼Œæ—§åŠ¨ä½œçš„ç»„åˆ log_prob (å¸¦é›…å¯æ¯”ä¿®æ­£) ---
                # # è¿ç»­éƒ¨åˆ†
                # new_log_prob_u = new_dists['continuous'].log_prob(u_from_buffer)
                # new_log_prob_correction = 2 * (np.log(2.0) - u_from_buffer - torch.nn.functional.softplus(-2 * u_from_buffer))
                # new_log_prob_cont = (new_log_prob_u - new_log_prob_correction).sum(dim=-1)

                new_log_prob_disc = sum(
                    new_dists[key].log_prob(discrete_actions_from_buffer[key])
                    for key in discrete_actions_from_buffer
                )
                new_prob = new_log_prob_cont + new_log_prob_disc

                # 4. è®¡ç®—ç»„åˆç­–ç•¥ç†µ
                entropy_cont = new_dists['continuous'].entropy().sum(dim=-1)
                entropy_disc = sum(
                    dist.entropy() for key, dist in new_dists.items() if key != 'continuous'
                )
                total_entropy = (entropy_cont + entropy_disc).mean()
                # --- æ ¸å¿ƒä¿®æ”¹ ---
                # å®šä¹‰ä¸åŒçš„ç†µç³»æ•°
                CONTINUOUS_ENTROPY_COEFF = AGENTPARA.entropy  # e.g., 0.01
                DISCRETE_ENTROPY_COEFF = AGENTPARA.entropy #0.05  # ç»™ç¦»æ•£åŠ¨ä½œä¸€ä¸ªé«˜ 5 å€çš„ç†µç³»æ•°æ¥é¼“åŠ±æ¢ç´¢

                # è®¡ç®—åŠ æƒçš„ç†µå¥–åŠ±
                total_entropy_bonus = (CONTINUOUS_ENTROPY_COEFF * entropy_cont.mean() +
                                       DISCRETE_ENTROPY_COEFF * entropy_disc.mean())

                # 5. è®¡ç®—é‡è¦æ€§é‡‡æ ·æ¯”ç‡å’ŒPPO clipped loss (åç»­é€»è¾‘ä¸å˜)
                log_ratio = new_prob - old_prob
                ratio = torch.exp(torch.clamp(log_ratio, -20.0, 20.0))
                surr1 = ratio * advantage.squeeze(-1)
                surr2 = torch.clamp(ratio, 1.0 - AGENTPARA.epsilon, 1.0 + AGENTPARA.epsilon) * advantage.squeeze(-1)
                actor_loss = -torch.min(surr1, surr2).mean() - total_entropy_bonus #AGENTPARA.entropy * total_entropy

                # 6. Actoræ¢¯åº¦æ›´æ–°, ä½¿ç”¨ scaler
                self.Actor.optim.zero_grad()
                actor_loss.backward()
                torch.nn.utils.clip_grad_norm_(self.Actor.parameters(), max_norm=1.0)
                self.Actor.optim.step()

                ######################### Critic è®­ç»ƒ #########################
                # 7. è®¡ç®—ä»·å€¼ç›®æ ‡å¹¶æ›´æ–°Critic (é€»è¾‘ä¸å˜)
                old_value_from_buffer = check(values[batch]).to(**CRITIC_PARA.tpdv).view(-1, 1)
                return_ = advantage + old_value_from_buffer
                new_value = self.Critic(state)
                critic_loss = torch.nn.functional.mse_loss(new_value, return_)
                self.Critic.optim.zero_grad()
                critic_loss.backward()
                torch.nn.utils.clip_grad_norm_(self.Critic.parameters(), max_norm=1.0)
                self.Critic.optim.step()

                # # 1ï¸âƒ£ è®¡ç®—æ—§ valueï¼ˆæ¥è‡ª bufferï¼‰
                # old_value_from_buffer = check(values[batch]).to(**CRITIC_PARA.tpdv).view(-1, 1)
                #
                # # 2ï¸âƒ£ ç›®æ ‡å›æŠ¥ (return_t = advantage + V_old)
                # return_ = advantage + old_value_from_buffer
                #
                # # 3ï¸âƒ£ é‡æ–°è¯„ä¼°æ–° value
                # new_value = self.Critic(state)
                #
                # # 4ï¸âƒ£ --- Value Clipping ---
                # # # é™åˆ¶æ–°é¢„æµ‹ä¸æ—§å€¼çš„åå·®ä¸è¶…è¿‡ epsilon
                # # value_clipped = old_value_from_buffer + torch.clamp(
                # #     new_value - old_value_from_buffer,
                # #     -AGENTPARA.epsilon,
                # #     AGENTPARA.epsilon
                # #     # - AGENTPARA.epsilon * old_value_from_buffer.abs(),  # ç›¸å¯¹æ¯”ä¾‹ clip,
                # #     # AGENTPARA.epsilon * old_value_from_buffer.abs()  # ç›¸å¯¹æ¯”ä¾‹ clip
                # # )
                #
                # #ä¿æŒåŸ reward é‡çº§ï¼Œä½†æ”¹æˆâ€œç›¸å¯¹æ¯”ä¾‹è£å‰ªâ€è§£é‡Šï¼š
                # # å½“ V_old=50 æ—¶ï¼Œå…è®¸å˜åŒ– Â±10ï¼›
                # # å½“ V_old=2 æ—¶ï¼Œå…è®¸å˜åŒ– Â±0.4ï¼›
                # # å½“ V_oldâ‰ˆ0 æ—¶ï¼Œä¼šå¤ªå°ï¼Œå¯ä»¥åŠ ä¸€ä¸ªä¸‹é™ï¼š
                # scale = torch.clamp(torch.abs(old_value_from_buffer), min=1.0)  # é˜²æ­¢å¤ªå°
                # value_clipped = old_value_from_buffer + torch.clamp(
                #     new_value - old_value_from_buffer,
                #     -AGENTPARA.epsilon * scale,
                #     +AGENTPARA.epsilon * scale
                # )
                #
                # # 5ï¸âƒ£ --- è®¡ç®—ä¸¤ç§è¯¯å·® ---
                # # æ™®é€š MSE loss
                # value_losses = (new_value - return_) ** 2
                # # è£å‰ªåçš„ lossï¼ˆä½¿ç”¨è£å‰ªå€¼ï¼‰
                # value_losses_clipped = (value_clipped - return_) ** 2
                #
                # # 6ï¸âƒ£ --- å–ä¸¤è€…çš„æœ€å¤§å€¼ï¼ˆé˜²æ­¢è¿‡åº¦æ›´æ–°ï¼‰---
                # critic_loss = 0.5 * torch.max(value_losses, value_losses_clipped).mean()
                #
                # # 7ï¸âƒ£ --- ä¼˜åŒ–å™¨æ›´æ–° ---
                # self.Critic.optim.zero_grad()
                # critic_loss.backward()
                # torch.nn.utils.clip_grad_norm_(self.Critic.parameters(), max_norm=1.0)
                # self.Critic.optim.step()

                # 8. è®°å½•è®­ç»ƒä¿¡æ¯
                train_info['critic_loss'].append(critic_loss.item())
                train_info['actor_loss'].append(actor_loss.item())
                train_info['dist_entropy'].append(total_entropy.item())
                train_info['entropy_cont'].append(entropy_cont.mean().item())
                train_info['adv_targ'].append(advantage.mean().item())
                train_info['ratio'].append(ratio.mean().item())

        self.buffer.clear_memory()
        for key in train_info: train_info[key] = np.mean(train_info[key])
        train_info['actor_lr'] = self.Actor.optim.param_groups[0]['lr']
        train_info['critic_lr'] = self.Critic.optim.param_groups[0]['lr']
        self.save()
        return train_info

    def prep_training_rl(self):
        """å°†ç½‘ç»œè®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼"""
        self.Actor.train()
        self.Critic.train()

    def prep_eval_rl(self):
        """å°†ç½‘ç»œè®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼"""
        self.Actor.eval()
        self.Critic.eval()

    def save(self, prefix=""):
        """
        ä¿å­˜ Actor å’Œ Critic æ¨¡å‹çš„æƒé‡ã€‚
        - å¦‚æœæä¾›äº† prefixï¼Œåˆ™ä¿å­˜åˆ° 'èƒœç‡æ¨¡å‹' å­ç›®å½•ä¸­ã€‚
        - å¦åˆ™ï¼Œä¿å­˜åˆ°ä¸»è¿è¡Œç›®å½•ä¸­ã€‚
        """
        # --- 3. åˆå§‹åŒ–æ—¶åˆ›å»ºæ‰€æœ‰éœ€è¦çš„ç›®å½• (æ¨èåšæ³•) ---
        try:
            os.makedirs(self.run_save_dir, exist_ok=True)
            os.makedirs(self.win_rate_dir, exist_ok=True)
            print(f"è®­ç»ƒå­˜æ¡£ç›®å½•å·²åˆ›å»º: {self.run_save_dir}")
        except Exception as e:
            print(f"åˆ›å»ºå­˜æ¡£ç›®å½•å¤±è´¥: {e}")

        # --- 1. æ ¹æ® prefix ç¡®å®šç›®æ ‡ä¿å­˜ç›®å½• (é€»è¾‘æ›´æ¸…æ™°) ---
        if prefix:
            target_dir = self.win_rate_dir
            print(f"èƒœç‡æ¨¡å‹å°†è¢«ä¿å­˜è‡³: {target_dir}")
        else:
            target_dir = self.run_save_dir
            print(f"å¸¸è§„æ¨¡å‹å°†è¢«ä¿å­˜è‡³: {target_dir}")

        # --- 2. å¾ªç¯ä¿å­˜æ¨¡å‹ (ä»£ç æ— é‡å¤) ---
        for net_name in ['Actor', 'Critic']:
            try:
                # è·å–æ¨¡å‹å¯¹è±¡
                net_model = getattr(self, net_name)

                # æ„é€ æ–‡ä»¶å
                filename = f"{prefix}_{net_name}.pkl" if prefix else f"{net_name}.pkl"
                full_path = os.path.join(target_dir, filename)

                # ä¿å­˜æ¨¡å‹çš„çŠ¶æ€å­—å…¸
                torch.save(net_model.state_dict(), full_path)
                print(f"  - {filename} ä¿å­˜æˆåŠŸã€‚")

            except AttributeError:
                print(f"  - é”™è¯¯: æ‰¾ä¸åˆ°åä¸º '{net_name}' çš„æ¨¡å‹ã€‚")
            except Exception as e:
                print(f"  - ä¿å­˜æ¨¡å‹ {net_name} åˆ° {full_path} æ—¶å‘ç”Ÿé”™è¯¯: {e}")

    # def save(self, prefix=""):
    #     """å°†æ¨¡å‹ä¿å­˜åˆ°ä»¥è®­ç»ƒå¼€å§‹æ—¶é—´å‘½åçš„ä¸“å±æ–‡ä»¶å¤¹ä¸­ã€‚"""
    #     try:
    #         os.makedirs(self.run_save_dir, exist_ok=True)
    #         print(f"æ¨¡å‹å°†è¢«ä¿å­˜è‡³: {self.run_save_dir}")
    #     except Exception as e:
    #         print(f"åˆ›å»ºæ¨¡å‹æ–‡ä»¶å¤¹ {self.run_save_dir} å¤±è´¥: {e}")
    #         return
    #
    #     for net in ['Actor', 'Critic']:
    #         try:
    #             filename = f"{prefix}_{net}.pkl" if prefix else f"{net}.pkl"
    #             full_path = os.path.join(self.run_save_dir, filename)
    #             torch.save(getattr(self, net).state_dict(), full_path)
    #             print(f"  - {filename} ä¿å­˜æˆåŠŸã€‚")
    #         except Exception as e:
    #             print(f"  - ä¿å­˜æ¨¡å‹ {net} åˆ° {full_path} æ—¶å‘ç”Ÿé”™è¯¯: {e}")