# --- START OF FILE Hybrid_PPO_jsbsim_SeparateHeads.py ---

import torch
from torch.nn import *
from torch.distributions import Bernoulli, Categorical
from torch.distributions import Normal
# å¯¼å…¥é…ç½®æ–‡ä»¶ï¼Œå…¶ä¸­åŒ…å«å„ç§è¶…å‚æ•°
from Interference_code.PPO_model.PPO_evasion_fuza.ConfigAttn import *
# å¯¼å…¥æ”¯æŒ GRU çš„ç»éªŒå›æ”¾æ± 
from Interference_code.PPO_model.PPO_evasion_fuza.BufferGRUAttn import *
from torch.optim import lr_scheduler
import numpy as np
import os
import re
import time
# å¯¼å…¥ PyTorch çš„è‡ªåŠ¨æ··åˆç²¾åº¦è®­ç»ƒå·¥å…·ï¼Œç”¨äºåŠ é€Ÿè®­ç»ƒå¹¶å‡å°‘æ˜¾å­˜å ç”¨
from torch.cuda.amp import GradScaler, autocast

# --- åŠ¨ä½œç©ºé—´é…ç½® (ä¸åŸç‰ˆç›¸åŒ) ---
# å®šä¹‰è¿ç»­åŠ¨ä½œçš„ç»´åº¦å’Œé”®å
CONTINUOUS_DIM = 4
CONTINUOUS_ACTION_KEYS = ['throttle', 'elevator', 'aileron', 'rudder']
# å®šä¹‰ç¦»æ•£åŠ¨ä½œçš„ç»´åº¦ã€‚æ¯ä¸ªé”®ä»£è¡¨ä¸€ä¸ªç¦»æ•£å†³ç­–ï¼Œå€¼ä»£è¡¨è¯¥å†³ç­–æœ‰å¤šå°‘ä¸ªé€‰é¡¹ã€‚
DISCRETE_DIMS = {
    'flare_trigger': 1,  # å¹²æ‰°å¼¹è§¦å‘ï¼Œä¼¯åŠªåˆ©åˆ†å¸ƒ (æ˜¯/å¦)ï¼Œæ‰€ä»¥æ˜¯1ä¸ª logit
    'salvo_size': 3,  # é½å°„æ•°é‡ï¼Œ3ä¸ªé€‰é¡¹
    'intra_interval': 3,  # ç»„å†…é—´éš”ï¼Œ3ä¸ªé€‰é¡¹
    'num_groups': 3,  # ç»„æ•°ï¼Œ3ä¸ªé€‰é¡¹
    'inter_interval': 3,  # ç»„é—´é—´éš”ï¼Œ3ä¸ªé€‰é¡¹
}
# è®¡ç®—æ‰€æœ‰ç¦»æ•£åŠ¨ä½œ logits çš„æ€»ç»´åº¦
TOTAL_DISCRETE_LOGITS = sum(DISCRETE_DIMS.values())
# è®¡ç®—å­˜å‚¨åœ¨ Buffer ä¸­çš„æ€»åŠ¨ä½œç»´åº¦ï¼ˆè¿ç»­åŠ¨ä½œ + ç¦»æ•£åŠ¨ä½œçš„ç±»åˆ«ç´¢å¼•ï¼‰
TOTAL_ACTION_DIM_BUFFER = CONTINUOUS_DIM + len(DISCRETE_DIMS)
# ç¦»æ•£åŠ¨ä½œçš„ç±»åˆ«ç´¢å¼•åˆ°å®é™…ç‰©ç†å€¼çš„æ˜ å°„
DISCRETE_ACTION_MAP = {
    # 'salvo_size': [2, 4, 6],
    # 'intra_interval': [0.02, 0.04, 0.1],
    # 'num_groups': [1, 2, 3],
    # 'inter_interval': [0.5, 1.0, 2.0]
    'salvo_size': [1, 2, 3],  # ä¿®æ”¹ä¸ºå‘å°„1ã€2ã€3æš
    # 'intra_interval': [0.05, 0.1, 0.15],
    'intra_interval': [0.02, 0.04, 0.08],
    'num_groups': [1, 2, 3],
    'inter_interval': [0.2, 0.5, 1.0]
}
# è¿ç»­åŠ¨ä½œçš„ç‰©ç†èŒƒå›´ï¼Œç”¨äºå°†ç½‘ç»œè¾“å‡º (-1, 1) ç¼©æ”¾åˆ°å®é™…èŒƒå›´
ACTION_RANGES = {
    'throttle': {'low': 0.0, 'high': 1.0},
    'elevator': {'low': -1.0, 'high': 1.0},
    'aileron': {'low': -1.0, 'high': 1.0},
    'rudder': {'low': -1.0, 'high': 1.0},
}

# <<< GRU/RNN/Attention ä¿®æ”¹ >>>: æ–°å¢ RNN å’Œ Attention é…ç½®
RNN_HIDDEN_SIZE = 256  # GRU å±‚çš„éšè—å•å…ƒæ•°é‡
SEQUENCE_LENGTH = 10  # è®­ç»ƒæ—¶ä»ç»éªŒæ± ä¸­é‡‡æ ·çš„è¿ç»­è½¨è¿¹ç‰‡æ®µçš„é•¿åº¦
# ATTN_NUM_HEADS = 8     # æ³¨æ„åŠ›æœºåˆ¶çš„å¤´æ•° (å¿…é¡»èƒ½è¢« MLP è¾“å‡ºç»´åº¦æ•´é™¤)
ATTN_NUM_HEADS = 2 #3 #4 #8 #4 #1 #2       # <<< è¿™æ˜¯æ‚¨çš„å…³é”®ä¿®æ”¹

# ==============================================================================
# Original MLP-based Actor and Critic (ä¿ç•™åŸå§‹ç‰ˆæœ¬ä»¥ä¾›é€‰æ‹©)
# ==============================================================================

class Actor(Module):
    # ... åŸç‰ˆ Actor ä»£ç ä¿æŒä¸å˜ ...
    """
       Actor ç½‘ç»œ (ç­–ç•¥ç½‘ç»œ) - åŸºäº MLPï¼ˆå¤šå±‚æ„ŸçŸ¥æœºï¼‰çš„ç‰ˆæœ¬ã€‚
       è¯¥ç½‘ç»œè´Ÿè´£æ ¹æ®å½“å‰çŠ¶æ€å†³å®šè¦é‡‡å–çš„åŠ¨ä½œç­–ç•¥ã€‚
       å®ƒå…·æœ‰ä¸€ä¸ªå…±äº«çš„éª¨å¹²ç½‘ç»œï¼Œåæ¥ä¸¤ä¸ªç‹¬ç«‹çš„å¤´éƒ¨ï¼Œåˆ†åˆ«å¤„ç†è¿ç»­åŠ¨ä½œå’Œç¦»æ•£åŠ¨ä½œã€‚
       """

    def __init__(self):
        super(Actor, self).__init__()
        self.input_dim = ACTOR_PARA.input_dim
        # <<< æ›´æ”¹ >>> è¾“å‡ºç»´åº¦ç°åœ¨æ˜¯ (è¿ç»­*2) + æ–°çš„logitsæ€»æ•°
        # æ³¨æ„ï¼šè¿™é‡Œä¸å†éœ€è¦ä¸€ä¸ªæ€»çš„ output_dim
        # self.output_dim = (CONTINUOUS_DIM * 2) + TOTAL_DISCRETE_LOGITS
        self.log_std_min = -20.0 # é™åˆ¶å¯¹æ•°æ ‡å‡†å·®çš„æœ€å°å€¼ï¼Œé˜²æ­¢æ•°å€¼ä¸ç¨³å®š
        self.log_std_max = 2.0 # é™åˆ¶å¯¹æ•°æ ‡å‡†å·®çš„æœ€å¤§å€¼

        # å®šä¹‰å…±äº«éª¨å¹²ç½‘ç»œ
        # è´Ÿè´£ä»åŸå§‹çŠ¶æ€ä¸­æå–é«˜çº§ç‰¹å¾
        shared_layers_dims = [self.input_dim] + ACTOR_PARA.model_layer_dim
        self.shared_network = Sequential()
        for i in range(len(shared_layers_dims) - 1):
            # æ·»åŠ çº¿æ€§ï¼ˆå…¨è¿æ¥ï¼‰å±‚
            self.shared_network.add_module(f'fc_{i}', Linear(shared_layers_dims[i], shared_layers_dims[i + 1]))
            # --- åœ¨æ­¤å¤„æ·»åŠ  LayerNorm ---
            # LayerNorm çš„è¾“å…¥ç»´åº¦æ˜¯å‰ä¸€ä¸ªçº¿æ€§å±‚çš„è¾“å‡ºç»´åº¦
            # LayerNorm å¯¹æ¯ä¸ªæ ·æœ¬çš„ç‰¹å¾è¿›è¡Œå½’ä¸€åŒ–ï¼Œæœ‰åŠ©äºç¨³å®šè®­ç»ƒ
            self.shared_network.add_module(f'LayerNorm_{i}', LayerNorm(shared_layers_dims[i + 1]))
            # æ·»åŠ  LeakyReLU æ¿€æ´»å‡½æ•°ï¼Œä»¥é¿å…æ¢¯åº¦æ¶ˆå¤±é—®é¢˜
            self.shared_network.add_module(f'LeakyReLU_{i}', LeakyReLU())

        # éª¨å¹²ç½‘ç»œçš„è¾“å‡ºç»´åº¦ï¼Œå°†ä½œä¸ºå„ä¸ªå¤´éƒ¨çš„è¾“å…¥
        shared_output_dim = ACTOR_PARA.model_layer_dim[-1]
        # --- ç‹¬ç«‹å¤´éƒ¨ç½‘ç»œ ---
        # 1. è¿ç»­åŠ¨ä½œå¤´éƒ¨ï¼šè¾“å‡ºé«˜æ–¯åˆ†å¸ƒçš„å‡å€¼ (mu) å’Œå¯¹æ•°æ ‡å‡†å·® (log_std)ï¼Œæ¯ä¸ªè¿ç»­åŠ¨ä½œç»´åº¦éƒ½éœ€è¦è¿™ä¸¤ä¸ªå‚æ•°
        self.continuous_head = Linear(shared_output_dim, CONTINUOUS_DIM * 2)

        # 2. ç¦»æ•£åŠ¨ä½œå¤´éƒ¨ï¼šè¾“å‡ºæ‰€æœ‰ç¦»æ•£å†³ç­–æ‰€éœ€çš„ logitsï¼ˆæœªç» Softmax çš„åŸå§‹åˆ†æ•°ï¼‰
        self.discrete_head = Linear(shared_output_dim, TOTAL_DISCRETE_LOGITS)

        # self.init_model() # (è¢«æ³¨é‡Šæ‰ï¼Œå› ä¸ºç½‘ç»œç»“æ„åœ¨ init ä¸­ç›´æ¥å®šä¹‰)
        # --- ä¼˜åŒ–å™¨å’Œè®¾å¤‡è®¾ç½® ---
        self.optim = torch.optim.Adam(self.parameters(), ACTOR_PARA.lr)
        self.actor_scheduler = lr_scheduler.LinearLR(
            self.optim,
            start_factor=1.0,# åˆå§‹å­¦ä¹ ç‡å› å­
            end_factor=AGENTPARA.mini_lr / ACTOR_PARA.lr,# æœ€ç»ˆå­¦ä¹ ç‡å› å­
            total_iters=AGENTPARA.MAX_EXE_NUM # è¾¾åˆ°æœ€ç»ˆå­¦ä¹ ç‡æ‰€éœ€çš„æ€»è¿­ä»£æ¬¡æ•°
        )
        self.to(ACTOR_PARA.device) # å°†æ¨¡å‹ç§»åŠ¨åˆ°æŒ‡å®šçš„è®¾å¤‡ (CPU æˆ– GPU)

    def forward(self, obs):
        """
        å‰å‘ä¼ æ’­æ–¹æ³•ï¼Œä¸ºæ¯ä¸ªåŠ¨ä½œç»´åº¦åˆ›å»ºå¹¶è¿”å›ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒã€‚
        """
        # ç¡®ä¿è¾“å…¥æ˜¯ PyTorch å¼ é‡å¹¶ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡
        obs_tensor = check(obs).to(**ACTOR_PARA.tpdv)
        # å¦‚æœè¾“å…¥æ˜¯ä¸€ç»´çš„ï¼ˆå•ä¸ªçŠ¶æ€ï¼‰ï¼Œå¢åŠ ä¸€ä¸ªæ‰¹æ¬¡ç»´åº¦
        if obs_tensor.dim() == 1:
            obs_tensor = obs_tensor.unsqueeze(0)

        # 1. é€šè¿‡å…±äº«éª¨å¹²ç½‘ç»œæå–é€šç”¨ç‰¹å¾
        shared_features = self.shared_network(obs_tensor)

        # 2. å°†å…±äº«ç‰¹å¾åˆ†åˆ«é€å…¥ä¸åŒçš„å¤´éƒ¨ç½‘ç»œ
        cont_params = self.continuous_head(shared_features) # è·å–è¿ç»­åŠ¨ä½œçš„å‚æ•°
        all_disc_logits = self.discrete_head(shared_features) # è·å–æ‰€æœ‰ç¦»æ•£åŠ¨ä½œçš„ logits

        # ... åç»­é€»è¾‘ä¸åŸç‰ˆå®Œå…¨ç›¸åŒ ...
        # æ ¹æ® DISCRETE_DIMS çš„å®šä¹‰ï¼Œå°†æ€»çš„ logits åˆ‡åˆ†æˆå¯¹åº”æ¯ä¸ªç¦»æ•£åŠ¨ä½œçš„éƒ¨åˆ†
        split_sizes = list(DISCRETE_DIMS.values())
        logits_parts = torch.split(all_disc_logits, split_sizes, dim=1)
        trigger_logits, salvo_size_logits, intra_interval_logits, num_groups_logits, inter_interval_logits = logits_parts
        # è·å–çŠ¶æ€ä¸­å…³äºå¹²æ‰°å¼¹æ•°é‡çš„ä¿¡æ¯ï¼ˆç´¢å¼•ä¸º7ï¼‰
        has_flares_info = obs_tensor[:, 7]
        # åˆ›å»ºä¸€ä¸ªæ©ç ï¼Œå½“å¹²æ‰°å¼¹æ•°é‡ä¸º0æ—¶ä¸º True
        mask = (has_flares_info == 0)
        trigger_logits_masked = trigger_logits.clone()
        # å¦‚æœå­˜åœ¨å¹²æ‰°å¼¹æ•°é‡ä¸º0çš„æƒ…å†µï¼Œå°†å¯¹åº”çš„ trigger_logits è®¾ç½®ä¸ºè´Ÿæ— ç©·å¤§
        # è¿™æ ·åœ¨åº”ç”¨ sigmoid/softmax åï¼Œè§¦å‘æ¦‚ç‡ä¼šè¶‹è¿‘äº0ï¼Œå®ç°äº†åŠ¨ä½œå±è”½
        if torch.any(mask):
            trigger_logits_masked[mask] = torch.finfo(torch.float32).min
        # ä»è¿ç»­åŠ¨ä½œå‚æ•°ä¸­åˆ†ç¦»å‡ºå‡å€¼å’Œå¯¹æ•°æ ‡å‡†å·®
        mu, log_std = cont_params.chunk(2, dim=-1)
        # è£å‰ªå¯¹æ•°æ ‡å‡†å·®ä»¥ä¿è¯æ•°å€¼ç¨³å®šæ€§
        log_std = torch.clamp(log_std, self.log_std_min, self.log_std_max)
        # è®¡ç®—æ ‡å‡†å·®
        std = torch.exp(log_std)
        # åˆ›å»ºè¿ç»­åŠ¨ä½œçš„æ­£æ€åˆ†å¸ƒ
        continuous_base_dist = Normal(mu, std)
        # åˆ›å»ºç¦»æ•£åŠ¨ä½œçš„åˆ†å¸ƒ
        trigger_dist = Bernoulli(logits=trigger_logits_masked.squeeze(-1)) # ä¼¯åŠªåˆ©åˆ†å¸ƒ
        salvo_size_dist = Categorical(logits=salvo_size_logits) # åˆ†ç±»åˆ†å¸ƒ
        intra_interval_dist = Categorical(logits=intra_interval_logits)
        num_groups_dist = Categorical(logits=num_groups_logits)
        inter_interval_dist = Categorical(logits=inter_interval_logits)
        # å°†æ‰€æœ‰åˆ†å¸ƒæ‰“åŒ…æˆä¸€ä¸ªå­—å…¸è¿”å›
        distributions = {
            'continuous': continuous_base_dist,
            'trigger': trigger_dist,
            'salvo_size': salvo_size_dist,
            'intra_interval': intra_interval_dist,
            'num_groups': num_groups_dist,
            'inter_interval': inter_interval_dist
        }
        return distributions


class Critic(Module):
    # ... åŸç‰ˆ Critic ä»£ç ä¿æŒä¸å˜ ...
    """
       Critic ç½‘ç»œ (ä»·å€¼ç½‘ç»œ)ï¼Œè¯„ä¼°çŠ¶æ€çš„ä»·å€¼ V(s)ã€‚
       è¿™æ˜¯ä¸€ä¸ªæ ‡å‡†çš„ MLP æ¨¡å‹ï¼Œè´Ÿè´£é¢„æµ‹è¾“å…¥çŠ¶æ€çš„æœŸæœ›å›æŠ¥ã€‚
       """

    def __init__(self):
        super(Critic, self).__init__()
        self.input_dim = CRITIC_PARA.input_dim
        self.output_dim = CRITIC_PARA.output_dim
        self.init_model()
        self.optim = torch.optim.Adam(self.parameters(), CRITIC_PARA.lr)
        self.critic_scheduler = lr_scheduler.LinearLR(
            self.optim,
            start_factor=1.0,
            end_factor=AGENTPARA.mini_lr / CRITIC_PARA.lr,
            total_iters=AGENTPARA.MAX_EXE_NUM
        )
        self.to(CRITIC_PARA.device)

    def init_model(self):
        """åˆå§‹åŒ– Critic çš„ç½‘ç»œç»“æ„ã€‚"""
        self.network = Sequential()
        layers_dims = [self.input_dim] + CRITIC_PARA.model_layer_dim
        for i in range(len(layers_dims) - 1):
            self.network.add_module(f'fc_{i}', Linear(layers_dims[i], layers_dims[i + 1]))
            self.network.add_module(f'LayerNorm_{i}', LayerNorm(layers_dims[i + 1]))
            self.network.add_module(f'LeakyReLU_{i}', LeakyReLU())
        # è¾“å‡ºå±‚ï¼Œè¾“å‡ºä¸€ä¸ªæ ‡é‡å€¼ï¼Œå³çŠ¶æ€ä»·å€¼ V(s)
        self.network.add_module('fc_out', Linear(layers_dims[-1], self.output_dim))

    def forward(self, obs):
        """å‰å‘ä¼ æ’­ï¼Œè®¡ç®—çŠ¶æ€ä»·å€¼ã€‚"""
        obs = check(obs).to(**CRITIC_PARA.tpdv)
        value = self.network(obs)
        return value

# ==============================================================================
# <<< ä¿®æ”¹ >>>: å®šä¹‰æ–°çš„åŸºäº Attention + GRU çš„ Actor å’Œ Critic
#               [ğŸ’¥ æ–°ç»“æ„: GRU -> Attention -> MLP -> Heads]
# ==============================================================================

def init_weights(m, gain=1.0):
    """
    ä¸€ä¸ªé€šç”¨çš„æƒé‡åˆå§‹åŒ–å‡½æ•°ã€‚
    :param m: PyTorch module
    :param gain: æ­£äº¤åˆå§‹åŒ–çš„å¢ç›Šå› å­
    """
    if isinstance(m, Linear):
        # å¯¹çº¿æ€§å±‚ä½¿ç”¨ Kaiming Normal åˆå§‹åŒ–ï¼Œé€‚ç”¨äº LeakyReLU
        torch.nn.init.kaiming_normal_(m.weight, a=0.01, mode='fan_in', nonlinearity='leaky_relu')
        if m.bias is not None:
            torch.nn.init.constant_(m.bias, 0)
    elif isinstance(m, GRU):
        # å¯¹ GRU çš„æƒé‡ä½¿ç”¨æ­£äº¤åˆå§‹åŒ–
        for name, param in m.named_parameters():
            if 'weight_ih' in name:
                torch.nn.init.orthogonal_(param.data, gain=gain)
            elif 'weight_hh' in name:
                torch.nn.init.orthogonal_(param.data, gain=gain)
            elif 'bias' in name:
                param.data.fill_(0)


# ==============================================================================
#           æœ€ç»ˆç‰ˆæœ¬ï¼šç‰¹å¾çº§æ³¨æ„åŠ› + GRUæ—¶åºå»ºæ¨¡
# ==============================================================================
# æ³¨æ„ï¼šæ­¤æ®µä»£ç å‡å®šä½ åœ¨æ–‡ä»¶é¡¶éƒ¨å·²ç»åšäº†å¿…è¦çš„ importï¼Œ
# å¹¶ä¸”ä¿ç•™äº†ä½ åŸæ¥ä»£ç é‡Œä½¿ç”¨çš„åå­—ï¼ˆLinear, GRU, LayerNorm, Dropout, LeakyReLU, Sequential, Module, Normal, Bernoulli, Categorical ç­‰ï¼‰
# ä»¥åŠ ACTOR_PARA, CRITIC_PARA, RNN_HIDDEN_SIZE, ATTN_NUM_HEADS, CONTINUOUS_DIM, TOTAL_DISCRETE_LOGITS, DISCRETE_DIMS, init_weights, check ç­‰å¤–éƒ¨å®šä¹‰

class Actor_GRU(Module):
    """
    Actor ç½‘ç»œ (ç­–ç•¥ç½‘ç»œ) - [ç‰¹å¾çº§ Attention -> GRU -> MLP -> åŠ¨ä½œå¤´]

    è¯¥æ¶æ„çš„æ ¸å¿ƒæ€æƒ³æ˜¯ä¸¤é˜¶æ®µå¤„ç†ï¼š
    1. ç‰¹å¾äº¤äº’é˜¶æ®µï¼ˆç©ºé—´/ç‰¹å¾ç»´åº¦ï¼‰ï¼šåœ¨æ¯ä¸ªç‹¬ç«‹çš„æ—¶é—´æ­¥ï¼Œå°†çŠ¶æ€å‘é‡ä¸­çš„ D ä¸ªç‰¹å¾è§†ä¸ºä¸€ä¸ªåºåˆ—ï¼Œ
       ä½¿ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶è®¡ç®—è¿™äº›ç‰¹å¾ä¹‹é—´çš„åŠ¨æ€å…³ç³»ï¼Œä»è€Œç”Ÿæˆä¸€ä¸ªä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„çŠ¶æ€è¡¨ç¤ºã€‚
    2. æ—¶åºå»ºæ¨¡é˜¶æ®µï¼ˆæ—¶é—´ç»´åº¦ï¼‰ï¼šå°†ç»è¿‡ç‰¹å¾æ³¨æ„åŠ›å¤„ç†åçš„ä¸€ç³»åˆ—çŠ¶æ€è¡¨ç¤ºé€å…¥ GRUï¼Œ
       ä»¥æ•æ‰çŠ¶æ€éšæ—¶é—´å˜åŒ–çš„æ¨¡å¼å’Œè¶‹åŠ¿ã€‚
    """
    """
      Actor ç½‘ç»œ (ç­–ç•¥ç½‘ç»œ) - [ç‰¹å¾çº§ Attention -> GRU -> MLP -> åŠ¨ä½œå¤´]

      æ¶æ„ç‰¹ç‚¹ï¼š
      - ç‰¹å¾äº¤äº’ï¼šåœ¨æ¯ä¸ªæ—¶é—´æ­¥ï¼Œå°†Dä¸ªç‰¹å¾è§†ä¸ºDä¸ªTokenï¼Œä½¿ç”¨è‡ªæ³¨æ„åŠ›è®¡ç®—ç‰¹å¾é—´çš„åŠ¨æ€å…³ç³»ã€‚
      - ç¨³å®šæ€§è®¾è®¡ï¼šé‡‡ç”¨ Pre-and-Post-LN ç»“æ„å¢å¼ºè®­ç»ƒç¨³å®šæ€§ã€‚
      - ç²¾ç»†åŒ–ä¼˜åŒ–ï¼šå¯¹æ³¨æ„åŠ›ç›¸å…³å‚æ•°ä½¿ç”¨ç‹¬ç«‹çš„å­¦ä¹ ç‡å’Œæƒé‡è¡°å‡ã€‚
      """

    def __init__(self, dropout_rate=0.05, weight_decay=1e-4):
        super(Actor_GRU, self).__init__()
        # --- åŸºç¡€å‚æ•°å®šä¹‰ ---
        self.input_dim = ACTOR_PARA.input_dim  # D (åŸå§‹çŠ¶æ€ç‰¹å¾çš„æ•°é‡)
        self.log_std_min = -20.0
        self.log_std_max = 2.0
        self.rnn_hidden_size = RNN_HIDDEN_SIZE
        # å®šä¹‰æ³¨æ„åŠ›æœºåˆ¶å’ŒGRUçš„å·¥ä½œç»´åº¦
        self.embedding_dim = 64 #128 #RNN_HIDDEN_SIZE
        self.weight_decay = weight_decay  # ä¿å­˜é»˜è®¤çš„weight_decay

        # --- æ¨¡å—å®šä¹‰ ---

        # 1. ç‰¹å¾åµŒå…¥å±‚ (Feature Embedding)
        # ä½œç”¨ï¼šå°†æ¯ä¸ªç‹¬ç«‹çš„æ ‡é‡ç‰¹å¾ (ç»´åº¦ä¸º1) æŠ•å½±åˆ°ä¸€ä¸ªé«˜ç»´çš„åµŒå…¥ç©ºé—´ (embedding_dim)ã€‚
        # è¿™æ˜¯å®ç°ç‰¹å¾TokenåŒ–çš„å…³é”®ä¸€æ­¥ã€‚
        self.feature_embed = Linear(1, self.embedding_dim)

        # 2. ç‰¹å¾çº§è‡ªæ³¨æ„åŠ›å±‚ (Feature-wise Self-Attention)
        # ä½œç”¨ï¼šåœ¨ç‰¹å¾ç»´åº¦ä¸Šè¿›è¡Œäº¤äº’ï¼Œè®¡ç®— D ä¸ªç‰¹å¾Tokenä¹‹é—´çš„ç›¸äº’å…³ç³»ã€‚
        # embed_dim æ˜¯æ¯ä¸ªç‰¹å¾Tokenè¢«æŠ•å½±åçš„ç»´åº¦ã€‚
        assert self.embedding_dim % ATTN_NUM_HEADS == 0, "embedding_dim must be divisible by num_heads"
        self.attention = MultiheadAttention(embed_dim=self.embedding_dim,
                                            num_heads=ATTN_NUM_HEADS,
                                            dropout=0.1,
                                            batch_first=True)
        self.attn_dropout = Dropout(p=dropout_rate)
        # LayerNorm åœ¨æ³¨æ„åŠ›è®¡ç®—ä¹‹å‰åº”ç”¨ï¼Œä½œç”¨äºæ¯ä¸ªç‰¹å¾Tokençš„åµŒå…¥å‘é‡ä¸Šã€‚
        # self.attention_layernorm = LayerNorm(self.embedding_dim)
        # --- å®šä¹‰ä¸¤ä¸ªLayerNorm ---
        self.attention_layernorm = LayerNorm(self.embedding_dim)  # è¿™ä¸ªä½œä¸º Pre-LN
        self.post_layernorm = LayerNorm(self.embedding_dim)  # <<< âœ… æ–°å¢è¿™ä¸ªä½œä¸º Post-LN >>>

        # 3. GRU æ—¶åºå»ºæ¨¡å±‚
        # ä½œç”¨ï¼šå¤„ç†ç”±ç‰¹å¾æ³¨æ„åŠ›æ¨¡å—è¾“å‡ºçš„æ—¶é—´åºåˆ—ï¼Œæ•æ‰æ—¶é—´ä¾èµ–æ€§ã€‚
        # è¾“å…¥ç»´åº¦æ˜¯ embedding_dimï¼Œå› ä¸ºæ± åŒ–åçš„å‘é‡ç»´åº¦ä¸åµŒå…¥ç»´åº¦ç›¸åŒã€‚
        self.gru = GRU(self.embedding_dim, self.rnn_hidden_size, batch_first=True)

        # 4. å…±äº«MLPéª¨å¹²ç½‘ç»œ
        # ä½œç”¨ï¼šå¯¹GRUè¾“å‡ºçš„æ—¶åºç‰¹å¾è¿›è¡Œè¿›ä¸€æ­¥çš„éçº¿æ€§å˜æ¢ï¼Œæå–æ›´é«˜çº§çš„å†³ç­–ç‰¹å¾ã€‚
        shared_layers_dims = [self.rnn_hidden_size] + ACTOR_PARA.model_layer_dim
        self.shared_network = Sequential()
        for i in range(len(shared_layers_dims) - 1):
            self.shared_network.add_module(f'fc_{i}', Linear(shared_layers_dims[i], shared_layers_dims[i + 1]))
            self.shared_network.add_module(f'LayerNorm_{i}', LayerNorm(shared_layers_dims[i + 1]))
            self.shared_network.add_module(f'LeakyReLU_{i}', LeakyReLU())

        mlp_output_dim = ACTOR_PARA.model_layer_dim[-1]

        # 5. è¾“å‡ºå¤´ (Action Heads)
        # ä½œç”¨ï¼šå°†æœ€ç»ˆçš„ç‰¹å¾æ˜ å°„åˆ°å…·ä½“çš„åŠ¨ä½œåˆ†å¸ƒå‚æ•°ã€‚
        self.mu_head = Linear(mlp_output_dim, CONTINUOUS_DIM)
        self.log_std_param = torch.nn.Parameter(torch.zeros(1, CONTINUOUS_DIM) * -0.5)
        self.discrete_head = Linear(mlp_output_dim, TOTAL_DISCRETE_LOGITS)

        # --- åˆå§‹åŒ–ä¸ä¼˜åŒ–å™¨ ---
        self.apply(init_weights)
        init_range = 3e-3
        self.mu_head.weight.data.uniform_(-init_range, init_range)
        self.mu_head.bias.data.fill_(0)
        self.discrete_head.weight.data.uniform_(-init_range, init_range)
        self.discrete_head.bias.data.fill_(0)

        # --- ç²¾ç»†åŒ–ä¼˜åŒ–å™¨è®¾ç½® ---

        # 1. å®šä¹‰ä¸åŒç»„çš„è¶…å‚æ•°
        attention_weight_decay = 1e-3  # å¯¹æ³¨æ„åŠ›ç›¸å…³å±‚ä½¿ç”¨æ›´å¼ºçš„æƒé‡è¡°å‡
        default_weight_decay = self.weight_decay

        attention_params, other_params = [], []

        # 2. éå†å¹¶åˆ†ç»„å‚æ•°
        for name, param in self.named_parameters():
            if not param.requires_grad:
                continue
            # å°† attention å’Œ layernorm ç›¸å…³çš„å‚æ•°å½’ä¸ºä¸€ç»„
            if any(key in name.lower() for key in ['attention', 'attn', 'layernorm']):
                attention_params.append(param)
            else:
                other_params.append(param)

        # 3. åˆ›å»ºå‚æ•°ç»„åˆ—è¡¨
        param_groups = [
            # ç»„1: æ³¨æ„åŠ›ç›¸å…³å‚æ•°ï¼Œä½¿ç”¨æ›´å¼ºçš„æ­£åˆ™åŒ–å’Œæ›´ä½çš„å­¦ä¹ ç‡
            {'params': attention_params, 'weight_decay': attention_weight_decay, 'lr': ACTOR_PARA.lr * 0.5},
            # ç»„2: å…¶ä»–å‚æ•°ï¼Œä½¿ç”¨é»˜è®¤è®¾ç½®
            {'params': other_params, 'weight_decay': default_weight_decay}
        ]

        # 4. åˆ›å»ºä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨
        self.optim = torch.optim.Adam(param_groups, lr=ACTOR_PARA.lr)  # å…¨å±€lrä½œä¸ºå…¶ä»–ç»„çš„é»˜è®¤å€¼
        self.actor_scheduler = lr_scheduler.LinearLR(
            self.optim,
            start_factor=1.0,
            end_factor=AGENTPARA.mini_lr / ACTOR_PARA.lr,
            total_iters=AGENTPARA.MAX_EXE_NUM
        )
        self.to(ACTOR_PARA.device)

        # æ‰“å°è°ƒè¯•ä¿¡æ¯
        print(f"--- [DEBUG] Actor_GRU Initialized ---")
        print(
            f"[Optimizer] Attention group ({len(attention_params)} params): lr={ACTOR_PARA.lr * 0.5}, decay={attention_weight_decay}")
        print(f"[Optimizer] Other group ({len(other_params)} params): lr={ACTOR_PARA.lr}, decay={default_weight_decay}")

        # self.optim = torch.optim.Adam(self.parameters(), ACTOR_PARA.lr, weight_decay=weight_decay)
        # self.actor_scheduler = lr_scheduler.LinearLR(self.optim, start_factor=1.0,
        #                                              end_factor=AGENTPARA.mini_lr / ACTOR_PARA.lr,
        #                                              total_iters=AGENTPARA.MAX_EXE_NUM)
        # self.to(ACTOR_PARA.device)
        #
        # # æ‰“å°è°ƒè¯•ä¿¡æ¯ï¼Œç¡®è®¤æ¨¡å‹é…ç½®
        # print(f"--- [DEBUG] Actor_GRU initialized with {self.attention.num_heads} attention heads, "
        #       f"num_feature_tokens = {self.input_dim}, feature_embed_dim = {self.embedding_dim} ---")

    def forward(self, obs, hidden_state):
        """
        å‰å‘ä¼ æ’­å‡½æ•°ã€‚
        obs: åŸå§‹çŠ¶æ€è¾“å…¥ï¼Œå½¢çŠ¶ä¸º (B, D) æˆ– (B, S, D)ã€‚
        hidden_state: GRUçš„éšè—çŠ¶æ€ã€‚
        """
        obs_tensor = check(obs).to(**ACTOR_PARA.tpdv)
        # æ£€æŸ¥è¾“å…¥æ˜¯å¦ä¸ºåºåˆ—ï¼Œå¦‚æœä¸æ˜¯ï¼ˆå•æ­¥æ¨ç†ï¼‰ï¼Œåˆ™å¢åŠ ä¸€ä¸ªé•¿åº¦ä¸º1çš„åºåˆ—ç»´åº¦ã€‚
        is_sequence = obs_tensor.dim() == 3
        if not is_sequence:
            obs_tensor = obs_tensor.unsqueeze(1)  # (B, D) -> (B, 1, D)

        B, S, D = obs_tensor.shape

        # --- é˜¶æ®µä¸€ï¼šç‰¹å¾äº¤äº’ ---

        # 1. ç‰¹å¾TokenåŒ–: å°†æ¯ä¸ªæ—¶é—´æ­¥çš„Dä¸ªç‰¹å¾çœ‹ä½œDä¸ªç‹¬ç«‹çš„Tokenã€‚
        # (B, S, D) -> (B*S, D, 1)
        feat_tokens = obs_tensor.contiguous().view(B * S, D, 1)

        # 2. ç‰¹å¾åµŒå…¥: å°†æ¯ä¸ªç»´åº¦ä¸º1çš„TokenæŠ•å½±åˆ°é«˜ç»´ç©ºé—´ã€‚
        # (B*S, D, 1) -> (B*S, D, embedding_dim)
        token_embeds = self.feature_embed(feat_tokens)

        # # 3. ç‰¹å¾çº§è‡ªæ³¨æ„åŠ›: è®¡ç®—Dä¸ªç‰¹å¾Tokenä¹‹é—´çš„å…³ç³»ã€‚
        # # 3a. Pre-LayerNorm: åœ¨é€å…¥æ³¨æ„åŠ›å±‚å‰è¿›è¡Œå½’ä¸€åŒ–ã€‚
        # normed_tokens = self.attention_layernorm(token_embeds)
        # # 3b. Multi-head Self-attention: è¾“å…¥å½¢çŠ¶ä¸º(Batch, SeqLen, EmbedDim)ï¼Œ
        # #    åœ¨è¿™é‡Œ Batch=B*S, SeqLen=D, EmbedDim=embedding_dimã€‚
        # attn_out, _ = self.attention(normed_tokens, normed_tokens, normed_tokens)
        # attn_out = self.attn_dropout(attn_out)  # (B*S, D, embedding_dim)
        #
        # # 3c. æ®‹å·®è¿æ¥ + (å¯é€‰) åå½’ä¸€åŒ–
        # token_context = token_embeds + attn_out

        # ğŸ”‡ å…³é—­æ³¨æ„åŠ›ï¼šç›´æ¥è·³è¿‡
        token_context = token_embeds

        token_context = self.post_layernorm(token_context)  # <<< âœ… æ–°å¢è¿™è¡Œï¼Œå¯é€‰çš„åå½’ä¸€åŒ–å±‚ >>>

        # 4. æ± åŒ–: å°†Dä¸ªäº¤äº’åçš„ç‰¹å¾Tokenèšåˆæˆä¸€ä¸ªå•ä¸€çš„ä¸Šä¸‹æ–‡å‘é‡ã€‚
        # ä½¿ç”¨å¹³å‡æ± åŒ–ï¼Œä½œç”¨åœ¨ç‰¹å¾ç»´åº¦(D)ä¸Šã€‚
        # (B*S, D, embedding_dim) -> (B*S, embedding_dim)
        pooled_context = token_context.mean(dim=1)

        # --- é˜¶æ®µäºŒï¼šæ—¶åºå»ºæ¨¡ ---

        # 5. åºåˆ—åŒ–: å°†å¤„ç†å¥½çš„ä¸Šä¸‹æ–‡å‘é‡æ¢å¤æˆæ—¶é—´åºåˆ—å½¢å¼ã€‚
        # (B*S, embedding_dim) -> (B, S, embedding_dim)
        contextualized_sequence = pooled_context.view(B, S, self.embedding_dim)

        # 6. GRUå¤„ç†: å°†ä¸Šä¸‹æ–‡åºåˆ—é€å…¥GRUè¿›è¡Œæ—¶åºå»ºæ¨¡ã€‚
        # gru_out shape: (B, S, rnn_hidden_size)
        gru_out, new_hidden = self.gru(contextualized_sequence, hidden_state)

        # --- é˜¶æ®µä¸‰ï¼šå†³ç­– ---

        # 7. MLPåŠ å·¥: å¯¹GRUçš„è¾“å‡ºè¿›è¡Œæ·±åº¦åŠ å·¥ã€‚
        mlp_output = self.shared_network(gru_out)

        # å¦‚æœæ˜¯å•æ­¥æ¨ç†ï¼Œç§»é™¤åºåˆ—ç»´åº¦ã€‚
        final_features = mlp_output
        if not is_sequence:
            final_features = final_features.squeeze(1)  # (B, S, mlp_dim) -> (B, mlp_dim)

        # 8. åŠ¨ä½œå¤´: ç”Ÿæˆæœ€ç»ˆçš„åŠ¨ä½œåˆ†å¸ƒã€‚
        mu = self.mu_head(final_features)
        all_disc_logits = self.discrete_head(final_features)

        # (åç»­åŠ¨ä½œåˆ†å¸ƒçš„åˆ›å»ºé€»è¾‘ä¸åŸç‰ˆå®Œå…¨ç›¸åŒ)
        split_sizes = list(DISCRETE_DIMS.values())
        logits_parts = torch.split(all_disc_logits, split_sizes, dim=-1)
        trigger_logits, salvo_size_logits, intra_interval_logits, num_groups_logits, inter_interval_logits = logits_parts

        has_flares_info = obs_tensor[..., 7]
        mask = (has_flares_info == 0)
        trigger_logits_masked = trigger_logits.clone()
        if torch.any(mask):
            if mask.dim() < trigger_logits_masked.dim():
                mask = mask.unsqueeze(-1)
            trigger_logits_masked[mask] = torch.finfo(torch.float32).min

        log_std = torch.clamp(self.log_std_param, self.log_std_min, self.log_std_max)
        std = torch.exp(log_std).expand_as(mu)
        continuous_base_dist = Normal(mu, std)

        trigger_dist = Bernoulli(logits=trigger_logits_masked.squeeze(-1))
        salvo_size_dist = Categorical(logits=salvo_size_logits)
        intra_interval_dist = Categorical(logits=intra_interval_logits)
        num_groups_dist = Categorical(logits=num_groups_logits)
        inter_interval_dist = Categorical(logits=inter_interval_logits)

        distributions = {
            'continuous': continuous_base_dist,
            'trigger': trigger_dist,
            'salvo_size': salvo_size_dist,
            'intra_interval': intra_interval_dist,
            'num_groups': num_groups_dist,
            'inter_interval': inter_interval_dist
        }
        return distributions, new_hidden


class Critic_GRU(Module):
    """
    Critic ç½‘ç»œ (ä»·å€¼ç½‘ç»œ) - é‡‡ç”¨ä¸ Actor å®Œå…¨ç›¸åŒçš„ [ç‰¹å¾çº§ Attention -> GRU -> MLP] ç»“æ„ã€‚
    è¿™ä¿è¯äº† Actor å’Œ Critic å¯¹çŠ¶æ€çš„ç†è§£å’Œå¤„ç†æ–¹å¼æ˜¯ä¸€è‡´çš„ï¼Œæœ‰åŠ©äºç¨³å®šè®­ç»ƒã€‚
    """

    def __init__(self, dropout_rate=0.05, weight_decay=1e-4):
        super(Critic_GRU, self).__init__()
        # --- åŸºç¡€å‚æ•°å®šä¹‰ ---
        self.input_dim = CRITIC_PARA.input_dim
        self.output_dim = CRITIC_PARA.output_dim
        self.rnn_hidden_size = RNN_HIDDEN_SIZE
        self.embedding_dim = 64 #128 #RNN_HIDDEN_SIZE
        self.weight_decay = weight_decay
        # --- æ¨¡å—å®šä¹‰ (ä¸ Actor ç»“æ„ç›¸åŒ) ---

        # 1. ç‰¹å¾åµŒå…¥å±‚
        self.feature_embed = Linear(1, self.embedding_dim)

        # 2. ç‰¹å¾çº§è‡ªæ³¨æ„åŠ›å±‚
        assert self.embedding_dim % ATTN_NUM_HEADS == 0
        self.attention = MultiheadAttention(embed_dim=self.embedding_dim,
                                            num_heads=ATTN_NUM_HEADS,
                                            dropout=0.1,
                                            batch_first=True)
        self.attn_dropout = Dropout(p=dropout_rate)
        # --- å®šä¹‰ä¸¤ä¸ªLayerNorm ---
        self.attention_layernorm = LayerNorm(self.embedding_dim)  # è¿™ä¸ªä½œä¸º Pre-LN
        self.post_layernorm = LayerNorm(self.embedding_dim)  # <<< âœ… æ–°å¢è¿™ä¸ªä½œä¸º Post-LN >>>

        # 3. GRU æ—¶åºå»ºæ¨¡å±‚
        self.gru = GRU(self.embedding_dim, self.rnn_hidden_size, batch_first=True)

        # 4. MLPéª¨å¹²ç½‘ç»œ
        layers_dims = [self.rnn_hidden_size] + CRITIC_PARA.model_layer_dim
        self.network_base = Sequential()
        for i in range(len(layers_dims) - 1):
            self.network_base.add_module(f'fc_{i}', Linear(layers_dims[i], layers_dims[i + 1]))
            self.network_base.add_module(f'LayerNorm_{i}', LayerNorm(layers_dims[i + 1]))
            self.network_base.add_module(f'LeakyReLU_{i}', LeakyReLU())

        # 5. è¾“å‡ºå¤´ (Value Head)
        base_output_dim = CRITIC_PARA.model_layer_dim[-1]
        self.fc_out = Linear(base_output_dim, self.output_dim)

        # --- åˆå§‹åŒ–ä¸ä¼˜åŒ–å™¨ ---
        self.apply(init_weights)
        init_range = 3e-3
        self.fc_out.weight.data.uniform_(-init_range, init_range)
        self.fc_out.bias.data.fill_(0)

        # --- ç²¾ç»†åŒ–ä¼˜åŒ–å™¨è®¾ç½® (ä¸ Actor é€»è¾‘ç›¸åŒ) ---
        attention_weight_decay = 1e-3
        default_weight_decay = self.weight_decay
        attention_params, other_params = [], []

        for name, param in self.named_parameters():
            if not param.requires_grad:
                continue
            if any(key in name.lower() for key in ['attention', 'attn', 'layernorm']):
                attention_params.append(param)
            else:
                other_params.append(param)

        param_groups = [
            {'params': attention_params, 'weight_decay': attention_weight_decay, 'lr': CRITIC_PARA.lr * 0.5},
            {'params': other_params, 'weight_decay': default_weight_decay}
        ]

        self.optim = torch.optim.Adam(param_groups, lr=CRITIC_PARA.lr)
        self.critic_scheduler = lr_scheduler.LinearLR(
            self.optim,
            start_factor=1.0,
            end_factor=AGENTPARA.mini_lr / CRITIC_PARA.lr,
            total_iters=AGENTPARA.MAX_EXE_NUM
        )
        self.to(CRITIC_PARA.device)

        # æ‰“å°è°ƒè¯•ä¿¡æ¯
        print(f"--- [DEBUG] Critic_GRU Initialized ---")
        print(
            f"[Optimizer] Attention group ({len(attention_params)} params): lr={CRITIC_PARA.lr * 0.5}, decay={attention_weight_decay}")
        print(
            f"[Optimizer] Other group ({len(other_params)} params): lr={CRITIC_PARA.lr}, decay={default_weight_decay}")

        # self.optim = torch.optim.Adam(self.parameters(), CRITIC_PARA.lr, weight_decay=weight_decay)
        # self.critic_scheduler = lr_scheduler.LinearLR(self.optim, start_factor=1.0,
        #                                               end_factor=AGENTPARA.mini_lr / CRITIC_PARA.lr,
        #                                               total_iters=AGENTPARA.MAX_EXE_NUM)
        # self.to(CRITIC_PARA.device)
        #
        # # æ‰“å°è°ƒè¯•ä¿¡æ¯
        # print(f"--- [DEBUG] Critic_GRU initialized with {self.attention.num_heads} attention heads, "
        #       f"num_feature_tokens = {self.input_dim}, feature_embed_dim = {self.embedding_dim} ---")

    def forward(self, obs, hidden_state):
        """
        å‰å‘ä¼ æ’­å‡½æ•°ã€‚
        obs: åŸå§‹çŠ¶æ€è¾“å…¥ï¼Œå½¢çŠ¶ä¸º (B, D) æˆ– (B, S, D)ã€‚
        hidden_state: GRUçš„éšè—çŠ¶æ€ã€‚
        """
        obs_tensor = check(obs).to(**CRITIC_PARA.tpdv)
        is_sequence = obs_tensor.dim() == 3
        if not is_sequence:
            obs_tensor = obs_tensor.unsqueeze(1)

        B, S, D = obs_tensor.shape

        # (æ•°æ®æµä¸ Actor å®Œå…¨ç›¸åŒ)
        feat_tokens = obs_tensor.contiguous().view(B * S, D, 1)
        token_embeds = self.feature_embed(feat_tokens)

        # normed_tokens = self.attention_layernorm(token_embeds)
        # attn_out, _ = self.attention(normed_tokens, normed_tokens, normed_tokens)
        # attn_out = self.attn_dropout(attn_out)
        #
        # token_context = token_embeds + attn_out

        # ğŸ”‡ å…³é—­æ³¨æ„åŠ›ï¼šç›´æ¥è·³è¿‡
        token_context = token_embeds

        token_context = self.post_layernorm(token_context)  # <<< âœ… æ–°å¢è¿™è¡Œï¼Œå¯é€‰çš„åå½’ä¸€åŒ–å±‚ >>>
        pooled_context = token_context.mean(dim=1)

        contextualized_sequence = pooled_context.view(B, S, self.embedding_dim)

        gru_out, new_hidden = self.gru(contextualized_sequence, hidden_state)

        # MLP åŠ å·¥å¹¶è¾“å‡ºä»·å€¼
        base_features_sequence = self.network_base(gru_out)
        value = self.fc_out(base_features_sequence)

        if not is_sequence:
            value = value.squeeze(1)

        return value, new_hidden



# ==============================================================================
# <<< ä¿®æ”¹ >>>: å®šä¹‰æ–°çš„åŸºäº Attention + GRU çš„ Actor å’Œ Critic
#               [ğŸ’¥ æ–°ç»“æ„: Attention -> GRU -> MLP -> Heads]
# ==============================================================================

# class Actor_GRU(Module):
#     """
#     Actor ç½‘ç»œ (ç­–ç•¥ç½‘ç»œ) - [æ–°ç»“æ„: Attention -> GRU -> MLP]
#     ç»“æ„ä¸º: è¾“å…¥åµŒå…¥ -> Self-Attention ä¸Šä¸‹æ–‡æ„ŸçŸ¥ -> GRU æ—¶åºå»ºæ¨¡ -> MLP æ·±åº¦åŠ å·¥ -> ç‹¬ç«‹åŠ¨ä½œå¤´ã€‚
#     [æ­£åˆ™åŒ–]: åœ¨ Attention è¾“å‡ºç«¯åº”ç”¨è½»é‡çº§ Dropoutï¼Œå¹¶ä½¿ç”¨ Weight Decayã€‚
#     """
#
#     def __init__(self, dropout_rate=0.05, weight_decay=1e-4):
#         super(Actor_GRU, self).__init__()
#         self.input_dim = ACTOR_PARA.input_dim
#         self.log_std_min = -20.0
#         self.log_std_max = 2.0
#         self.rnn_hidden_size = RNN_HIDDEN_SIZE
#         self.embedding_dim = RNN_HIDDEN_SIZE  # Attention å’Œ GRU çš„å·¥ä½œç»´åº¦
#
#         # 1. è¾“å…¥åµŒå…¥å±‚ (Input Embedding)
#         # ä½œç”¨ï¼šå°†åŸå§‹è¾“å…¥ç»´åº¦æ˜ å°„åˆ°æ›´é«˜ç»´çš„åµŒå…¥ç©ºé—´ï¼Œå¹¶è¿›è¡Œç‰¹å¾æŠ½è±¡
#         self.input_embedding = Sequential(
#             Linear(self.input_dim, self.embedding_dim),
#             LayerNorm(self.embedding_dim),
#             LeakyReLU()
#         )
#
#         # 2. æ³¨æ„åŠ›å±‚ (Self-Attention)
#         assert self.embedding_dim % ATTN_NUM_HEADS == 0
#         self.attention = MultiheadAttention(embed_dim=self.embedding_dim,
#                                             num_heads=ATTN_NUM_HEADS,
#                                             dropout=0.0,  # é¿å…ç­–ç•¥éšæœºæ€§
#                                             batch_first=True)
#         # <<< --- åœ¨è¿™é‡ŒåŠ å…¥è¿™è¡Œä»£ç  --- >>>
#         print(f"--- [DEBUG] Actor_GRU initialized with {self.attention.num_heads} attention heads. ---")
#
#         self.attn_dropout = Dropout(p=dropout_rate)
#         self.attention_layernorm = LayerNorm(self.embedding_dim)
#
#         # 3. GRU å±‚
#         self.gru = GRU(self.embedding_dim, self.rnn_hidden_size, batch_first=True)
#
#         # 4. å…±äº«çš„ MLP éª¨å¹²ç½‘ç»œ
#         shared_layers_dims = [self.rnn_hidden_size] + ACTOR_PARA.model_layer_dim
#         self.shared_network = Sequential()
#         for i in range(len(shared_layers_dims) - 1):
#             self.shared_network.add_module(f'fc_{i}', Linear(shared_layers_dims[i], shared_layers_dims[i + 1]))
#             self.shared_network.add_module(f'LayerNorm_{i}', LayerNorm(shared_layers_dims[i + 1]))
#             self.shared_network.add_module(f'LeakyReLU_{i}', LeakyReLU())
#
#         # # <<< ğŸ’¥ æ¶æ„ä¿®æ”¹å¼€å§‹ >>>
#         #
#         # # 1. ç§»é™¤äº† input_embedding å±‚
#         #
#         # # 2. æ³¨æ„åŠ›å±‚ (Self-Attention) - ç›´æ¥å¤„ç† input_dim
#         # # <<< å…³é”®çº¦æŸ >>>
#         # assert self.input_dim % ATTN_NUM_HEADS == 0, \
#         #     f"Input dimension ({self.input_dim}) must be divisible by the number of attention heads ({ATTN_NUM_HEADS})."
#         #
#         # self.attention = MultiheadAttention(embed_dim=self.input_dim,  # <-- ä½¿ç”¨ input_dim
#         #                                     num_heads=ATTN_NUM_HEADS,
#         #                                     dropout=0.0,
#         #                                     batch_first=True)
#         # self.attn_dropout = Dropout(p=dropout_rate)
#         # self.attention_layernorm = LayerNorm(self.input_dim)  # <-- ä½¿ç”¨ input_dim
#         #
#         # # 3. GRU å±‚
#         # #    è¾“å…¥ç»´åº¦æ˜¯ Attention çš„è¾“å‡ºç»´åº¦ï¼Œå³ self.input_dim
#         # self.gru = GRU(self.input_dim, self.rnn_hidden_size, batch_first=True)
#         #
#         # # 4. å…±äº«çš„ MLP éª¨å¹²ç½‘ç»œ
#         # #    è¾“å…¥ç»´åº¦æ˜¯ GRU çš„è¾“å‡ºç»´åº¦ï¼Œå³ self.rnn_hidden_size
#         # shared_layers_dims = [self.rnn_hidden_size] + ACTOR_PARA.model_layer_dim
#         # self.shared_network = Sequential()
#         # for i in range(len(shared_layers_dims) - 1):
#         #     self.shared_network.add_module(f'fc_{i}', Linear(shared_layers_dims[i], shared_layers_dims[i + 1]))
#         #     self.shared_network.add_module(f'LayerNorm_{i}', LayerNorm(shared_layers_dims[i + 1]))
#         #     self.shared_network.add_module(f'LeakyReLU_{i}', LeakyReLU())
#         #
#         # # <<< ğŸ’¥ æ¶æ„ä¿®æ”¹ç»“æŸ >>>
#
#         mlp_output_dim = ACTOR_PARA.model_layer_dim[-1]
#
#         # 5. ç‹¬ç«‹å¤´éƒ¨ç½‘ç»œ
#         self.mu_head = Linear(mlp_output_dim, CONTINUOUS_DIM)
#         self.log_std_param = torch.nn.Parameter(torch.zeros(1, CONTINUOUS_DIM) * -0.5)
#         self.discrete_head = Linear(mlp_output_dim, TOTAL_DISCRETE_LOGITS)
#
#         # åˆå§‹åŒ–
#         self.apply(init_weights)
#         init_range = 3e-3
#         self.mu_head.weight.data.uniform_(-init_range, init_range)
#         self.mu_head.bias.data.fill_(0)
#         self.discrete_head.weight.data.uniform_(-init_range, init_range)
#         self.discrete_head.bias.data.fill_(0)
#
#         # ä¼˜åŒ–å™¨
#         self.optim = torch.optim.Adam(self.parameters(), ACTOR_PARA.lr)
#         self.actor_scheduler = lr_scheduler.LinearLR(self.optim, start_factor=1.0,
#                                                      end_factor=AGENTPARA.mini_lr / ACTOR_PARA.lr,
#                                                      total_iters=AGENTPARA.MAX_EXE_NUM)
#         self.to(ACTOR_PARA.device)
#
#     def forward(self, obs, hidden_state):
#         obs_tensor = check(obs).to(**ACTOR_PARA.tpdv)
#         is_sequence = obs_tensor.dim() == 3
#         if not is_sequence:
#             obs_tensor = obs_tensor.unsqueeze(1)
#
#         # # 1. è¾“å…¥åµŒå…¥
#         # embedded_input = self.input_embedding(obs_tensor)
#         #
#         # # 2. Self-Attention æ¨¡å— (Pre-LN)
#         # normed_input = self.attention_layernorm(embedded_input)
#         # attn_output, _ = self.attention(normed_input, normed_input, normed_input)
#         # attn_output = self.attn_dropout(attn_output)
#         # contextualized_sequence = embedded_input + attn_output  # æ®‹å·®è¿æ¥
#         #
#         # # 3. GRU æ—¶åºå»ºæ¨¡
#         # gru_out, new_hidden = self.gru(contextualized_sequence, hidden_state)
#
#         # <<< ğŸ’¥ æ–°çš„å‰å‘ä¼ æ’­æµç¨‹ >>>
#
#         # 1. (æ— åµŒå…¥å±‚) ç›´æ¥ä½¿ç”¨ obs_tensor
#
#         # # 2. Self-Attention æ¨¡å— (Pre-LN)
#         # normed_input = self.attention_layernorm(obs_tensor)  # <-- ç›´æ¥å¯¹è¾“å…¥è¿›è¡Œ Norm
#         # attn_output, _ = self.attention(normed_input, normed_input, normed_input)
#         # attn_output = self.attn_dropout(attn_output)
#         # contextualized_sequence = obs_tensor + attn_output  # æ®‹å·®è¿æ¥
#         #
#         # # 3. GRU æ—¶åºå»ºæ¨¡
#         # gru_out, new_hidden = self.gru(contextualized_sequence, hidden_state)
#
#         # --- 1ï¸âƒ£ è¾“å…¥å‡†å¤‡ ---
#         # å¦‚æœæ˜¯åºåˆ—è®­ç»ƒæ¨¡å¼ï¼Œå°±ç›´æ¥è¾“å…¥æ•´æ®µåºåˆ—
#         # å¦‚æœæ˜¯å•æ­¥æ¨ç†æ¨¡å¼ï¼Œåˆ™åŠ ä¸Šæ—¶é—´ç»´åº¦ (B,1,D)
#
#         # --- 2ï¸âƒ£ Attentionï¼ˆåªå¯¹æ¯ä¸ªæ—¶é—´æ­¥å•ç‹¬å¤„ç†ï¼Œä¸è·¨æ—¶åˆ»ï¼‰---
#         # æŒ‰æ—¶é—´æ­¥é€æ­¥å¤„ç†ï¼šç­‰ä»·äºç‰¹å¾æ³¨æ„åŠ›ï¼Œè€Œéæ—¶é—´æ³¨æ„åŠ›
#         # --- 1ï¸âƒ£ è¾“å…¥åµŒå…¥ ---
#         # å°†åŸå§‹è¾“å…¥æ˜ å°„åˆ°æ›´é«˜ç»´çš„åµŒå…¥ç©ºé—´
#         embedded_input = self.input_embedding(obs_tensor)
#
#         # --- 2ï¸âƒ£ çº¯ç‰¹å¾æ³¨æ„åŠ›ï¼ˆåœ¨åµŒå…¥ç©ºé—´ä¸Šï¼Œä¸”è¡Œä¸ºä¸€è‡´ï¼‰---
#         # é€šè¿‡ reshape å¼ºåˆ¶æ³¨æ„åŠ›æœºåˆ¶åªå¤„ç†å•ä¸€æ—¶é—´æ­¥ï¼Œç¡®ä¿è®­ç»ƒå’Œæ¨ç†ä¸€è‡´
#         B, S, D_embed = embedded_input.shape
#         # å°† (B, S, D_embed) å˜å½¢ä¸º (B*S, 1, D_embed)
#         embedded_reshaped = embedded_input.reshape(B * S, 1, D_embed)
#
#         normed_input = self.attention_layernorm(embedded_reshaped)
#         attn_output, _ = self.attention(normed_input, normed_input, normed_input)
#         attn_output = self.attn_dropout(attn_output)
#
#         # æ®‹å·®è¿æ¥
#         contextualized = embedded_reshaped + attn_output
#
#         # å°†å½¢çŠ¶è¿˜åŸå› (B, S, D_embed)
#         contextualized_sequence = contextualized.view(B, S, D_embed)
#
#         # --- 3ï¸âƒ£ GRU æ—¶åºå»ºæ¨¡ï¼ˆè·¨æ—¶é—´ä¾èµ–ï¼‰---
#         gru_out, new_hidden = self.gru(contextualized_sequence, hidden_state)
#
#         # 4. MLP æ·±åº¦åŠ å·¥
#         mlp_output = self.shared_network(gru_out)
#
#         final_features = mlp_output
#         if not is_sequence:
#             final_features = final_features.squeeze(1)
#
#         # 5. åŠ¨ä½œå¤´å’Œåˆ†å¸ƒåˆ›å»º
#         mu = self.mu_head(final_features)
#         all_disc_logits = self.discrete_head(final_features)
#
#         split_sizes = list(DISCRETE_DIMS.values())
#         logits_parts = torch.split(all_disc_logits, split_sizes, dim=-1)
#         trigger_logits, salvo_size_logits, intra_interval_logits, num_groups_logits, inter_interval_logits = logits_parts
#
#         has_flares_info = obs_tensor[..., 7]
#         mask = (has_flares_info == 0)
#         trigger_logits_masked = trigger_logits.clone()
#         if torch.any(mask):
#             if mask.dim() < trigger_logits_masked.dim():
#                 mask = mask.unsqueeze(-1)
#             trigger_logits_masked[mask] = torch.finfo(torch.float32).min
#
#         log_std = torch.clamp(self.log_std_param, self.log_std_min, self.log_std_max)
#         std = torch.exp(log_std).expand_as(mu)
#         continuous_base_dist = Normal(mu, std)
#
#         trigger_dist = Bernoulli(logits=trigger_logits_masked.squeeze(-1))
#         salvo_size_dist = Categorical(logits=salvo_size_logits)
#         intra_interval_dist = Categorical(logits=intra_interval_logits)
#         num_groups_dist = Categorical(logits=num_groups_logits)
#         inter_interval_dist = Categorical(logits=inter_interval_logits)
#
#         distributions = {
#             'continuous': continuous_base_dist,
#             'trigger': trigger_dist,
#             'salvo_size': salvo_size_dist,
#             'intra_interval': intra_interval_dist,
#             'num_groups': num_groups_dist,
#             'inter_interval': inter_interval_dist
#         }
#         return distributions, new_hidden
#
#
# class Critic_GRU(Module):
#     """
#     Critic ç½‘ç»œ (ä»·å€¼ç½‘ç»œ) - [æ–°ç»“æ„: Attention -> GRU -> MLP]
#     ç»“æ„ä¸º: è¾“å…¥åµŒå…¥ -> Self-Attention ä¸Šä¸‹æ–‡æ„ŸçŸ¥ -> GRU æ—¶åºå»ºæ¨¡ -> MLP æ·±åº¦åŠ å·¥ -> ä»·å€¼è¾“å‡ºå¤´ã€‚
#     [æ­£åˆ™åŒ–]: åœ¨ Attention è¾“å‡ºç«¯åº”ç”¨è½»é‡çº§ Dropoutï¼Œå¹¶ä½¿ç”¨ Weight Decayã€‚
#     """
#
#     def __init__(self, dropout_rate=0.05, weight_decay=1e-4):
#         super(Critic_GRU, self).__init__()
#         self.input_dim = CRITIC_PARA.input_dim
#         self.output_dim = CRITIC_PARA.output_dim
#         self.rnn_hidden_size = RNN_HIDDEN_SIZE
#         self.embedding_dim = RNN_HIDDEN_SIZE  # Attention å’Œ GRU çš„å·¥ä½œç»´åº¦
#
#         # 1. è¾“å…¥åµŒå…¥å±‚ (Input Embedding)
#         self.input_embedding = Sequential(
#             Linear(self.input_dim, self.embedding_dim),
#             LayerNorm(self.embedding_dim),
#             LeakyReLU()
#         )
#
#         # 2. æ³¨æ„åŠ›å±‚ (Self-Attention)
#         assert self.embedding_dim % ATTN_NUM_HEADS == 0
#         self.attention = MultiheadAttention(embed_dim=self.embedding_dim,
#                                             num_heads=ATTN_NUM_HEADS,
#                                             dropout=0.0,
#                                             batch_first=True)
#         self.attn_dropout = Dropout(p=dropout_rate)
#         self.attention_layernorm = LayerNorm(self.embedding_dim)
#
#         # 3. GRU å±‚
#         self.gru = GRU(self.embedding_dim, self.rnn_hidden_size, batch_first=True)
#
#         # 4. MLP éª¨å¹²ç½‘ç»œ
#         layers_dims = [self.rnn_hidden_size] + CRITIC_PARA.model_layer_dim
#         self.network_base = Sequential()
#         for i in range(len(layers_dims) - 1):
#             self.network_base.add_module(f'fc_{i}', Linear(layers_dims[i], layers_dims[i + 1]))
#             self.network_base.add_module(f'LayerNorm_{i}', LayerNorm(layers_dims[i + 1]))
#             self.network_base.add_module(f'LeakyReLU_{i}', LeakyReLU())
#
#         # # <<< ğŸ’¥ æ¶æ„ä¿®æ”¹å¼€å§‹ >>>
#         #
#         # # 1. ç§»é™¤äº† input_embedding å±‚
#         #
#         # # 2. æ³¨æ„åŠ›å±‚ (Self-Attention) - ç›´æ¥å¤„ç† input_dim
#         # # <<< å…³é”®çº¦æŸ >>>
#         # assert self.input_dim % ATTN_NUM_HEADS == 0, \
#         #     f"Input dimension ({self.input_dim}) must be divisible by the number of attention heads ({ATTN_NUM_HEADS})."
#         #
#         # self.attention = MultiheadAttention(embed_dim=self.input_dim,  # <-- ä½¿ç”¨ input_dim
#         #                                     num_heads=ATTN_NUM_HEADS,
#         #                                     dropout=0.0,
#         #                                     batch_first=True)
#         # self.attn_dropout = Dropout(p=dropout_rate)
#         # self.attention_layernorm = LayerNorm(self.input_dim)  # <-- ä½¿ç”¨ input_dim
#         #
#         # # 3. GRU å±‚
#         # #    è¾“å…¥ç»´åº¦æ˜¯ Attention çš„è¾“å‡ºç»´åº¦ï¼Œå³ self.input_dim
#         # self.gru = GRU(self.input_dim, self.rnn_hidden_size, batch_first=True)
#         #
#         # # 4. å…±äº«çš„ MLP éª¨å¹²ç½‘ç»œ
#         # #    è¾“å…¥ç»´åº¦æ˜¯ GRU çš„è¾“å‡ºç»´åº¦ï¼Œå³ self.rnn_hidden_size
#         # shared_layers_dims = [self.rnn_hidden_size] + CRITIC_PARA.model_layer_dim
#         # # <<< âœ… ä¿®æ­£: å˜é‡åä» self.shared_network æ”¹ä¸º self.network_base >>>
#         # self.network_base = Sequential()
#         # for i in range(len(shared_layers_dims) - 1):
#         #     self.network_base.add_module(f'fc_{i}', Linear(shared_layers_dims[i], shared_layers_dims[i + 1]))
#         #     self.network_base.add_module(f'LayerNorm_{i}', LayerNorm(shared_layers_dims[i + 1]))
#         #     self.network_base.add_module(f'LeakyReLU_{i}', LeakyReLU())
#         #
#         # # <<< ğŸ’¥ æ¶æ„ä¿®æ”¹ç»“æŸ >>>
#
#         base_output_dim = CRITIC_PARA.model_layer_dim[-1]
#
#         # 5. è¾“å‡ºå¤´
#         self.fc_out = Linear(base_output_dim, self.output_dim)
#
#         # åˆå§‹åŒ–
#         self.apply(init_weights)
#         init_range = 3e-3
#         self.fc_out.weight.data.uniform_(-init_range, init_range)
#         self.fc_out.bias.data.fill_(0)
#
#         # ä¼˜åŒ–å™¨
#         self.optim = torch.optim.Adam(self.parameters(), CRITIC_PARA.lr)
#         self.critic_scheduler = lr_scheduler.LinearLR(self.optim, start_factor=1.0,
#                                                       end_factor=AGENTPARA.mini_lr / CRITIC_PARA.lr,
#                                                       total_iters=AGENTPARA.MAX_EXE_NUM)
#         self.to(CRITIC_PARA.device)
#
#     def forward(self, obs, hidden_state):
#         obs_tensor = check(obs).to(**CRITIC_PARA.tpdv)
#         is_sequence = obs_tensor.dim() == 3
#         if not is_sequence:
#             obs_tensor = obs_tensor.unsqueeze(1)
#
#         # # 1. è¾“å…¥åµŒå…¥
#         # embedded_input = self.input_embedding(obs_tensor)
#         #
#         # # 2. Self-Attention æ¨¡å— (Pre-LN)
#         # normed_input = self.attention_layernorm(embedded_input)
#         # attn_output, _ = self.attention(normed_input, normed_input, normed_input)
#         # attn_output = self.attn_dropout(attn_output)
#         # contextualized_sequence = embedded_input + attn_output  # æ®‹å·®è¿æ¥
#         #
#         # # 3. GRU æ—¶åºå»ºæ¨¡
#         # gru_out, new_hidden = self.gru(contextualized_sequence, hidden_state)
#
#         # <<< ğŸ’¥ æ–°çš„å‰å‘ä¼ æ’­æµç¨‹ >>>
#
#         # 1. (æ— åµŒå…¥å±‚) ç›´æ¥ä½¿ç”¨ obs_tensor
#
#         # # 2. Self-Attention æ¨¡å— (Pre-LN)
#         # normed_input = self.attention_layernorm(obs_tensor)  # <-- ç›´æ¥å¯¹è¾“å…¥è¿›è¡Œ Norm
#         # attn_output, _ = self.attention(normed_input, normed_input, normed_input)
#         # attn_output = self.attn_dropout(attn_output)
#         # contextualized_sequence = obs_tensor + attn_output  # æ®‹å·®è¿æ¥
#         #
#         # # 3. GRU æ—¶åºå»ºæ¨¡
#         # gru_out, new_hidden = self.gru(contextualized_sequence, hidden_state)
#
#         # --- 1ï¸âƒ£ è¾“å…¥å‡†å¤‡ ---
#         # å¦‚æœæ˜¯åºåˆ—è®­ç»ƒæ¨¡å¼ï¼Œå°±ç›´æ¥è¾“å…¥æ•´æ®µåºåˆ—
#         # å¦‚æœæ˜¯å•æ­¥æ¨ç†æ¨¡å¼ï¼Œåˆ™åŠ ä¸Šæ—¶é—´ç»´åº¦ (B,1,D)
#
#         # --- 2ï¸âƒ£ Attentionï¼ˆåªå¯¹æ¯ä¸ªæ—¶é—´æ­¥å•ç‹¬å¤„ç†ï¼Œä¸è·¨æ—¶åˆ»ï¼‰---
#         # æŒ‰æ—¶é—´æ­¥é€æ­¥å¤„ç†ï¼šç­‰ä»·äºç‰¹å¾æ³¨æ„åŠ›ï¼Œè€Œéæ—¶é—´æ³¨æ„åŠ›
#         # --- 1ï¸âƒ£ è¾“å…¥åµŒå…¥ ---
#         # å°†åŸå§‹è¾“å…¥æ˜ å°„åˆ°æ›´é«˜ç»´çš„åµŒå…¥ç©ºé—´
#         embedded_input = self.input_embedding(obs_tensor)
#
#         # --- 2ï¸âƒ£ çº¯ç‰¹å¾æ³¨æ„åŠ›ï¼ˆåœ¨åµŒå…¥ç©ºé—´ä¸Šï¼Œä¸”è¡Œä¸ºä¸€è‡´ï¼‰---
#         # é€šè¿‡ reshape å¼ºåˆ¶æ³¨æ„åŠ›æœºåˆ¶åªå¤„ç†å•ä¸€æ—¶é—´æ­¥ï¼Œç¡®ä¿è®­ç»ƒå’Œæ¨ç†ä¸€è‡´
#         B, S, D_embed = embedded_input.shape
#         # å°† (B, S, D_embed) å˜å½¢ä¸º (B*S, 1, D_embed)
#         embedded_reshaped = embedded_input.reshape(B * S, 1, D_embed)
#
#         normed_input = self.attention_layernorm(embedded_reshaped)
#         attn_output, _ = self.attention(normed_input, normed_input, normed_input)
#         attn_output = self.attn_dropout(attn_output)
#
#         # æ®‹å·®è¿æ¥
#         contextualized = embedded_reshaped + attn_output
#
#         # å°†å½¢çŠ¶è¿˜åŸå› (B, S, D_embed)
#         contextualized_sequence = contextualized.view(B, S, D_embed)
#
#         # --- 3ï¸âƒ£ GRU æ—¶åºå»ºæ¨¡ï¼ˆè·¨æ—¶é—´ä¾èµ–ï¼‰---
#         gru_out, new_hidden = self.gru(contextualized_sequence, hidden_state)
#
#         # 4. MLP æ·±åº¦åŠ å·¥
#         base_features_sequence = self.network_base(gru_out)
#
#         # 5. ä»·å€¼è¾“å‡ºå¤´
#         value = self.fc_out(base_features_sequence)
#
#         if not is_sequence:
#             value = value.squeeze(1)
#
#         return value, new_hidden

# ==============================================================================
# PPO Agent ä¸»ç±»
# ==============================================================================

class PPO_continuous(object):
    """
    PPO æ™ºèƒ½ä½“ä¸»ç±»ã€‚
    é€šè¿‡ `use_rnn` æ ‡å¿—æ¥å†³å®šæ˜¯ä½¿ç”¨ MLP æ¨¡å‹è¿˜æ˜¯ GRU æ¨¡å‹ã€‚
    """
    def __init__(self, load_able: bool, model_dir_path: str = None, use_rnn: bool = False):
        super(PPO_continuous, self).__init__()
        # æ ¹æ® use_rnn æ ‡å¿—ï¼Œå®ä¾‹åŒ–å¯¹åº”çš„ Actor å’Œ Critic ç½‘ç»œ
        self.use_rnn = use_rnn
        if self.use_rnn:
            print("--- åˆå§‹åŒ– PPO Agent (ä½¿ç”¨ [Attention -> GRU -> MLP] æ¨¡å‹) ---")
            # å®ä¾‹åŒ–æ–°çš„ Actor å’Œ Criticï¼Œå¹¶ä¼ å…¥æ­£åˆ™åŒ–å‚æ•°
            self.Actor = Actor_GRU(dropout_rate=0.1, weight_decay=1e-4)
            self.Critic = Critic_GRU(dropout_rate=0.1, weight_decay=1e-4)
        else:
            print("--- åˆå§‹åŒ– PPO Agent (ä½¿ç”¨ MLP æ¨¡å‹) ---")
            self.Actor = Actor()
            self.Critic = Critic()
        # å®ä¾‹åŒ–ç»éªŒå›æ”¾æ± ï¼Œå¹¶å‘ŠçŸ¥å®ƒæ˜¯å¦éœ€è¦å¤„ç† RNN éšè—çŠ¶æ€
        self.buffer = Buffer(use_rnn=self.use_rnn)
        # ä»é…ç½®ä¸­åŠ è½½ PPO ç®—æ³•çš„è¶…å‚æ•°
        self.gamma = AGENTPARA.gamma
        self.gae_lambda = AGENTPARA.lamda
        self.ppo_epoch = AGENTPARA.ppo_epoch
        self.total_steps = 0
        self.training_start_time = time.strftime("PPOGRU_Attn_%Y-%m-%d_%H-%M-%S") # <<< ä¿®æ”¹ >>> æ›´æ–°å­˜æ¡£æ–‡ä»¶å¤¹åç§°
        self.base_save_dir = "../../../save/save_evade_fuza"
        # ä¸ºæœ¬æ¬¡è¿è¡Œåˆ›å»ºä¸€ä¸ªå”¯ä¸€çš„å­˜æ¡£æ–‡ä»¶å¤¹
        self.run_save_dir = os.path.join(self.base_save_dir, self.training_start_time)
        # å¦‚æœéœ€è¦åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        if load_able:
            if model_dir_path:
                print(f"--- æ­£åœ¨ä»æŒ‡å®šæ–‡ä»¶å¤¹åŠ è½½æ¨¡å‹: {model_dir_path} ---")
                self.load_models_from_directory(model_dir_path)
            else:
                print("--- æœªæŒ‡å®šæ¨¡å‹æ–‡ä»¶å¤¹ï¼Œå°è¯•ä»é»˜è®¤æ–‡ä»¶å¤¹ 'test' åŠ è½½ ---")
                self.load_models_from_directory("../../../test/test_evade")

    def load_models_from_directory(self, directory_path: str):
        # This function is correct, no changes needed.
        """ä»æŒ‡å®šç›®å½•åŠ è½½ Actor å’Œ Critic æ¨¡å‹çš„æƒé‡ã€‚"""
        if not os.path.isdir(directory_path):
            print(f"[é”™è¯¯] æ¨¡å‹åŠ è½½å¤±è´¥ï¼šæä¾›çš„è·¯å¾„ '{directory_path}' ä¸æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„æ–‡ä»¶å¤¹ã€‚")
            return
        files = os.listdir(directory_path)
        # ä¼˜å…ˆå°è¯•åŠ è½½å¸¦æœ‰å‰ç¼€çš„æ¨¡å‹æ–‡ä»¶ (ä¾‹å¦‚ "1000_Actor.pkl")
        actor_files_with_prefix = [f for f in files if f.endswith("_Actor.pkl")]
        if len(actor_files_with_prefix) > 0:
            actor_filename = actor_files_with_prefix[0]
            prefix = actor_filename.replace("_Actor.pkl", "")
            critic_filename = f"{prefix}_Critic.pkl"
            print(f"  - æ£€æµ‹åˆ°å‰ç¼€ '{prefix}'ï¼Œå‡†å¤‡åŠ è½½æ¨¡å‹...")
            if critic_filename in files:
                actor_full_path = os.path.join(directory_path, actor_filename)
                critic_full_path = os.path.join(directory_path, critic_filename)
                try:
                    self.Actor.load_state_dict(torch.load(actor_full_path, map_location=ACTOR_PARA.device))
                    print(f"    - æˆåŠŸåŠ è½½ Actor: {actor_full_path}")
                    self.Critic.load_state_dict(torch.load(critic_full_path, map_location=CRITIC_PARA.device))
                    print(f"    - æˆåŠŸåŠ è½½ Critic: {critic_full_path}")
                    return
                except Exception as e:
                    print(f"    - [é”™è¯¯] åŠ è½½å¸¦å‰ç¼€çš„æ¨¡å‹æ—¶å¤±è´¥: {e}")
            else:
                print(f"    - [è­¦å‘Š] æ‰¾åˆ°äº† '{actor_filename}' ä½†æœªæ‰¾åˆ°å¯¹åº”çš„ '{critic_filename}'ã€‚")
        # å¦‚æœæ²¡æœ‰å¸¦å‰ç¼€çš„æ–‡ä»¶ï¼Œåˆ™å°è¯•åŠ è½½é»˜è®¤çš„ "Actor.pkl" å’Œ "Critic.pkl"
        if "Actor.pkl" in files and "Critic.pkl" in files:
            print("  - æ£€æµ‹åˆ°æ— å‰ç¼€æ ¼å¼ï¼Œå‡†å¤‡åŠ è½½ 'Actor.pkl' å’Œ 'Critic.pkl'...")
            actor_full_path = os.path.join(directory_path, "Actor.pkl")
            critic_full_path = os.path.join(directory_path, "Critic.pkl")
            try:
                self.Actor.load_state_dict(torch.load(actor_full_path, map_location=ACTOR_PARA.device))
                print(f"    - æˆåŠŸåŠ è½½ Actor: {actor_full_path}")
                self.Critic.load_state_dict(torch.load(critic_full_path, map_location=CRITIC_PARA.device))
                print(f"    - æˆåŠŸåŠ è½½ Critic: {critic_full_path}")
                return
            except Exception as e:
                print(f"    - [é”™è¯¯] åŠ è½½æ— å‰ç¼€æ¨¡å‹æ—¶å¤±è´¥: {e}")
        print(f"[é”™è¯¯] æ¨¡å‹åŠ è½½å¤±è´¥ï¼šåœ¨æ–‡ä»¶å¤¹ '{directory_path}' ä¸­æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„ Actor/Critic æ¨¡å‹å¯¹ã€‚")

    def scale_action(self, action_cont_tanh):
        """å°† tanh è¾“å‡ºçš„è¿ç»­åŠ¨ä½œ (-1, 1) ç¼©æ”¾åˆ°ç¯å¢ƒå®šä¹‰çš„å®é™…èŒƒå›´ã€‚"""
        lows = torch.tensor([ACTION_RANGES[k]['low'] for k in CONTINUOUS_ACTION_KEYS], **ACTOR_PARA.tpdv)
        highs = torch.tensor([ACTION_RANGES[k]['high'] for k in CONTINUOUS_ACTION_KEYS], **ACTOR_PARA.tpdv)
        scaled_action = lows + (action_cont_tanh + 1.0) * 0.5 * (highs - lows)
        return scaled_action

    def get_initial_hidden_states(self, batch_size=1):
        """ä¸º GRU æ¨¡å‹ç”Ÿæˆåˆå§‹çš„é›¶éšè—çŠ¶æ€ã€‚"""
        if not self.use_rnn:
            return None, None
        actor_hidden = torch.zeros((1, batch_size, self.Actor.rnn_hidden_size), device=ACTOR_PARA.device)
        critic_hidden = torch.zeros((1, batch_size, self.Critic.rnn_hidden_size), device=CRITIC_PARA.device)
        return actor_hidden, critic_hidden

    def store_experience(self, state, action, probs, value, reward, done, actor_hidden=None, critic_hidden=None):
        """å°†å•æ­¥ç»éªŒå­˜å‚¨åˆ° Buffer ä¸­ã€‚"""
        # æ£€æŸ¥ log_prob æ˜¯å¦åŒ…å«æ— æ•ˆå€¼ï¼ˆNaN æˆ– Infï¼‰
        if not np.all(np.isfinite(probs)):
            raise ValueError("åœ¨ log_prob ä¸­æ£€æµ‹åˆ° NaN/Infï¼")
        # å¦‚æœä½¿ç”¨ RNNï¼Œå¿…é¡»æä¾›éšè—çŠ¶æ€
        if self.use_rnn and (actor_hidden is None or critic_hidden is None):
            raise ValueError("ä½¿ç”¨ RNN æ¨¡å‹æ—¶å¿…é¡»å­˜å‚¨éšè—çŠ¶æ€ï¼")
        self.buffer.store_transition(state, value, action, probs, reward, done, actor_hidden, critic_hidden)

    def choose_action(self, state, actor_hidden, critic_hidden, deterministic=False):
        """
        æ ¹æ®å½“å‰çŠ¶æ€é€‰æ‹©åŠ¨ä½œã€‚
        :param deterministic: å¦‚æœä¸º Trueï¼Œåˆ™é€‰æ‹©æœ€å¯èƒ½çš„åŠ¨ä½œï¼ˆç”¨äºè¯„ä¼°ï¼‰ï¼Œå¦åˆ™è¿›è¡Œéšæœºé‡‡æ ·ï¼ˆç”¨äºè®­ç»ƒï¼‰ã€‚
        :return: A tuple containing the action for the environment, action to store in buffer,
                 log probability, state value, and new hidden states.
        """
        state_tensor = torch.as_tensor(state, dtype=torch.float32, device=ACTOR_PARA.device)
        # æ£€æŸ¥è¾“å…¥æ˜¯å¦æ˜¯æ‰¹å¤„ç†æ•°æ®
        is_batch = state_tensor.dim() > 1
        if not is_batch:
            state_tensor = state_tensor.unsqueeze(0)

        with torch.no_grad():# åœ¨æ­¤å—ä¸­ä¸è®¡ç®—æ¢¯åº¦
            if self.use_rnn:
                # GRU æ¨¡å‹éœ€è¦ä¼ å…¥éšè—çŠ¶æ€
                value, new_critic_hidden = self.Critic(state_tensor, critic_hidden)
                dists, new_actor_hidden = self.Actor(state_tensor, actor_hidden)
            else:
                # MLP æ¨¡å‹ä¸éœ€è¦éšè—çŠ¶æ€
                value = self.Critic(state_tensor)
                dists = self.Actor(state_tensor)
                new_critic_hidden, new_actor_hidden = None, None
            # ä»è¿ç»­åŠ¨ä½œåˆ†å¸ƒä¸­é‡‡æ ·
            continuous_base_dist = dists['continuous']
            # u æ˜¯æ­£æ€åˆ†å¸ƒçš„åŸå§‹æ ·æœ¬ï¼Œtanh(u) æ˜¯ä¸ºäº†é™åˆ¶èŒƒå›´å¹¶å¼•å…¥éçº¿æ€§
            u = continuous_base_dist.mean if deterministic else continuous_base_dist.rsample()
            action_cont_tanh = torch.tanh(u)
            log_prob_cont = continuous_base_dist.log_prob(u).sum(dim=-1)
            # ä»æ‰€æœ‰ç¦»æ•£åŠ¨ä½œåˆ†å¸ƒä¸­é‡‡æ ·
            sampled_actions_dict = {}
            for key, dist in dists.items():
                if key == 'continuous': continue
                if deterministic:
                    if isinstance(dist, Categorical): # åˆ†ç±»åˆ†å¸ƒï¼šå–æ¦‚ç‡æœ€å¤§çš„ç±»åˆ«
                        sampled_actions_dict[key] = torch.argmax(dist.probs, dim=-1)
                    else: # ä¼¯åŠªåˆ©åˆ†å¸ƒï¼šå–æ¦‚ç‡å¤§äº0.5çš„
                        # sampled_actions_dict[key] = (dist.probs > 0.5).float()
                        sampled_actions_dict[key] = dist.sample()
                else:
                    sampled_actions_dict[key] = dist.sample()
            # è®¡ç®—æ€»çš„å¯¹æ•°æ¦‚ç‡
            log_prob_disc = sum(dists[key].log_prob(act) for key, act in sampled_actions_dict.items())
            total_log_prob = log_prob_cont + log_prob_disc
            # å‡†å¤‡è¦å­˜å‚¨åœ¨ Buffer ä¸­çš„åŠ¨ä½œï¼ˆè¿ç»­éƒ¨åˆ†æ˜¯åŸå§‹æ ·æœ¬ uï¼Œç¦»æ•£éƒ¨åˆ†æ˜¯ç±»åˆ«ç´¢å¼•ï¼‰
            action_disc_to_store = torch.stack(list(sampled_actions_dict.values()), dim=-1).float()
            action_to_store = torch.cat([u, action_disc_to_store], dim=-1)
            # å‡†å¤‡è¦å‘é€ç»™ç¯å¢ƒçš„åŠ¨ä½œï¼ˆè¿ç»­éƒ¨åˆ†æ˜¯ç¼©æ”¾åçš„åŠ¨ä½œï¼Œç¦»æ•£éƒ¨åˆ†æ˜¯ç±»åˆ«ç´¢å¼•ï¼‰
            env_action_cont = self.scale_action(action_cont_tanh)
            final_env_action_tensor = torch.cat([env_action_cont, action_disc_to_store], dim=-1)
            # å°†æ‰€æœ‰ç»“æœè½¬æ¢ä¸º numpy æ•°ç»„
            value_np = value.cpu().numpy()
            action_to_store_np = action_to_store.cpu().numpy()
            log_prob_to_store_np = total_log_prob.cpu().numpy()
            final_env_action_np = final_env_action_tensor.cpu().numpy()
            # å¦‚æœè¾“å…¥ä¸æ˜¯æ‰¹å¤„ç†ï¼Œåˆ™ç§»é™¤æ‰¹æ¬¡ç»´åº¦
            if not is_batch:
                final_env_action_np = final_env_action_np[0]
                action_to_store_np = action_to_store_np[0]
                log_prob_to_store_np = log_prob_to_store_np[0]
                value_np = value_np[0]

        return final_env_action_np, action_to_store_np, log_prob_to_store_np, value_np, new_actor_hidden, new_critic_hidden

    def cal_gae(self, states, values, actions, probs, rewards, dones):
        """è®¡ç®—å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡ (Generalized Advantage Estimation, GAE)ã€‚"""
        advantage = np.zeros(len(rewards), dtype=np.float32)
        gae = 0
        # ä»åå‘å‰éå†è½¨è¿¹
        for t in reversed(range(len(rewards))):
            # è·å–ä¸‹ä¸€ä¸ªçŠ¶æ€çš„ä»·å€¼ï¼Œå¦‚æœæ˜¯æœ€åä¸€æ­¥ï¼Œåˆ™ä¸º 0
            next_value = values[t + 1] if t < len(rewards) - 1 else 0.0
            # done_mask ç”¨äºåœ¨å›åˆç»“æŸæ—¶åˆ‡æ–­ä»·å€¼çš„ä¼ æ’­
            done_mask = 1.0 - int(dones[t])
            # è®¡ç®— TD-error (delta)
            delta = rewards[t] + self.gamma * next_value * done_mask - values[t]
            # é€’å½’è®¡ç®— GAE
            gae = delta + self.gamma * self.gae_lambda * done_mask * gae
            advantage[t] = gae
        return advantage

    def learn(self):
        """
        æ‰§è¡Œ PPO çš„å­¦ä¹ å’Œæ›´æ–°æ­¥éª¤ã€‚å·²é€‚é… RNN æ¨¡å¼å¹¶ä¿®æ­£æ‰€æœ‰é€»è¾‘å’Œç»´åº¦é”™è¯¯ã€‚
        """
        # å¦‚æœ Buffer ä¸­çš„æ•°æ®ä¸è¶³ä¸€ä¸ªæ‰¹æ¬¡ï¼Œåˆ™è·³è¿‡å­¦ä¹ 
        if self.buffer.get_buffer_size() < BUFFERPARA.BATCH_SIZE:
            print(
                f"  [Info] Buffer size ({self.buffer.get_buffer_size()}) < batch size ({BUFFERPARA.BATCH_SIZE}). Skipping.")
            return None
        # 1. æ•°æ®å‡†å¤‡
        states, values, actions, old_probs, rewards, dones, _, __ = self.buffer.get_all_data()
        advantages = self.cal_gae(states, values, actions, old_probs, rewards, dones)
        # âœ… ç¡®ä¿ values æ˜¯ä¸€ç»´æ•°ç»„ï¼Œä¸ advantages å¯¹é½
        values = np.squeeze(values)  # ç§»é™¤å¤šä½™ç»´åº¦ï¼Œæ¯”å¦‚ (N,1) â†’ (N,)

        # âœ… ä¿è¯ returns ä¸ values å½¢çŠ¶ä¸€è‡´
        # returns (å³ G_t) æ˜¯ä¼˜åŠ¿å‡½æ•°çš„ unbiased estimator
        returns = advantages + values
        # ç”¨äºè®°å½•è®­ç»ƒè¿‡ç¨‹ä¸­çš„å„ç§æŸå¤±å’ŒæŒ‡æ ‡
        train_info = {'critic_loss': [], 'actor_loss': [], 'dist_entropy': [],'entropy_cont': [], 'adv_targ': [], 'ratio': []}
        # 2. PPO è®­ç»ƒå¾ªç¯
        for _ in range(self.ppo_epoch):
            # æ ¹æ®æ˜¯å¦ä½¿ç”¨ RNNï¼Œé€‰æ‹©ä¸åŒçš„æ‰¹æ¬¡ç”Ÿæˆå™¨
            if self.use_rnn:
                # ä¸º RNN ç”Ÿæˆè¿ç»­çš„åºåˆ—æ‰¹æ¬¡
                batch_generator = self.buffer.generate_sequence_batches(
                    SEQUENCE_LENGTH, BUFFERPARA.BATCH_SIZE, advantages, returns
                )
            else:
                # ä¸º MLP ç”Ÿæˆéšæœºçš„æ‰¹æ¬¡ç´¢å¼•
                batch_generator = self.buffer.generate_batches()

            for batch_data in batch_generator:
                # 3. æ‰¹æ¬¡æ•°æ®å¤„ç†
                if self.use_rnn:
                    # è§£åŒ… RNN çš„åºåˆ—æ‰¹æ¬¡æ•°æ®
                    state, action_batch, old_prob, advantage, return_, initial_actor_h, initial_critic_h = batch_data
                    # å°† numpy æ•°ç»„è½¬æ¢ä¸º tensor å¹¶ç§»åŠ¨åˆ°è®¾å¤‡
                    state = check(state).to(**ACTOR_PARA.tpdv)
                    action_batch = check(action_batch).to(**ACTOR_PARA.tpdv)
                    old_prob = check(old_prob).to(**ACTOR_PARA.tpdv)
                    advantage = check(advantage).to(**ACTOR_PARA.tpdv)
                    return_ = check(return_).to(**CRITIC_PARA.tpdv)
                    initial_actor_h = check(initial_actor_h).to(**ACTOR_PARA.tpdv)
                    initial_critic_h = check(initial_critic_h).to(**CRITIC_PARA.tpdv)
                else:
                    # å¤„ç† MLP çš„æ‰¹æ¬¡ç´¢å¼•
                    batch_indices = batch_data
                    state = check(states[batch_indices]).to(**ACTOR_PARA.tpdv)
                    action_batch = check(actions[batch_indices]).to(**ACTOR_PARA.tpdv)
                    old_prob = check(old_probs[batch_indices]).to(**ACTOR_PARA.tpdv).view(-1)
                    advantage = check(advantages[batch_indices]).to(**ACTOR_PARA.tpdv).view(-1, 1)
                    return_ = check(returns[batch_indices]).to(**CRITIC_PARA.tpdv).view(-1, 1)
                # # ä»æ‰¹æ¬¡åŠ¨ä½œä¸­è§£æå‡ºè¿ç»­å’Œç¦»æ•£éƒ¨åˆ†
                # ğŸ’¥ ä¸å†éœ€è¦åˆ‡ç‰‡å‡ºæœ€åä¸€ä¸ªæ—¶é—´æ­¥ï¼Œç›´æ¥ä½¿ç”¨å®Œæ•´çš„åºåˆ—
                u_from_buffer = action_batch[..., :CONTINUOUS_DIM]
                discrete_actions_from_buffer = {
                    'trigger': action_batch[..., CONTINUOUS_DIM],
                    'salvo_size': action_batch[..., CONTINUOUS_DIM + 1].long(), # ç±»åˆ«ç´¢å¼•éœ€è¦æ˜¯ long ç±»å‹
                    'intra_interval': action_batch[..., CONTINUOUS_DIM + 2].long(),
                    'num_groups': action_batch[..., CONTINUOUS_DIM + 3].long(),
                    'inter_interval': action_batch[..., CONTINUOUS_DIM + 4].long(),
                }

                # 4. Actor (ç­–ç•¥) ç½‘ç»œè®­ç»ƒ
                if self.use_rnn:
                    new_dists, _ = self.Actor(state, initial_actor_h)
                else:
                    new_dists = self.Actor(state)
                # è®¡ç®—æ–°ç­–ç•¥ä¸‹ï¼Œæ—§åŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡
                # ğŸ’¥ ç°åœ¨ u_from_buffer å’Œ new_dists['continuous'] çš„å½¢çŠ¶éƒ½æ˜¯ (B, 4)ï¼Œå¯ä»¥åŒ¹é…äº†
                # è®¡ç®— log_probï¼Œæ‰€æœ‰å¼ é‡éƒ½æ˜¯åºåˆ—ï¼Œç»´åº¦å¯ä»¥æ­£ç¡®åŒ¹é…
                new_log_prob_cont = new_dists['continuous'].log_prob(u_from_buffer).sum(dim=-1)
                new_log_prob_disc = sum(
                    new_dists[key].log_prob(discrete_actions_from_buffer[key])
                    for key in discrete_actions_from_buffer
                )
                new_prob = new_log_prob_cont + new_log_prob_disc
                # è®¡ç®—ç­–ç•¥çš„ç†µï¼Œä»¥é¼“åŠ±æ¢ç´¢
                entropy_cont = new_dists['continuous'].entropy().sum(dim=-1)
                total_entropy = (entropy_cont + sum(
                    dist.entropy() for key, dist in new_dists.items() if key != 'continuous'
                )).mean()
                # è®¡ç®—æ–°æ—§ç­–ç•¥çš„æ¯”ç‡ (importance sampling ratio)
                # ğŸ’¥ ä½¿ç”¨å®Œæ•´çš„åºåˆ— old_prob å’Œ advantage
                # æ³¨æ„ï¼šéœ€è¦ç¡®ä¿å®ƒä»¬çš„å½¢çŠ¶ä¸ new_prob åŒ¹é…
                log_ratio = new_prob - old_prob
                ratio = torch.exp(torch.clamp(log_ratio, -20.0, 20.0)) # clamp é˜²æ­¢æ•°å€¼æº¢å‡º
                # advantage å¯èƒ½æ˜¯ (B, S, 1) æˆ– (B, S)ï¼Œéœ€è¦ä¸ ratio (B, S) å¯¹é½
                advantage_squeezed = advantage.squeeze(-1) if advantage.dim() > ratio.dim() else advantage
                surr1 = ratio * advantage_squeezed
                surr2 = torch.clamp(ratio, 1.0 - AGENTPARA.epsilon, 1.0 + AGENTPARA.epsilon) * advantage_squeezed
                # Actor çš„æŸå¤±æ˜¯è£å‰ªåçš„ç›®æ ‡å‡½æ•°çš„è´Ÿå€¼ï¼ŒåŠ ä¸Šç†µçš„æ­£åˆ™é¡¹
                actor_loss = -torch.min(surr1, surr2).mean() - AGENTPARA.entropy * total_entropy
                # åå‘ä¼ æ’­å’Œä¼˜åŒ–
                self.Actor.optim.zero_grad()
                actor_loss.backward()
                torch.nn.utils.clip_grad_norm_(self.Actor.parameters(), max_norm=1.0)  # æ¢¯åº¦è£å‰ª
                self.Actor.optim.step()

                # 5. Critic (ä»·å€¼) ç½‘ç»œè®­ç»ƒ
                if self.use_rnn:
                    # å¯¹äº RNN æ¨¡å‹ï¼ŒCritic è¾“å‡ºçš„æ˜¯èšåˆåçš„å•ä¸€ä»·å€¼
                    # new_value shape: (B, 1)
                    new_value, _ = self.Critic(state, initial_critic_h)
                else:
                    # å¯¹äº MLP æ¨¡å‹ï¼Œé€»è¾‘ä¿æŒä¸å˜
                    # new_value shape: (B, 1)
                    new_value = self.Critic(state)

                # # ####################################################################
                # # # <<< FINAL, DEFINITIVE FIX FOR THE BROADCASTING WARNING >>>
                # # ####################################################################
                # # We ensure the target `return_` tensor has the same number of dimensions
                # # as the network's output `new_value`.
                # # ç¡®ä¿ç›®æ ‡å€¼ `return_` å’Œç½‘ç»œè¾“å‡º `new_value` çš„ç»´åº¦ä¸€è‡´ï¼Œä»¥é¿å… PyTorch çš„å¹¿æ’­è­¦å‘Šã€‚
                # # ä¾‹å¦‚ï¼Œ`new_value` å¯èƒ½æ˜¯ (B, S, 1)ï¼Œè€Œ `return_` æ˜¯ (B, S)ï¼Œè¿™ä¼šå¯¼è‡´ä¸æ˜ç¡®çš„å¹¿æ’­ã€‚
                # # é€šè¿‡ unsqueeze(-1) å°† `return_` å˜ä¸º (B, S, 1)ï¼Œä½¿å…¶å½¢çŠ¶å®Œå…¨åŒ¹é…ã€‚
                # ğŸ’¥ ç¡®ä¿ return_ ç»´åº¦ä¸ new_value åŒ¹é…
                if new_value.dim() > return_.dim():
                    return_ = return_.unsqueeze(-1)
                # ####################################################################
                # Critic çš„æŸå¤±æ˜¯é¢„æµ‹å€¼å’ŒçœŸå®å›æŠ¥ï¼ˆreturnsï¼‰ä¹‹é—´çš„å‡æ–¹è¯¯å·®
                critic_loss = torch.nn.functional.mse_loss(new_value, return_)
                # åå‘ä¼ æ’­å’Œä¼˜åŒ–
                self.Critic.optim.zero_grad()
                critic_loss.backward()
                torch.nn.utils.clip_grad_norm_(self.Critic.parameters(), max_norm=1.0)
                self.Critic.optim.step()
                # è®°å½•è¯¥æ‰¹æ¬¡çš„è®­ç»ƒä¿¡æ¯
                train_info['critic_loss'].append(critic_loss.item())
                train_info['actor_loss'].append(actor_loss.item())
                train_info['dist_entropy'].append(total_entropy.item())
                train_info['entropy_cont'].append(entropy_cont.mean().item())
                train_info['adv_targ'].append(advantage.mean().item())
                train_info['ratio'].append(ratio.mean().item())
            # 6. æ›´æ–°å­¦ä¹ ç‡ã€æ¸…ç†å’Œè¿”å›

            # <<< âœ… åœ¨æ­¤å¤„æ›´æ–° Actor çš„å­¦ä¹ ç‡ >>>
        # åœ¨å®Œæˆæ•´ä¸ªæ‰¹æ¬¡æ•°æ®çš„å­¦ä¹ åï¼Œè®©è°ƒåº¦å™¨æ­¥è¿›ä¸€æ¬¡
        self.Actor.actor_scheduler.step()

        # <<< âœ… åœ¨æ­¤å¤„æ›´æ–° Critic çš„å­¦ä¹ ç‡ >>>
        self.Critic.critic_scheduler.step()

        # 6. æ¸…ç†å’Œè¿”å›
        self.buffer.clear_memory() # å®Œæˆä¸€è½®å­¦ä¹ åæ¸…ç©º Buffer
        # è®¡ç®—æ•´ä¸ª epoch çš„å¹³å‡æŒ‡æ ‡
        for key in train_info:
            train_info[key] = np.mean(train_info[key])
        # è®°å½•å½“å‰å­¦ä¹ ç‡
        # train_info['actor_lr'] = self.Actor.optim.param_groups[0]['lr']
        # train_info['critic_lr'] = self.Critic.optim.param_groups[0]['lr']
        # è®°å½• Actor çš„å­¦ä¹ ç‡
        # param_groups[0] æ˜¯æ³¨æ„åŠ›ç»„, param_groups[1] æ˜¯å…¶ä»–å‚æ•°ï¼ˆä¸»ï¼‰ç»„
        train_info['actor_lr_attn'] = self.Actor.optim.param_groups[0]['lr']
        train_info['actor_lr_main'] = self.Actor.optim.param_groups[1]['lr']

        # è®°å½• Critic çš„å­¦ä¹ ç‡
        train_info['critic_lr_attn'] = self.Critic.optim.param_groups[0]['lr']
        train_info['critic_lr_main'] = self.Critic.optim.param_groups[1]['lr']
        self.save() # ä¿å­˜æ¨¡å‹
        return train_info

    def prep_training_rl(self):
        """å°† Actor å’Œ Critic ç½‘ç»œè®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼ã€‚"""
        self.Actor.train()
        self.Critic.train()

    def prep_eval_rl(self):
        """å°† Actor å’Œ Critic ç½‘ç»œè®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ã€‚"""
        self.Actor.eval()
        self.Critic.eval()

    def save(self, prefix=""):
        """ä¿å­˜ Actor å’Œ Critic æ¨¡å‹çš„æƒé‡ã€‚"""
        try:
            # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
            os.makedirs(self.run_save_dir, exist_ok=True)
            print(f"æ¨¡å‹å°†è¢«ä¿å­˜è‡³: {self.run_save_dir}")
        except Exception as e:
            print(f"åˆ›å»ºæ¨¡å‹æ–‡ä»¶å¤¹ {self.run_save_dir} å¤±è´¥: {e}")
            return
        # åˆ†åˆ«ä¿å­˜ Actor å’Œ Critic
        for net in ['Actor', 'Critic']:
            try:
                filename = f"{prefix}_{net}.pkl" if prefix else f"{net}.pkl"
                full_path = os.path.join(self.run_save_dir, filename)
                torch.save(getattr(self, net).state_dict(), full_path)
                print(f"  - {filename} ä¿å­˜æˆåŠŸã€‚")
            except Exception as e:
                print(f"  - ä¿å­˜æ¨¡å‹ {net} åˆ° {full_path} æ—¶å‘ç”Ÿé”™è¯¯: {e}")