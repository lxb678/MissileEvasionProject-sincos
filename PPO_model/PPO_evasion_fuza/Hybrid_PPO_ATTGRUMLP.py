# å¯¼å…¥ PyTorch æ ¸å¿ƒåº“
import torch
from torch.nn import *
# å¯¼å…¥æ¦‚ç‡åˆ†å¸ƒå·¥å…·ï¼Œç”¨äºæ„å»ºç­–ç•¥ç½‘ç»œçš„è¾“å‡º
from torch.distributions import Bernoulli, Categorical
from torch.distributions import Normal
# å¯¼å…¥é…ç½®æ–‡ä»¶ï¼Œå…¶ä¸­åŒ…å«å„ç§è¶…å‚æ•°
from Interference_code.PPO_model.PPO_evasion_fuza.ConfigAttn import *
# å¯¼å…¥æ”¯æŒ GRU çš„ç»éªŒå›æ”¾æ± 
from Interference_code.PPO_model.PPO_evasion_fuza.BufferGRUAttn import *
# å¯¼å…¥å­¦ä¹ ç‡è°ƒåº¦å™¨
from torch.optim import lr_scheduler
import numpy as np
import os
import re
import time
# å¯¼å…¥ PyTorch çš„è‡ªåŠ¨æ··åˆç²¾åº¦è®­ç»ƒå·¥å…·ï¼Œç”¨äºåŠ é€Ÿè®­ç»ƒå¹¶å‡å°‘æ˜¾å­˜å ç”¨
from torch.cuda.amp import GradScaler, autocast

# --- åŠ¨ä½œç©ºé—´é…ç½® (ä¸åŸç‰ˆç›¸åŒ) ---
# å®šä¹‰è¿ç»­åŠ¨ä½œçš„ç»´åº¦å’Œé”®å
CONTINUOUS_DIM = 4
CONTINUOUS_ACTION_KEYS = ['throttle', 'elevator', 'aileron', 'rudder']# å¯¹åº”é£æœºçš„æ²¹é—¨ã€å‡é™èˆµã€å‰¯ç¿¼ã€æ–¹å‘èˆµ
# å®šä¹‰ç¦»æ•£åŠ¨ä½œçš„ç»´åº¦ã€‚æ¯ä¸ªé”®ä»£è¡¨ä¸€ä¸ªç¦»æ•£å†³ç­–ï¼Œå€¼ä»£è¡¨è¯¥å†³ç­–æœ‰å¤šå°‘ä¸ªé€‰é¡¹ã€‚
DISCRETE_DIMS = {
    'flare_trigger': 1,  # å¹²æ‰°å¼¹è§¦å‘ï¼Œä¼¯åŠªåˆ©åˆ†å¸ƒ (æ˜¯/å¦)ï¼Œæ‰€ä»¥æ˜¯1ä¸ª logit
    'salvo_size': 3,  # é½å°„æ•°é‡ï¼Œ3ä¸ªé€‰é¡¹ï¼Œå¯¹åº”ä¸€ä¸ª3ç±»çš„åˆ†ç±»åˆ†å¸ƒ
    'intra_interval': 3,  # ç»„å†…é—´éš”ï¼Œ3ä¸ªé€‰é¡¹
    'num_groups': 3,  # ç»„æ•°ï¼Œ3ä¸ªé€‰é¡¹
    'inter_interval': 3,  # ç»„é—´é—´éš”ï¼Œ3ä¸ªé€‰é¡¹
}
# è®¡ç®—æ‰€æœ‰ç¦»æ•£åŠ¨ä½œ logits çš„æ€»ç»´åº¦
TOTAL_DISCRETE_LOGITS = sum(DISCRETE_DIMS.values())
# è®¡ç®—å­˜å‚¨åœ¨ Buffer ä¸­çš„æ€»åŠ¨ä½œç»´åº¦ï¼ˆè¿ç»­åŠ¨ä½œ + ç¦»æ•£åŠ¨ä½œçš„ç±»åˆ«ç´¢å¼•ï¼‰
TOTAL_ACTION_DIM_BUFFER = CONTINUOUS_DIM + len(DISCRETE_DIMS)
# ç¦»æ•£åŠ¨ä½œçš„ç±»åˆ«ç´¢å¼•åˆ°å®é™…ç‰©ç†å€¼çš„æ˜ å°„
DISCRETE_ACTION_MAP = {
    # 'salvo_size': [2, 4, 6],
    # 'intra_interval': [0.02, 0.04, 0.1],
    # 'num_groups': [1, 2, 3],
    # 'inter_interval': [0.5, 1.0, 2.0]
    'salvo_size': [1, 2, 3],  # ä¿®æ”¹ä¸ºå‘å°„1ã€2ã€3æš
    # 'intra_interval': [0.05, 0.1, 0.15],
    'intra_interval': [0.02, 0.04, 0.08],
    'num_groups': [1, 2, 3],
    'inter_interval': [0.2, 0.5, 1.0]
}
# è¿ç»­åŠ¨ä½œçš„ç‰©ç†èŒƒå›´ï¼Œç”¨äºå°†ç½‘ç»œè¾“å‡º (-1, 1) ç¼©æ”¾åˆ°å®é™…èŒƒå›´
ACTION_RANGES = {
    'throttle': {'low': 0.0, 'high': 1.0},
    'elevator': {'low': -1.0, 'high': 1.0},
    'aileron': {'low': -1.0, 'high': 1.0},
    'rudder': {'low': -1.0, 'high': 1.0},
}

# <<< GRU/RNN/Attention ä¿®æ”¹ >>>: æ–°å¢ RNN å’Œ Attention é…ç½®
RNN_HIDDEN_SIZE = 256  # GRU å±‚çš„éšè—å•å…ƒæ•°é‡
SEQUENCE_LENGTH = 10  # è®­ç»ƒæ—¶ä»ç»éªŒæ± ä¸­é‡‡æ ·çš„è¿ç»­è½¨è¿¹ç‰‡æ®µçš„é•¿åº¦
# ATTN_NUM_HEADS = 8     # æ³¨æ„åŠ›æœºåˆ¶çš„å¤´æ•° (å¿…é¡»èƒ½è¢« MLP è¾“å‡ºç»´åº¦æ•´é™¤)
ATTN_NUM_HEADS = 2  # 2 #3 #4 #8 #4 #1 #2       # <<< è¿™æ˜¯æ‚¨çš„å…³é”®ä¿®æ”¹ï¼šè®¾ç½®æ³¨æ„åŠ›æœºåˆ¶çš„å¤´æ•°


# ==============================================================================
# Original MLP-based Actor and Critic (ä¿ç•™åŸå§‹ç‰ˆæœ¬ä»¥ä¾›é€‰æ‹©)
# ==============================================================================

class Actor(Module):
    """
    Actor ç½‘ç»œ (ç­–ç•¥ç½‘ç»œ) - åŸºäº MLPï¼ˆå¤šå±‚æ„ŸçŸ¥æœºï¼‰çš„ç‰ˆæœ¬ã€‚
    è¯¥ç½‘ç»œè´Ÿè´£æ ¹æ®å½“å‰çŠ¶æ€å†³å®šè¦é‡‡å–çš„åŠ¨ä½œç­–ç•¥ã€‚
    å®ƒå…·æœ‰ä¸€ä¸ªå…±äº«çš„éª¨å¹²ç½‘ç»œï¼Œåæ¥ä¸¤ä¸ªç‹¬ç«‹çš„å¤´éƒ¨ï¼Œåˆ†åˆ«å¤„ç†è¿ç»­åŠ¨ä½œå’Œç¦»æ•£åŠ¨ä½œã€‚
    """

    def __init__(self):
        """åˆå§‹åŒ– Actor ç½‘ç»œçš„ç»“æ„å’Œä¼˜åŒ–å™¨ã€‚"""
        super(Actor, self).__init__()
        self.input_dim = ACTOR_PARA.input_dim  # è¾“å…¥ç»´åº¦ï¼Œå³çŠ¶æ€ç©ºé—´çš„ç»´åº¦
        # <<< æ›´æ”¹ >>> è¾“å‡ºç»´åº¦ç°åœ¨æ˜¯ (è¿ç»­*2) + æ–°çš„logitsæ€»æ•°
        # æ³¨æ„ï¼šè¿™é‡Œä¸å†éœ€è¦ä¸€ä¸ªæ€»çš„ output_dim
        # self.output_dim = (CONTINUOUS_DIM * 2) + TOTAL_DISCRETE_LOGITS
        self.log_std_min = -20.0  # é™åˆ¶å¯¹æ•°æ ‡å‡†å·®çš„æœ€å°å€¼ï¼Œé˜²æ­¢æ•°å€¼ä¸ç¨³å®š
        self.log_std_max = 2.0  # é™åˆ¶å¯¹æ•°æ ‡å‡†å·®çš„æœ€å¤§å€¼

        # å®šä¹‰å…±äº«éª¨å¹²ç½‘ç»œ
        # è´Ÿè´£ä»åŸå§‹çŠ¶æ€ä¸­æå–é«˜çº§ç‰¹å¾
        shared_layers_dims = [self.input_dim] + ACTOR_PARA.model_layer_dim
        self.shared_network = Sequential()
        for i in range(len(shared_layers_dims) - 1):
            # æ·»åŠ çº¿æ€§ï¼ˆå…¨è¿æ¥ï¼‰å±‚
            self.shared_network.add_module(f'fc_{i}', Linear(shared_layers_dims[i], shared_layers_dims[i + 1]))
            # --- åœ¨æ­¤å¤„æ·»åŠ  LayerNorm ---
            # LayerNorm çš„è¾“å…¥ç»´åº¦æ˜¯å‰ä¸€ä¸ªçº¿æ€§å±‚çš„è¾“å‡ºç»´åº¦
            # LayerNorm å¯¹æ¯ä¸ªæ ·æœ¬çš„ç‰¹å¾è¿›è¡Œå½’ä¸€åŒ–ï¼Œæœ‰åŠ©äºç¨³å®šè®­ç»ƒ
            # self.shared_network.add_module(f'LayerNorm_{i}', LayerNorm(shared_layers_dims[i + 1]))
            # æ·»åŠ  LeakyReLU æ¿€æ´»å‡½æ•°ï¼Œä»¥é¿å…æ¢¯åº¦æ¶ˆå¤±é—®é¢˜
            self.shared_network.add_module(f'LeakyReLU_{i}', LeakyReLU())

        # éª¨å¹²ç½‘ç»œçš„è¾“å‡ºç»´åº¦ï¼Œå°†ä½œä¸ºå„ä¸ªå¤´éƒ¨çš„è¾“å…¥
        shared_output_dim = ACTOR_PARA.model_layer_dim[-1]

        # --- ç‹¬ç«‹å¤´éƒ¨ç½‘ç»œ ---
        # 1. è¿ç»­åŠ¨ä½œå¤´éƒ¨ï¼šè¾“å‡ºé«˜æ–¯åˆ†å¸ƒçš„å‡å€¼ (mu) å’Œå¯¹æ•°æ ‡å‡†å·® (log_std)ï¼Œæ¯ä¸ªè¿ç»­åŠ¨ä½œç»´åº¦éƒ½éœ€è¦è¿™ä¸¤ä¸ªå‚æ•°
        self.continuous_head = Linear(shared_output_dim, CONTINUOUS_DIM * 2)

        # 2. ç¦»æ•£åŠ¨ä½œå¤´éƒ¨ï¼šè¾“å‡ºæ‰€æœ‰ç¦»æ•£å†³ç­–æ‰€éœ€çš„ logitsï¼ˆæœªç» Softmax çš„åŸå§‹åˆ†æ•°ï¼‰
        self.discrete_head = Linear(shared_output_dim, TOTAL_DISCRETE_LOGITS)

        # self.init_model() # (è¢«æ³¨é‡Šæ‰ï¼Œå› ä¸ºç½‘ç»œç»“æ„åœ¨ init ä¸­ç›´æ¥å®šä¹‰)
        # --- ä¼˜åŒ–å™¨å’Œè®¾å¤‡è®¾ç½® ---
        self.optim = torch.optim.Adam(self.parameters(), ACTOR_PARA.lr)  # ä½¿ç”¨ Adam ä¼˜åŒ–å™¨
        # å®šä¹‰å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼Œå®ç°å­¦ä¹ ç‡çš„çº¿æ€§è¡°å‡
        self.actor_scheduler = lr_scheduler.LinearLR(
            self.optim,
            start_factor=1.0,  # åˆå§‹å­¦ä¹ ç‡å› å­
            end_factor=AGENTPARA.mini_lr / ACTOR_PARA.lr,  # æœ€ç»ˆå­¦ä¹ ç‡å› å­
            total_iters=AGENTPARA.MAX_EXE_NUM  # è¾¾åˆ°æœ€ç»ˆå­¦ä¹ ç‡æ‰€éœ€çš„æ€»è¿­ä»£æ¬¡æ•°
        )
        self.to(ACTOR_PARA.device)  # å°†æ¨¡å‹ç§»åŠ¨åˆ°æŒ‡å®šçš„è®¾å¤‡ (CPU æˆ– GPU)

    def forward(self, obs):
        """
        å‰å‘ä¼ æ’­æ–¹æ³•ï¼Œä¸ºæ¯ä¸ªåŠ¨ä½œç»´åº¦åˆ›å»ºå¹¶è¿”å›ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒã€‚
        :param obs: çŠ¶æ€è§‚æµ‹å€¼
        :return: ä¸€ä¸ªåŒ…å«æ‰€æœ‰åŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒçš„å­—å…¸
        """
        # ç¡®ä¿è¾“å…¥æ˜¯ PyTorch å¼ é‡å¹¶ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡
        obs_tensor = check(obs).to(**ACTOR_PARA.tpdv)
        # å¦‚æœè¾“å…¥æ˜¯ä¸€ç»´çš„ï¼ˆå•ä¸ªçŠ¶æ€ï¼‰ï¼Œå¢åŠ ä¸€ä¸ªæ‰¹æ¬¡ç»´åº¦
        if obs_tensor.dim() == 1:
            obs_tensor = obs_tensor.unsqueeze(0)

        # 1. é€šè¿‡å…±äº«éª¨å¹²ç½‘ç»œæå–é€šç”¨ç‰¹å¾
        shared_features = self.shared_network(obs_tensor)

        # 2. å°†å…±äº«ç‰¹å¾åˆ†åˆ«é€å…¥ä¸åŒçš„å¤´éƒ¨ç½‘ç»œ
        cont_params = self.continuous_head(shared_features)  # è·å–è¿ç»­åŠ¨ä½œçš„å‚æ•°
        all_disc_logits = self.discrete_head(shared_features)  # è·å–æ‰€æœ‰ç¦»æ•£åŠ¨ä½œçš„ logits

        # ... åç»­é€»è¾‘ä¸åŸç‰ˆå®Œå…¨ç›¸åŒ ...
        # æ ¹æ® DISCRETE_DIMS çš„å®šä¹‰ï¼Œå°†æ€»çš„ logits åˆ‡åˆ†æˆå¯¹åº”æ¯ä¸ªç¦»æ•£åŠ¨ä½œçš„éƒ¨åˆ†
        split_sizes = list(DISCRETE_DIMS.values())
        logits_parts = torch.split(all_disc_logits, split_sizes, dim=1)
        trigger_logits, salvo_size_logits, intra_interval_logits, num_groups_logits, inter_interval_logits = logits_parts

        # è·å–çŠ¶æ€ä¸­å…³äºå¹²æ‰°å¼¹æ•°é‡çš„ä¿¡æ¯ï¼ˆçŠ¶æ€å‘é‡çš„ç¬¬8ä¸ªå…ƒç´ ï¼Œç´¢å¼•ä¸º7ï¼‰
        has_flares_info = obs_tensor[:, 7]
        # åˆ›å»ºä¸€ä¸ªæ©ç ï¼Œå½“å¹²æ‰°å¼¹æ•°é‡ä¸º0æ—¶ä¸º True
        mask = (has_flares_info == 0)
        trigger_logits_masked = trigger_logits.clone()
        # å¦‚æœå­˜åœ¨å¹²æ‰°å¼¹æ•°é‡ä¸º0çš„æƒ…å†µï¼Œå°†å¯¹åº”çš„ trigger_logits è®¾ç½®ä¸ºè´Ÿæ— ç©·å¤§
        # è¿™æ ·åœ¨åº”ç”¨ sigmoid/softmax åï¼Œè§¦å‘æ¦‚ç‡ä¼šè¶‹è¿‘äº0ï¼Œå®ç°äº†åŠ¨ä½œå±è”½
        if torch.any(mask):
            trigger_logits_masked[mask] = torch.finfo(torch.float32).min

        # ä»è¿ç»­åŠ¨ä½œå‚æ•°ä¸­åˆ†ç¦»å‡ºå‡å€¼å’Œå¯¹æ•°æ ‡å‡†å·®
        mu, log_std = cont_params.chunk(2, dim=-1)
        # è£å‰ªå¯¹æ•°æ ‡å‡†å·®ä»¥ä¿è¯æ•°å€¼ç¨³å®šæ€§
        log_std = torch.clamp(log_std, self.log_std_min, self.log_std_max)
        # è®¡ç®—æ ‡å‡†å·®
        std = torch.exp(log_std)

        # åˆ›å»ºè¿ç»­åŠ¨ä½œçš„æ­£æ€åˆ†å¸ƒ
        continuous_base_dist = Normal(mu, std)
        # åˆ›å»ºç¦»æ•£åŠ¨ä½œçš„åˆ†å¸ƒ
        trigger_dist = Bernoulli(logits=trigger_logits_masked.squeeze(-1))  # ä¼¯åŠªåˆ©åˆ†å¸ƒ
        salvo_size_dist = Categorical(logits=salvo_size_logits)  # åˆ†ç±»åˆ†å¸ƒ
        intra_interval_dist = Categorical(logits=intra_interval_logits)
        num_groups_dist = Categorical(logits=num_groups_logits)
        inter_interval_dist = Categorical(logits=inter_interval_logits)

        # å°†æ‰€æœ‰åˆ†å¸ƒæ‰“åŒ…æˆä¸€ä¸ªå­—å…¸è¿”å›
        distributions = {
            'continuous': continuous_base_dist,
            'trigger': trigger_dist,
            'salvo_size': salvo_size_dist,
            'intra_interval': intra_interval_dist,
            'num_groups': num_groups_dist,
            'inter_interval': inter_interval_dist
        }
        return distributions


class Critic(Module):
    # ... åŸç‰ˆ Critic ä»£ç ä¿æŒä¸å˜ ...
    """
    Critic ç½‘ç»œ (ä»·å€¼ç½‘ç»œ)ï¼Œè¯„ä¼°çŠ¶æ€çš„ä»·å€¼ V(s)ã€‚
    è¿™æ˜¯ä¸€ä¸ªæ ‡å‡†çš„ MLP æ¨¡å‹ï¼Œè´Ÿè´£é¢„æµ‹è¾“å…¥çŠ¶æ€çš„æœŸæœ›å›æŠ¥ã€‚
    """

    def __init__(self):
        """åˆå§‹åŒ– Critic ç½‘ç»œçš„ç»“æ„å’Œä¼˜åŒ–å™¨ã€‚"""
        super(Critic, self).__init__()
        self.input_dim = CRITIC_PARA.input_dim
        self.output_dim = CRITIC_PARA.output_dim
        self.init_model()
        self.optim = torch.optim.Adam(self.parameters(), CRITIC_PARA.lr)
        self.critic_scheduler = lr_scheduler.LinearLR(
            self.optim,
            start_factor=1.0,
            end_factor=AGENTPARA.mini_lr / CRITIC_PARA.lr,
            total_iters=AGENTPARA.MAX_EXE_NUM
        )
        self.to(CRITIC_PARA.device)

    def init_model(self):
        """åˆå§‹åŒ– Critic çš„ç½‘ç»œç»“æ„ã€‚"""
        self.network = Sequential()
        layers_dims = [self.input_dim] + CRITIC_PARA.model_layer_dim
        for i in range(len(layers_dims) - 1):
            self.network.add_module(f'fc_{i}', Linear(layers_dims[i], layers_dims[i + 1]))
            # self.network.add_module(f'LayerNorm_{i}', LayerNorm(layers_dims[i + 1]))
            self.network.add_module(f'LeakyReLU_{i}', LeakyReLU())
        # è¾“å‡ºå±‚ï¼Œè¾“å‡ºä¸€ä¸ªæ ‡é‡å€¼ï¼Œå³çŠ¶æ€ä»·å€¼ V(s)
        self.network.add_module('fc_out', Linear(layers_dims[-1], self.output_dim))

    def forward(self, obs):
        """å‰å‘ä¼ æ’­ï¼Œè®¡ç®—çŠ¶æ€ä»·å€¼ã€‚"""
        obs = check(obs).to(**CRITIC_PARA.tpdv)
        value = self.network(obs)
        return value


def init_weights(m, gain=1.0):
    """
    ä¸€ä¸ªé€šç”¨çš„æƒé‡åˆå§‹åŒ–å‡½æ•°ã€‚
    :param m: PyTorch module
    :param gain: æ­£äº¤åˆå§‹åŒ–çš„å¢ç›Šå› å­
    """
    if isinstance(m, Linear):
        # å¯¹çº¿æ€§å±‚ä½¿ç”¨ Kaiming Normal åˆå§‹åŒ–ï¼Œé€‚ç”¨äº LeakyReLU æ¿€æ´»å‡½æ•°
        torch.nn.init.kaiming_normal_(m.weight, a=0.01, mode='fan_in', nonlinearity='leaky_relu')
        if m.bias is not None:
            # å°†åç½®åˆå§‹åŒ–ä¸º0
            torch.nn.init.constant_(m.bias, 0)
    elif isinstance(m, GRU):
        # å¯¹ GRU çš„æƒé‡ä½¿ç”¨æ­£äº¤åˆå§‹åŒ–ï¼Œæœ‰åŠ©äºç¨³å®š RNN çš„è®­ç»ƒ
        for name, param in m.named_parameters():
            if 'weight_ih' in name:  # è¾“å…¥åˆ°éšè—å±‚çš„æƒé‡
                torch.nn.init.orthogonal_(param.data, gain=gain)
            elif 'weight_hh' in name:  # éšè—å±‚åˆ°éšè—å±‚çš„æƒé‡
                torch.nn.init.orthogonal_(param.data, gain=gain)
            elif 'bias' in name:  # åç½®
                param.data.fill_(0)


# ==============================================================================
#           æœ€ç»ˆç‰ˆæœ¬ï¼šç‰¹å¾çº§æ³¨æ„åŠ› + GRUæ—¶åºå»ºæ¨¡ (ä¿®æ”¹ç‰ˆï¼Œæ— æ± åŒ–ç“¶é¢ˆ)
# ==============================================================================

class Actor_GRU(Module):
    """
    Actor ç½‘ç»œ (ç­–ç•¥ç½‘ç»œ) - [ä¿®æ”¹ç‰ˆ V3: æœ‰åµŒå…¥å±‚, æ— æ± åŒ–ç“¶é¢ˆ]
    æ¶æ„æµç¨‹: [ç‰¹å¾åµŒå…¥ -> åŸå§‹ç‰¹å¾çº§ Attention -> GRU -> MLP -> åŠ¨ä½œå¤´]

    æ¶æ„ç‰¹ç‚¹ï¼š
    - ç‰¹å¾åµŒå…¥ï¼šå°†æ¯ä¸ªçŠ¶æ€ç‰¹å¾ï¼ˆæ ‡é‡ï¼‰æ˜ å°„åˆ°ä¸€ä¸ªé«˜ç»´å‘é‡ç©ºé—´ï¼Œä½¿è‡ªæ³¨æ„åŠ›èƒ½å¤Ÿåœ¨æ­¤ç©ºé—´ä¸­æ•æ‰æ›´ä¸°å¯Œçš„å…³ç³»ã€‚
    - æ— æ± åŒ–ç“¶é¢ˆï¼šæ³¨æ„åŠ›æ¨¡å—çš„è¾“å‡ºï¼ˆæ¯ä¸ªç‰¹å¾çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥åµŒå…¥ï¼‰è¢«å®Œå…¨ä¿ç•™ã€‚å®ƒä»¬è¢«å±•å¹³(flatten)æˆä¸€ä¸ªé•¿å‘é‡ï¼Œ
                  ç„¶åé€å…¥GRUã€‚è¿™é¿å…äº†æ± åŒ–æ“ä½œå¯èƒ½å¯¼è‡´çš„ä¿¡æ¯æŸå¤±ã€‚
    - å¤šå¤´æ³¨æ„åŠ›ï¼šç”±äºç‰¹å¾è¢«åµŒå…¥åˆ°é«˜ç»´ç©ºé—´ï¼Œç°åœ¨å¯ä»¥ä½¿ç”¨å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶æ¥å¹¶è¡Œåœ°å…³æ³¨æ¥è‡ªä¸åŒè¡¨ç¤ºå­ç©ºé—´çš„ä¿¡æ¯ã€‚
    """

    def __init__(self, dropout_rate=0.05, weight_decay=1e-4):
        """åˆå§‹åŒ–ç½‘ç»œçš„å„ä¸ªæ¨¡å—ã€æƒé‡å’Œä¼˜åŒ–å™¨ã€‚"""
        super(Actor_GRU, self).__init__()
        # --- åŸºç¡€å‚æ•°å®šä¹‰ ---
        self.input_dim = ACTOR_PARA.input_dim  # D, çŠ¶æ€ç‰¹å¾çš„æ•°é‡
        self.log_std_min = -20.0
        self.log_std_max = 2.0
        self.rnn_hidden_size = RNN_HIDDEN_SIZE
        self.weight_decay = weight_decay
        self.feature_dim_for_attn = 1  # æ¯ä¸ªç‰¹å¾æœ¬èº«æ˜¯1ç»´æ ‡é‡

        # <<< MODIFIED 1/6 >>>: å®šä¹‰ä¸€ä¸ªæ–°çš„åµŒå…¥ç»´åº¦
        self.embedding_dim = 64  # æ¯ä¸ªç‰¹å¾å°†è¢«æ˜ å°„åˆ°è¿™ä¸ªç»´åº¦

        # <<< MODIFIED 2/6 >>>: æ£€æŸ¥å¤šå¤´æ³¨æ„åŠ›çš„çº¦æŸ
        # å¤šå¤´æ³¨æ„åŠ›çš„ä¸€ä¸ªè¦æ±‚æ˜¯ï¼šåµŒå…¥ç»´åº¦å¿…é¡»èƒ½è¢«å¤´çš„æ•°é‡æ•´é™¤
        assert self.embedding_dim % ATTN_NUM_HEADS == 0, \
            f"embedding_dim ({self.embedding_dim}) must be divisible by ATTN_NUM_HEADS ({ATTN_NUM_HEADS})"

        # --- æ¨¡å—å®šä¹‰ ---
        # <<< ADDED BACK 3/6 >>>: é‡æ–°å¼•å…¥ç‰¹å¾åµŒå…¥å±‚
        # å°†æ¯ä¸ª1ç»´ç‰¹å¾æ˜ å°„åˆ° embedding_dim ç»´
        self.feature_embed = Linear(1, self.embedding_dim)

        # 1. ç‰¹å¾çº§è‡ªæ³¨æ„åŠ›å±‚
        # åœ¨ D ä¸ªç‰¹å¾çš„åµŒå…¥å‘é‡ä¹‹é—´è®¡ç®—æ³¨æ„åŠ›ï¼Œä»¥æ•æ‰ç‰¹å¾é—´çš„ç›¸äº’å…³ç³»
        self.attention = MultiheadAttention(embed_dim=self.embedding_dim,
                                            num_heads=ATTN_NUM_HEADS,
                                            dropout=0.0,  # åœ¨ç­–ç•¥ç½‘ç»œä¸­é€šå¸¸ä¸ä½¿ç”¨ dropout
                                            batch_first=True)  # è¾“å…¥è¾“å‡ºæ ¼å¼ä¸º (Batch, Seq, Feature)
        # self.attention_layernorm = LayerNorm(self.embedding_dim) # å¯é€‰çš„å±‚å½’ä¸€åŒ–

        # <<< ä¿®æ”¹ 1/3 >>>: ç§»é™¤æ± åŒ–ç›¸å…³çš„å±‚
        # self.pooling_query = torch.nn.Parameter(torch.randn(1, self.feature_dim_for_attn))
        # self.pooling_softmax = torch.nn.Softmax(dim=1)

        # 2. GRU æ—¶åºå»ºæ¨¡å±‚
        # <<< MODIFIED 5/6 >>>: GRUçš„è¾“å…¥ç»´åº¦ç°åœ¨æ˜¯ D * embedding_dim
        # å› ä¸ºæ‰€æœ‰ç‰¹å¾çš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥åµŒå…¥è¢«å±•å¹³åé€å…¥GRU
        gru_input_dim = self.input_dim * self.embedding_dim
        self.gru = GRU(gru_input_dim, self.rnn_hidden_size, batch_first=True)

        # 3. å…±äº«MLPéª¨å¹²ç½‘ç»œ (ä¸å˜)
        # å¯¹ GRU çš„æ—¶åºè¾“å‡ºè¿›è¡Œæ·±åº¦åŠ å·¥
        shared_layers_dims = [self.rnn_hidden_size] + ACTOR_PARA.model_layer_dim
        self.shared_network = Sequential()
        for i in range(len(shared_layers_dims) - 1):
            self.shared_network.add_module(f'fc_{i}', Linear(shared_layers_dims[i], shared_layers_dims[i + 1]))
            # self.shared_network.add_module(f'LayerNorm_{i}', LayerNorm(shared_layers_dims[i + 1]))
            self.shared_network.add_module(f'LeakyReLU_{i}', LeakyReLU())
        mlp_output_dim = ACTOR_PARA.model_layer_dim[-1]

        # 4. è¾“å‡ºå¤´ (Action Heads) (ä¸å˜)
        self.mu_head = Linear(mlp_output_dim, CONTINUOUS_DIM)  # è¿ç»­åŠ¨ä½œçš„å‡å€¼
        # å°†å¯¹æ•°æ ‡å‡†å·®è®¾ä¸ºå¯å­¦ä¹ çš„å‚æ•°ï¼Œè€Œä¸æ˜¯ç½‘ç»œè¾“å‡ºï¼Œå¯ä»¥å¢åŠ ç¨³å®šæ€§
        self.log_std_param = torch.nn.Parameter(torch.zeros(1, CONTINUOUS_DIM) * -0.5)
        self.discrete_head = Linear(mlp_output_dim, TOTAL_DISCRETE_LOGITS)  # ç¦»æ•£åŠ¨ä½œçš„ logits

        # --- [æ–°å¢] åº”ç”¨åˆå§‹åŒ– ---
        self.apply(init_weights)  # å¯¹æ‰€æœ‰å­æ¨¡å—åº”ç”¨é€šç”¨åˆå§‹åŒ–

        # --- [æ–°å¢] å¯¹è¾“å‡ºå±‚è¿›è¡Œç‰¹æ®Šåˆå§‹åŒ– ---
        # è¿™æ ·åšæ˜¯ä¸ºäº†åœ¨è®­ç»ƒå¼€å§‹æ—¶æœ‰æ›´ç¨³å®šã€æ›´å…·æ¢ç´¢æ€§çš„ç­–ç•¥
        init_range = 3e-3
        self.mu_head.weight.data.uniform_(-init_range, init_range)
        self.mu_head.bias.data.fill_(0)
        self.discrete_head.weight.data.uniform_(-init_range, init_range)
        self.discrete_head.bias.data.fill_(0)
        # --- åˆå§‹åŒ–ç»“æŸ ---

        # --- ç²¾ç»†åŒ–ä¼˜åŒ–å™¨è®¾ç½® (ä¸å˜) ---
        # å¯ä»¥ä¸ºç½‘ç»œçš„ä¸åŒéƒ¨åˆ†è®¾ç½®ä¸åŒçš„è¶…å‚æ•°ï¼ˆå¦‚æƒé‡è¡°å‡ï¼‰
        attention_weight_decay = 1e-3
        default_weight_decay = self.weight_decay
        attention_params, other_params = [], []
        # å°†å‚æ•°åˆ†ä¸ºä¸æ³¨æ„åŠ›ç›¸å…³çš„å’Œå…¶ä»–ä¸¤ç»„
        for name, param in self.named_parameters():
            if not param.requires_grad:
                continue
            if any(key in name.lower() for key in ['attention', 'attn', 'layernorm']):
                attention_params.append(param)
            else:
                other_params.append(param)
        param_groups = [{'params': attention_params}, {'params': other_params}]
        self.optim = torch.optim.Adam(param_groups, lr=ACTOR_PARA.lr)
        self.actor_scheduler = lr_scheduler.LinearLR(
            self.optim,
            start_factor=1.0,
            end_factor=AGENTPARA.mini_lr / ACTOR_PARA.lr,
            total_iters=AGENTPARA.MAX_EXE_NUM
        )
        self.to(ACTOR_PARA.device)
        # print(f"--- [DEBUG] Actor_GRU Initialized (No Pooling Bottleneck) ---")

    def forward(self, obs, hidden_state):
        """
        å‰å‘ä¼ æ’­æ–¹æ³•ã€‚
        :param obs: çŠ¶æ€è§‚æµ‹å€¼ï¼Œå½¢çŠ¶ä¸º (B, D) æˆ– (B, S, D)
        :param hidden_state: GRU çš„éšè—çŠ¶æ€
        :return: åŒ…å«åŠ¨ä½œåˆ†å¸ƒçš„å­—å…¸å’Œæ–°çš„ GRU éšè—çŠ¶æ€
        """
        obs_tensor = check(obs).to(**ACTOR_PARA.tpdv)
        # æ£€æŸ¥è¾“å…¥æ˜¯å¦ä¸ºåºåˆ—ã€‚å¦‚æœä¸æ˜¯ï¼ˆä¾‹å¦‚ï¼Œå•æ­¥æ¨ç†ï¼‰ï¼Œåˆ™å¢åŠ ä¸€ä¸ªé•¿åº¦ä¸º1çš„åºåˆ—ç»´åº¦ã€‚
        is_sequence = obs_tensor.dim() == 3
        if not is_sequence:
            obs_tensor = obs_tensor.unsqueeze(1)  # (B, D) -> (B, 1, D)
        B, S, D = obs_tensor.shape  # B:æ‰¹å¤§å°, S:åºåˆ—é•¿åº¦, D:ç‰¹å¾ç»´åº¦

        # --- é˜¶æ®µä¸€ï¼šç‰¹å¾äº¤äº’ ---
        # 1. å‡†å¤‡ç‰¹å¾ token: (B, S, D) -> (B*S, D, 1)
        feat_tokens = obs_tensor.contiguous().view(B * S, D, 1)

        # <<< ADDED BACK 6/6, part 1 >>>: åº”ç”¨åµŒå…¥å±‚
        # 2. ç‰¹å¾åµŒå…¥: (B*S, D, 1) -> (B*S, D, embedding_dim)
        token_embeds = self.feature_embed(feat_tokens)

        # 3. è‡ªæ³¨æ„åŠ›è®¡ç®—
        # query, key, value éƒ½æ˜¯ token_embedsï¼Œè¿›è¡Œè‡ªæ³¨æ„åŠ›è®¡ç®—
        attn_out, _ = self.attention(token_embeds, token_embeds, token_embeds)

        # 4. æ®‹å·®è¿æ¥ï¼šå°†æ³¨æ„åŠ›è¾“å‡ºä¸åŸå§‹åµŒå…¥ç›¸åŠ ï¼Œæœ‰åŠ©äºæ¢¯åº¦ä¼ æ’­
        token_context = token_embeds + attn_out  # å½¢çŠ¶: (B*S, D, embedding_dim)

        # <<< MODIFIED 6/6, part 2 >>>: é‡å¡‘ä»¥ä¿ç•™æ‰€æœ‰ä¿¡æ¯
        # 5. å±•å¹³ä¸Šä¸‹æ–‡å‘é‡: (B*S, D, embedding_dim) -> (B*S, D * embedding_dim)
        flattened_context = token_context.contiguous().view(B * S, -1)

        # 6. æ¢å¤åºåˆ—ç»“æ„: (B*S, D * embedding_dim) -> (B, S, D * embedding_dim)
        contextualized_sequence = flattened_context.view(B, S, -1)

        # --- é˜¶æ®µäºŒï¼šæ—¶åºå»ºæ¨¡ ---
        # 7. GRUå¤„ç†ï¼Œç°åœ¨è¾“å…¥çš„æ˜¯ç»è¿‡ç‰¹å¾äº¤äº’å’Œå±•å¹³åçš„å®Œæ•´åºåˆ—
        gru_out, new_hidden = self.gru(contextualized_sequence, hidden_state)

        # --- é˜¶æ®µä¸‰ï¼šå†³ç­– (ä¸å˜) ---
        # 8. MLPåŠ å·¥
        mlp_output = self.shared_network(gru_out)
        final_features = mlp_output
        # å¦‚æœåŸå§‹è¾“å…¥æ˜¯å•æ­¥ï¼Œåˆ™ç§»é™¤åºåˆ—ç»´åº¦
        if not is_sequence:
            final_features = final_features.squeeze(1)

        # 9. åŠ¨ä½œå¤´å’Œåˆ†å¸ƒåˆ›å»º
        mu = self.mu_head(final_features)
        all_disc_logits = self.discrete_head(final_features)

        # (åç»­åˆ›å»ºåˆ†å¸ƒçš„é€»è¾‘ä¸ MLP ç‰ˆæœ¬å®Œå…¨ç›¸åŒ)
        split_sizes = list(DISCRETE_DIMS.values())
        logits_parts = torch.split(all_disc_logits, split_sizes, dim=-1)
        trigger_logits, salvo_size_logits, intra_interval_logits, num_groups_logits, inter_interval_logits = logits_parts

        has_flares_info = obs_tensor[..., 7]
        mask = (has_flares_info == 0)
        trigger_logits_masked = trigger_logits.clone()
        if torch.any(mask):
            if mask.dim() < trigger_logits_masked.dim():
                mask = mask.unsqueeze(-1)
            trigger_logits_masked[mask] = torch.finfo(torch.float32).min

        log_std = torch.clamp(self.log_std_param, self.log_std_min, self.log_std_max)
        std = torch.exp(log_std).expand_as(mu)
        continuous_base_dist = Normal(mu, std)

        trigger_dist = Bernoulli(logits=trigger_logits_masked.squeeze(-1))
        salvo_size_dist = Categorical(logits=salvo_size_logits)
        intra_interval_dist = Categorical(logits=intra_interval_logits)
        num_groups_dist = Categorical(logits=num_groups_logits)
        inter_interval_dist = Categorical(logits=inter_interval_logits)

        distributions = {
            'continuous': continuous_base_dist,
            'trigger': trigger_dist,
            'salvo_size': salvo_size_dist,
            'intra_interval': intra_interval_dist,
            'num_groups': num_groups_dist,
            'inter_interval': inter_interval_dist
        }
        return distributions, new_hidden


class Critic_GRU(Module):
    """
    Critic ç½‘ç»œ (ä»·å€¼ç½‘ç»œ) - é‡‡ç”¨ä¸ Actor ç›¸ä¼¼çš„ GRU ç»“æ„ï¼Œä½†ä¸ºäº†ç®€åŒ–ï¼Œè¿™é‡Œæ²¡æœ‰ä½¿ç”¨ Attentionã€‚
    è¿™æ˜¯ä¸€ä¸ª GRU -> MLP -> Value Head çš„ç»“æ„ã€‚
    """

    def __init__(self, dropout_rate=0.05, weight_decay=1e-4):
        """åˆå§‹åŒ– Critic ç½‘ç»œçš„æ¨¡å—ã€æƒé‡å’Œä¼˜åŒ–å™¨ã€‚"""
        super(Critic_GRU, self).__init__()
        # --- åŸºç¡€å‚æ•°å®šä¹‰ ---
        self.input_dim = CRITIC_PARA.input_dim  # D
        self.output_dim = CRITIC_PARA.output_dim
        self.rnn_hidden_size = RNN_HIDDEN_SIZE
        self.weight_decay = weight_decay

        # --- æ¨¡å—å®šä¹‰ ---
        # 1. GRU æ—¶åºå»ºæ¨¡å±‚
        # è¾“å…¥ç»´åº¦æ˜¯åŸå§‹çŠ¶æ€ç»´åº¦ D (self.input_dim)
        self.gru = GRU(self.input_dim, self.rnn_hidden_size, batch_first=True)

        # 2. MLPéª¨å¹²ç½‘ç»œ (ä¸å˜)
        layers_dims = [self.rnn_hidden_size] + CRITIC_PARA.model_layer_dim
        self.network_base = Sequential()
        for i in range(len(layers_dims) - 1):
            self.network_base.add_module(f'fc_{i}', Linear(layers_dims[i], layers_dims[i + 1]))
            # self.network_base.add_module(f'LayerNorm_{i}', LayerNorm(layers_dims[i + 1]))
            self.network_base.add_module(f'LeakyReLU_{i}', LeakyReLU())

        # 3. è¾“å‡ºå¤´ (Value Head) (ä¸å˜)
        base_output_dim = CRITIC_PARA.model_layer_dim[-1]
        self.fc_out = Linear(base_output_dim, self.output_dim)

        # --- [æ–°å¢] åº”ç”¨åˆå§‹åŒ– ---
        self.apply(init_weights)  # å¯¹æ‰€æœ‰å­æ¨¡å—åº”ç”¨é€šç”¨åˆå§‹åŒ–

        # --- [æ–°å¢] å¯¹è¾“å‡ºå±‚è¿›è¡Œç‰¹æ®Šåˆå§‹åŒ– ---
        # è¿™æ ·åšæ˜¯ä¸ºäº†åœ¨è®­ç»ƒå¼€å§‹æ—¶æœ‰æ›´ç¨³å®šçš„ä»·å€¼ä¼°è®¡
        init_range = 3e-3
        self.fc_out.weight.data.uniform_(-init_range, init_range)
        self.fc_out.bias.data.fill_(0)
        # --- åˆå§‹åŒ–ç»“æŸ ---

        # ä¼˜åŒ–å™¨è®¾ç½®
        attention_weight_decay = 1e-3
        default_weight_decay = self.weight_decay
        attention_params, other_params = [], []
        for name, param in self.named_parameters():
            if not param.requires_grad:
                continue
            if any(key in name.lower() for key in ['attention', 'attn', 'layernorm']):
                attention_params.append(param)
            else:
                other_params.append(param)
        param_groups = [{'params': attention_params}, {'params': other_params}]
        self.optim = torch.optim.Adam(param_groups, lr=CRITIC_PARA.lr)
        self.critic_scheduler = lr_scheduler.LinearLR(
            self.optim,
            start_factor=1.0,
            end_factor=AGENTPARA.mini_lr / CRITIC_PARA.lr,
            total_iters=AGENTPARA.MAX_EXE_NUM
        )
        self.to(CRITIC_PARA.device)
        # print(f"--- [DEBUG] Critic_GRU Initialized (No Pooling Bottleneck) ---")

    def forward(self, obs, hidden_state):
        """
        å‰å‘ä¼ æ’­æ–¹æ³•ã€‚
        :param obs: çŠ¶æ€è§‚æµ‹å€¼ï¼Œå½¢çŠ¶ä¸º (B, D) æˆ– (B, S, D)
        :param hidden_state: GRU çš„éšè—çŠ¶æ€
        :return: çŠ¶æ€ä»·å€¼å’Œæ–°çš„ GRU éšè—çŠ¶æ€
        """
        obs_tensor = check(obs).to(**CRITIC_PARA.tpdv)
        is_sequence = obs_tensor.dim() == 3
        if not is_sequence:
            obs_tensor = obs_tensor.unsqueeze(1)

        # --- é˜¶æ®µä¸€ï¼šæ—¶åºå»ºæ¨¡ ---
        # ç›´æ¥å°†åŸå§‹çŠ¶æ€åºåˆ—é€å…¥GRU
        gru_out, new_hidden = self.gru(obs_tensor, hidden_state)

        # --- é˜¶æ®µäºŒï¼šä»·å€¼è¯„ä¼° (ä¸å˜) ---
        # MLPåŠ å·¥
        base_features_sequence = self.network_base(gru_out)
        # è¾“å‡ºä»·å€¼
        value = self.fc_out(base_features_sequence)

        # å¦‚æœæ˜¯å•æ­¥æ¨ç†ï¼Œç§»é™¤åºåˆ—ç»´åº¦
        if not is_sequence:
            value = value.squeeze(1)

        return value, new_hidden


# ==============================================================================
# PPO Agent ä¸»ç±»
# ==============================================================================

class PPO_continuous(object):
    """
    PPO æ™ºèƒ½ä½“ä¸»ç±»ã€‚
    é€šè¿‡ `use_rnn` æ ‡å¿—æ¥å†³å®šæ˜¯ä½¿ç”¨ MLP æ¨¡å‹è¿˜æ˜¯ GRU+Attention æ¨¡å‹ã€‚
    """

    def __init__(self, load_able: bool, model_dir_path: str = None, use_rnn: bool = False):
        """
        åˆå§‹åŒ–PPOæ™ºèƒ½ä½“ã€‚
        :param load_able: æ˜¯å¦åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ã€‚
        :param model_dir_path: é¢„è®­ç»ƒæ¨¡å‹çš„æ–‡ä»¶å¤¹è·¯å¾„ã€‚
        :param use_rnn: æ˜¯å¦ä½¿ç”¨åŸºäº GRU+Attention çš„æ¨¡å‹ã€‚
        """
        super(PPO_continuous, self).__init__()
        # æ ¹æ® use_rnn æ ‡å¿—ï¼Œå®ä¾‹åŒ–å¯¹åº”çš„ Actor å’Œ Critic ç½‘ç»œ
        self.use_rnn = use_rnn
        if self.use_rnn:
            print("--- åˆå§‹åŒ– PPO Agent (ä½¿ç”¨ [Attention -> GRU -> MLP] æ¨¡å‹) ---")
            # å®ä¾‹åŒ–æ–°çš„ Actor å’Œ Criticï¼Œå¹¶ä¼ å…¥æ­£åˆ™åŒ–å‚æ•°
            self.Actor = Actor_GRU(dropout_rate=0.1, weight_decay=1e-4)
            self.Critic = Critic_GRU(dropout_rate=0.1, weight_decay=1e-4)
        else:
            print("--- åˆå§‹åŒ– PPO Agent (ä½¿ç”¨ MLP æ¨¡å‹) ---")
            self.Actor = Actor()
            self.Critic = Critic()
        # å®ä¾‹åŒ–ç»éªŒå›æ”¾æ± ï¼Œå¹¶å‘ŠçŸ¥å®ƒæ˜¯å¦éœ€è¦å¤„ç† RNN éšè—çŠ¶æ€
        self.buffer = Buffer(use_rnn=self.use_rnn)
        # ä»é…ç½®ä¸­åŠ è½½ PPO ç®—æ³•çš„è¶…å‚æ•°
        self.gamma = AGENTPARA.gamma  # æŠ˜æ‰£å› å­
        self.gae_lambda = AGENTPARA.lamda  # GAE çš„ lambda å‚æ•°
        self.ppo_epoch = AGENTPARA.ppo_epoch  # æ¯æ¬¡å­¦ä¹ æ—¶ï¼Œæ•°æ®è¢«é‡å¤ä½¿ç”¨çš„æ¬¡æ•°
        self.total_steps = 0
        # ä¸ºæœ¬æ¬¡è®­ç»ƒåˆ›å»ºä¸€ä¸ªå¸¦æ—¶é—´æˆ³çš„å­˜æ¡£æ–‡ä»¶å¤¹å
        self.training_start_time = time.strftime("PPO_ATT_GRU_%Y-%m-%d_%H-%M-%S")  # <<< ä¿®æ”¹ >>> æ›´æ–°å­˜æ¡£æ–‡ä»¶å¤¹åç§°
        self.base_save_dir = "../../save/save_evade_fuza"
        # æ‹¼æ¥æˆå®Œæ•´çš„å­˜æ¡£è·¯å¾„
        self.run_save_dir = os.path.join(self.base_save_dir, self.training_start_time)
        # å¦‚æœéœ€è¦åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        if load_able:
            if model_dir_path:
                print(f"--- æ­£åœ¨ä»æŒ‡å®šæ–‡ä»¶å¤¹åŠ è½½æ¨¡å‹: {model_dir_path} ---")
                self.load_models_from_directory(model_dir_path)
            else:
                print("--- æœªæŒ‡å®šæ¨¡å‹æ–‡ä»¶å¤¹ï¼Œå°è¯•ä»é»˜è®¤æ–‡ä»¶å¤¹ 'test' åŠ è½½ ---")
                self.load_models_from_directory("../../test/test_evade")

    def load_models_from_directory(self, directory_path: str):
        """ä»æŒ‡å®šç›®å½•åŠ è½½ Actor å’Œ Critic æ¨¡å‹çš„æƒé‡ã€‚"""
        # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨ä¸”ä¸ºæ–‡ä»¶å¤¹
        if not os.path.isdir(directory_path):
            print(f"[é”™è¯¯] æ¨¡å‹åŠ è½½å¤±è´¥ï¼šæä¾›çš„è·¯å¾„ '{directory_path}' ä¸æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„æ–‡ä»¶å¤¹ã€‚")
            return
        files = os.listdir(directory_path)
        # ä¼˜å…ˆå°è¯•åŠ è½½å¸¦æœ‰å‰ç¼€çš„æ¨¡å‹æ–‡ä»¶ (ä¾‹å¦‚ "1000_Actor.pkl")
        actor_files_with_prefix = [f for f in files if f.endswith("_Actor.pkl")]
        if len(actor_files_with_prefix) > 0:
            actor_filename = actor_files_with_prefix[0]
            prefix = actor_filename.replace("_Actor.pkl", "")
            critic_filename = f"{prefix}_Critic.pkl"
            print(f"  - æ£€æµ‹åˆ°å‰ç¼€ '{prefix}'ï¼Œå‡†å¤‡åŠ è½½æ¨¡å‹...")
            if critic_filename in files:
                actor_full_path = os.path.join(directory_path, actor_filename)
                critic_full_path = os.path.join(directory_path, critic_filename)
                try:
                    # åŠ è½½æƒé‡åˆ°æ¨¡å‹
                    self.Actor.load_state_dict(torch.load(actor_full_path, map_location=ACTOR_PARA.device))
                    print(f"    - æˆåŠŸåŠ è½½ Actor: {actor_full_path}")
                    self.Critic.load_state_dict(torch.load(critic_full_path, map_location=CRITIC_PARA.device))
                    print(f"    - æˆåŠŸåŠ è½½ Critic: {critic_full_path}")
                    return
                except Exception as e:
                    print(f"    - [é”™è¯¯] åŠ è½½å¸¦å‰ç¼€çš„æ¨¡å‹æ—¶å¤±è´¥: {e}")
            else:
                print(f"    - [è­¦å‘Š] æ‰¾åˆ°äº† '{actor_filename}' ä½†æœªæ‰¾åˆ°å¯¹åº”çš„ '{critic_filename}'ã€‚")
        # å¦‚æœæ²¡æœ‰å¸¦å‰ç¼€çš„æ–‡ä»¶ï¼Œåˆ™å°è¯•åŠ è½½é»˜è®¤çš„ "Actor.pkl" å’Œ "Critic.pkl"
        if "Actor.pkl" in files and "Critic.pkl" in files:
            print("  - æ£€æµ‹åˆ°æ— å‰ç¼€æ ¼å¼ï¼Œå‡†å¤‡åŠ è½½ 'Actor.pkl' å’Œ 'Critic.pkl'...")
            actor_full_path = os.path.join(directory_path, "Actor.pkl")
            critic_full_path = os.path.join(directory_path, "Critic.pkl")
            try:
                self.Actor.load_state_dict(torch.load(actor_full_path, map_location=ACTOR_PARA.device))
                print(f"    - æˆåŠŸåŠ è½½ Actor: {actor_full_path}")
                self.Critic.load_state_dict(torch.load(critic_full_path, map_location=CRITIC_PARA.device))
                print(f"    - æˆåŠŸåŠ è½½ Critic: {critic_full_path}")
                return
            except Exception as e:
                print(f"    - [é”™è¯¯] åŠ è½½æ— å‰ç¼€æ¨¡å‹æ—¶å¤±è´¥: {e}")
        print(f"[é”™è¯¯] æ¨¡å‹åŠ è½½å¤±è´¥ï¼šåœ¨æ–‡ä»¶å¤¹ '{directory_path}' ä¸­æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„ Actor/Critic æ¨¡å‹å¯¹ã€‚")

    def scale_action(self, action_cont_tanh):
        """å°† tanh è¾“å‡ºçš„è¿ç»­åŠ¨ä½œ (-1, 1) ç¼©æ”¾åˆ°ç¯å¢ƒå®šä¹‰çš„å®é™…èŒƒå›´ã€‚"""
        lows = torch.tensor([ACTION_RANGES[k]['low'] for k in CONTINUOUS_ACTION_KEYS], **ACTOR_PARA.tpdv)
        highs = torch.tensor([ACTION_RANGES[k]['high'] for k in CONTINUOUS_ACTION_KEYS], **ACTOR_PARA.tpdv)
        # çº¿æ€§æ’å€¼å…¬å¼: low + (tanh_out + 1) / 2 * (high - low)
        scaled_action = lows + (action_cont_tanh + 1.0) * 0.5 * (highs - lows)
        return scaled_action

    def get_initial_hidden_states(self, batch_size=1):
        """ä¸º GRU æ¨¡å‹ç”Ÿæˆåˆå§‹çš„é›¶éšè—çŠ¶æ€ã€‚"""
        if not self.use_rnn:
            return None, None
        # GRU éšè—çŠ¶æ€çš„å½¢çŠ¶æ˜¯ (num_layers, batch_size, hidden_size)
        actor_hidden = torch.zeros((1, batch_size, self.Actor.rnn_hidden_size), device=ACTOR_PARA.device)
        critic_hidden = torch.zeros((1, batch_size, self.Critic.rnn_hidden_size), device=CRITIC_PARA.device)
        return actor_hidden, critic_hidden

    def store_experience(self, state, action, probs, value, reward, done, actor_hidden=None, critic_hidden=None):
        """å°†å•æ­¥ç»éªŒå­˜å‚¨åˆ° Buffer ä¸­ã€‚"""
        # æ£€æŸ¥ log_prob æ˜¯å¦åŒ…å«æ— æ•ˆå€¼ï¼ˆNaN æˆ– Infï¼‰
        if not np.all(np.isfinite(probs)):
            raise ValueError("åœ¨ log_prob ä¸­æ£€æµ‹åˆ° NaN/Infï¼")
        # å¦‚æœä½¿ç”¨ RNNï¼Œå¿…é¡»æä¾›éšè—çŠ¶æ€
        if self.use_rnn and (actor_hidden is None or critic_hidden is None):
            raise ValueError("ä½¿ç”¨ RNN æ¨¡å‹æ—¶å¿…é¡»å­˜å‚¨éšè—çŠ¶æ€ï¼")
        self.buffer.store_transition(state, value, action, probs, reward, done, actor_hidden, critic_hidden)

    def choose_action(self, state, actor_hidden, critic_hidden, deterministic=False):
        """
        æ ¹æ®å½“å‰çŠ¶æ€é€‰æ‹©åŠ¨ä½œã€‚
        :param state: å½“å‰çŠ¶æ€è§‚æµ‹å€¼ã€‚
        :param actor_hidden: Actor GRU çš„éšè—çŠ¶æ€ã€‚
        :param critic_hidden: Critic GRU çš„éšè—çŠ¶æ€ã€‚
        :param deterministic: å¦‚æœä¸º Trueï¼Œåˆ™é€‰æ‹©æœ€å¯èƒ½çš„åŠ¨ä½œï¼ˆç”¨äºè¯„ä¼°ï¼‰ï¼Œå¦åˆ™è¿›è¡Œéšæœºé‡‡æ ·ï¼ˆç”¨äºè®­ç»ƒï¼‰ã€‚
        :return: (ç¯å¢ƒåŠ¨ä½œ, å­˜å‚¨åŠ¨ä½œ, å¯¹æ•°æ¦‚ç‡, çŠ¶æ€ä»·å€¼, æ–°çš„actoréšè—çŠ¶æ€, æ–°çš„criticéšè—çŠ¶æ€)
        """
        state_tensor = torch.as_tensor(state, dtype=torch.float32, device=ACTOR_PARA.device)
        # æ£€æŸ¥è¾“å…¥æ˜¯å¦æ˜¯æ‰¹å¤„ç†æ•°æ®
        is_batch = state_tensor.dim() > 1
        if not is_batch:
            state_tensor = state_tensor.unsqueeze(0)

        with torch.no_grad():  # åœ¨æ­¤å—ä¸­ä¸è®¡ç®—æ¢¯åº¦ï¼Œä»¥æé«˜æ€§èƒ½
            if self.use_rnn:
                # GRU æ¨¡å‹éœ€è¦ä¼ å…¥éšè—çŠ¶æ€
                value, new_critic_hidden = self.Critic(state_tensor, critic_hidden)
                dists, new_actor_hidden = self.Actor(state_tensor, actor_hidden)
            else:
                # MLP æ¨¡å‹ä¸éœ€è¦éšè—çŠ¶æ€
                value = self.Critic(state_tensor)
                dists = self.Actor(state_tensor)
                new_critic_hidden, new_actor_hidden = None, None

            # ä»è¿ç»­åŠ¨ä½œåˆ†å¸ƒä¸­é‡‡æ ·
            continuous_base_dist = dists['continuous']
            # u æ˜¯æ­£æ€åˆ†å¸ƒçš„åŸå§‹æ ·æœ¬
            u = continuous_base_dist.mean if deterministic else continuous_base_dist.rsample()
            # tanh(u) æ˜¯ä¸ºäº†é™åˆ¶èŒƒå›´å¹¶å¼•å…¥éçº¿æ€§ï¼Œè¿™æ˜¯ SAC å’Œ TQC ç­‰ç®—æ³•ä¸­å¸¸ç”¨çš„æŠ€å·§
            action_cont_tanh = torch.tanh(u)
            # è®¡ç®—è¿ç»­åŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡
            log_prob_cont = continuous_base_dist.log_prob(u).sum(dim=-1)

            # ä»æ‰€æœ‰ç¦»æ•£åŠ¨ä½œåˆ†å¸ƒä¸­é‡‡æ ·
            sampled_actions_dict = {}
            for key, dist in dists.items():
                if key == 'continuous': continue
                if deterministic:  # ç¡®å®šæ€§æ¨¡å¼
                    if isinstance(dist, Categorical):  # åˆ†ç±»åˆ†å¸ƒï¼šå–æ¦‚ç‡æœ€å¤§çš„ç±»åˆ«
                        sampled_actions_dict[key] = torch.argmax(dist.probs, dim=-1)
                    else:  # ä¼¯åŠªåˆ©åˆ†å¸ƒï¼šå–æ¦‚ç‡å¤§äº0.5çš„
                        sampled_actions_dict[key] = (dist.probs > 0.5).float()
                else:  # éšæœºæ¨¡å¼
                    sampled_actions_dict[key] = dist.sample()

            # è®¡ç®—æ€»çš„å¯¹æ•°æ¦‚ç‡
            log_prob_disc = sum(dists[key].log_prob(act) for key, act in sampled_actions_dict.items())
            total_log_prob = log_prob_cont + log_prob_disc

            # å‡†å¤‡è¦å­˜å‚¨åœ¨ Buffer ä¸­çš„åŠ¨ä½œï¼ˆè¿ç»­éƒ¨åˆ†æ˜¯åŸå§‹æ ·æœ¬ uï¼Œç¦»æ•£éƒ¨åˆ†æ˜¯ç±»åˆ«ç´¢å¼•ï¼‰
            action_disc_to_store = torch.stack(list(sampled_actions_dict.values()), dim=-1).float()
            action_to_store = torch.cat([u, action_disc_to_store], dim=-1)

            # å‡†å¤‡è¦å‘é€ç»™ç¯å¢ƒçš„åŠ¨ä½œï¼ˆè¿ç»­éƒ¨åˆ†æ˜¯ç¼©æ”¾åçš„åŠ¨ä½œï¼Œç¦»æ•£éƒ¨åˆ†æ˜¯ç±»åˆ«ç´¢å¼•ï¼‰
            env_action_cont = self.scale_action(action_cont_tanh)
            final_env_action_tensor = torch.cat([env_action_cont, action_disc_to_store], dim=-1)

            # å°†æ‰€æœ‰ç»“æœè½¬æ¢ä¸º numpy æ•°ç»„
            value_np = value.cpu().numpy()
            action_to_store_np = action_to_store.cpu().numpy()
            log_prob_to_store_np = total_log_prob.cpu().numpy()
            final_env_action_np = final_env_action_tensor.cpu().numpy()

            # å¦‚æœè¾“å…¥ä¸æ˜¯æ‰¹å¤„ç†ï¼Œåˆ™ç§»é™¤æ‰¹æ¬¡ç»´åº¦
            if not is_batch:
                final_env_action_np = final_env_action_np[0]
                action_to_store_np = action_to_store_np[0]
                log_prob_to_store_np = log_prob_to_store_np[0]
                value_np = value_np[0]

        return final_env_action_np, action_to_store_np, log_prob_to_store_np, value_np, new_actor_hidden, new_critic_hidden

    def cal_gae(self, states, values, actions, probs, rewards, dones):
        """è®¡ç®—å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡ (Generalized Advantage Estimation, GAE)ã€‚"""
        advantage = np.zeros(len(rewards), dtype=np.float32)
        gae = 0
        # ä»åå‘å‰éå†è½¨è¿¹
        for t in reversed(range(len(rewards))):
            # è·å–ä¸‹ä¸€ä¸ªçŠ¶æ€çš„ä»·å€¼ï¼Œå¦‚æœæ˜¯æœ€åä¸€æ­¥ï¼Œåˆ™ä»·å€¼ä¸º 0
            next_value = values[t + 1] if t < len(rewards) - 1 else 0.0
            # done_mask ç”¨äºåœ¨å›åˆç»“æŸæ—¶åˆ‡æ–­ä»·å€¼çš„ä¼ æ’­
            done_mask = 1.0 - int(dones[t])
            # è®¡ç®— TD-error (delta)
            delta = rewards[t] + self.gamma * next_value * done_mask - values[t]
            # é€’å½’è®¡ç®— GAE
            gae = delta + self.gamma * self.gae_lambda * done_mask * gae
            advantage[t] = gae
        return advantage

    def learn(self):
        """
        æ‰§è¡Œ PPO çš„å­¦ä¹ å’Œæ›´æ–°æ­¥éª¤ã€‚å·²é€‚é… RNN æ¨¡å¼ã€‚
        """
        # å¦‚æœ Buffer ä¸­çš„æ•°æ®ä¸è¶³ä¸€ä¸ªæ‰¹æ¬¡ï¼Œåˆ™è·³è¿‡å­¦ä¹ 
        if self.buffer.get_buffer_size() < BUFFERPARA.BATCH_SIZE:
            print(
                f"  [Info] Buffer size ({self.buffer.get_buffer_size()}) < batch size ({BUFFERPARA.BATCH_SIZE}). Skipping.")
            return None
        # 1. æ•°æ®å‡†å¤‡
        states, values, actions, old_probs, rewards, dones, _, __ = self.buffer.get_all_data()
        advantages = self.cal_gae(states, values, actions, old_probs, rewards, dones)
        values = np.squeeze(values)  # ç§»é™¤å¤šä½™ç»´åº¦ï¼Œæ¯”å¦‚ (N,1) â†’ (N,) ç¡®ä¿ values æ˜¯ä¸€ç»´æ•°ç»„ï¼Œä¸ advantages å¯¹é½

        # returns (å³ G_tï¼Œç›®æ ‡ä»·å€¼) æ˜¯ä¼˜åŠ¿å‡½æ•° + çŠ¶æ€ä»·å€¼   ä¿è¯ returns ä¸ values å½¢çŠ¶ä¸€è‡´
        # returns (å³ G_t) æ˜¯ä¼˜åŠ¿å‡½æ•°çš„ unbiased estimator
        returns = advantages + values
        # ç”¨äºè®°å½•è®­ç»ƒè¿‡ç¨‹ä¸­çš„å„ç§æŸå¤±å’ŒæŒ‡æ ‡
        train_info = {'critic_loss': [], 'actor_loss': [], 'dist_entropy': [], 'entropy_cont': [], 'adv_targ': [],
                      'ratio': []}

        # 2. PPO è®­ç»ƒå¾ªç¯
        for _ in range(self.ppo_epoch):
            # æ ¹æ®æ˜¯å¦ä½¿ç”¨ RNNï¼Œé€‰æ‹©ä¸åŒçš„æ‰¹æ¬¡ç”Ÿæˆå™¨
            if self.use_rnn:
                # ä¸º RNN ç”Ÿæˆè¿ç»­çš„åºåˆ—æ‰¹æ¬¡
                batch_generator = self.buffer.generate_sequence_batches(
                    SEQUENCE_LENGTH, BUFFERPARA.BATCH_SIZE, advantages, returns
                )
            else:
                # ä¸º MLP ç”Ÿæˆéšæœºçš„æ‰¹æ¬¡ç´¢å¼•
                batch_generator = self.buffer.generate_batches()

            for batch_data in batch_generator:
                # 3. æ‰¹æ¬¡æ•°æ®å¤„ç†
                if self.use_rnn:
                    # è§£åŒ… RNN çš„åºåˆ—æ‰¹æ¬¡æ•°æ®
                    state, action_batch, old_prob, advantage, return_, initial_actor_h, initial_critic_h = batch_data
                    # å°† numpy æ•°ç»„è½¬æ¢ä¸º tensor å¹¶ç§»åŠ¨åˆ°è®¾å¤‡
                    state = check(state).to(**ACTOR_PARA.tpdv)
                    action_batch = check(action_batch).to(**ACTOR_PARA.tpdv)
                    old_prob = check(old_prob).to(**ACTOR_PARA.tpdv)
                    advantage = check(advantage).to(**ACTOR_PARA.tpdv)
                    return_ = check(return_).to(**CRITIC_PARA.tpdv)
                    initial_actor_h = check(initial_actor_h).to(**ACTOR_PARA.tpdv)
                    initial_critic_h = check(initial_critic_h).to(**CRITIC_PARA.tpdv)
                else:
                    # å¤„ç† MLP çš„æ‰¹æ¬¡ç´¢å¼•
                    batch_indices = batch_data
                    state = check(states[batch_indices]).to(**ACTOR_PARA.tpdv)
                    action_batch = check(actions[batch_indices]).to(**ACTOR_PARA.tpdv)
                    old_prob = check(old_probs[batch_indices]).to(**ACTOR_PARA.tpdv).view(-1)
                    advantage = check(advantages[batch_indices]).to(**ACTOR_PARA.tpdv).view(-1, 1)
                    return_ = check(returns[batch_indices]).to(**CRITIC_PARA.tpdv).view(-1, 1)

                # ä»æ‰¹æ¬¡åŠ¨ä½œä¸­è§£æå‡ºè¿ç»­å’Œç¦»æ•£éƒ¨åˆ† ğŸ’¥ ä¸å†éœ€è¦åˆ‡ç‰‡å‡ºæœ€åä¸€ä¸ªæ—¶é—´æ­¥ï¼Œç›´æ¥ä½¿ç”¨å®Œæ•´çš„åºåˆ—
                u_from_buffer = action_batch[..., :CONTINUOUS_DIM]
                discrete_actions_from_buffer = {
                    'trigger': action_batch[..., CONTINUOUS_DIM],
                    'salvo_size': action_batch[..., CONTINUOUS_DIM + 1].long(),  # ç±»åˆ«ç´¢å¼•éœ€è¦æ˜¯ long ç±»å‹
                    'intra_interval': action_batch[..., CONTINUOUS_DIM + 2].long(),
                    'num_groups': action_batch[..., CONTINUOUS_DIM + 3].long(),
                    'inter_interval': action_batch[..., CONTINUOUS_DIM + 4].long(),
                }

                # 4. Actor (ç­–ç•¥) ç½‘ç»œè®­ç»ƒ
                if self.use_rnn:
                    new_dists, _ = self.Actor(state, initial_actor_h)
                else:
                    new_dists = self.Actor(state)

                # è®¡ç®—æ–°ç­–ç•¥ä¸‹ï¼Œæ—§åŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡
                # ç°åœ¨ u_from_buffer å’Œ new_dists['continuous'] çš„å½¢çŠ¶éƒ½æ˜¯ (B, 4)ï¼Œå¯ä»¥åŒ¹é…äº†
                # è®¡ç®— log_probï¼Œæ‰€æœ‰å¼ é‡éƒ½æ˜¯åºåˆ—ï¼Œç»´åº¦å¯ä»¥æ­£ç¡®åŒ¹é…
                new_log_prob_cont = new_dists['continuous'].log_prob(u_from_buffer).sum(dim=-1)
                new_log_prob_disc = sum(
                    new_dists[key].log_prob(discrete_actions_from_buffer[key])
                    for key in discrete_actions_from_buffer
                )
                new_prob = new_log_prob_cont + new_log_prob_disc

                # è®¡ç®—ç­–ç•¥çš„ç†µï¼Œä»¥é¼“åŠ±æ¢ç´¢
                entropy_cont = new_dists['continuous'].entropy().sum(dim=-1)
                total_entropy = (entropy_cont + sum(
                    dist.entropy() for key, dist in new_dists.items() if key != 'continuous'
                )).mean()

                # è®¡ç®—æ–°æ—§ç­–ç•¥çš„æ¯”ç‡ (importance sampling ratio)
                # ä½¿ç”¨å®Œæ•´çš„åºåˆ— old_prob å’Œ advantage
                # æ³¨æ„ï¼šéœ€è¦ç¡®ä¿å®ƒä»¬çš„å½¢çŠ¶ä¸ new_prob åŒ¹é…
                log_ratio = new_prob - old_prob
                ratio = torch.exp(torch.clamp(log_ratio, -20.0, 20.0))  # clamp é˜²æ­¢æ•°å€¼æº¢å‡º

                # ç¡®ä¿ advantage å’Œ ratio çš„å½¢çŠ¶å¯ä»¥å¹¿æ’­  advantageå¯èƒ½æ˜¯ (B, S, 1) æˆ– (B, S)ï¼Œéœ€è¦ä¸ ratio (B, S) å¯¹é½
                advantage_squeezed = advantage.squeeze(-1) if advantage.dim() > ratio.dim() else advantage

                # PPO çš„è£å‰ªç›®æ ‡å‡½æ•°
                surr1 = ratio * advantage_squeezed
                surr2 = torch.clamp(ratio, 1.0 - AGENTPARA.epsilon, 1.0 + AGENTPARA.epsilon) * advantage_squeezed

                # Actor çš„æŸå¤±æ˜¯è£å‰ªåçš„ç›®æ ‡å‡½æ•°çš„è´Ÿå€¼ï¼ŒåŠ ä¸Šç†µçš„æ­£åˆ™é¡¹
                actor_loss = -torch.min(surr1, surr2).mean() - AGENTPARA.entropy * total_entropy

                # åå‘ä¼ æ’­å’Œä¼˜åŒ–
                self.Actor.optim.zero_grad()
                actor_loss.backward()
                torch.nn.utils.clip_grad_norm_(self.Actor.parameters(), max_norm=1.0)  # æ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
                self.Actor.optim.step()

                # 5. Critic (ä»·å€¼) ç½‘ç»œè®­ç»ƒ
                if self.use_rnn:
                    # å¯¹äº RNN æ¨¡å‹ï¼ŒCritic è¾“å‡ºçš„æ˜¯èšåˆåçš„å•ä¸€ä»·å€¼
                    # new_value shape: (B, 1)
                    new_value, _ = self.Critic(state, initial_critic_h)
                else:
                    # new_value shape: (B, 1)
                    new_value = self.Critic(state)

                # ç¡®ä¿ç›®æ ‡å€¼ `return_` å’Œç½‘ç»œè¾“å‡º `new_value` çš„ç»´åº¦ä¸€è‡´ï¼Œä»¥é¿å… PyTorch çš„å¹¿æ’­è­¦å‘Šã€‚
                if new_value.dim() > return_.dim():
                    return_ = return_.unsqueeze(-1)

                # Critic çš„æŸå¤±æ˜¯é¢„æµ‹å€¼å’ŒçœŸå®å›æŠ¥ï¼ˆreturnsï¼‰ä¹‹é—´çš„å‡æ–¹è¯¯å·®
                critic_loss = torch.nn.functional.mse_loss(new_value, return_)

                # åå‘ä¼ æ’­å’Œä¼˜åŒ–
                self.Critic.optim.zero_grad()
                critic_loss.backward()
                torch.nn.utils.clip_grad_norm_(self.Critic.parameters(), max_norm=1.0)
                self.Critic.optim.step()

                # è®°å½•è¯¥æ‰¹æ¬¡çš„è®­ç»ƒä¿¡æ¯
                train_info['critic_loss'].append(critic_loss.item())
                train_info['actor_loss'].append(actor_loss.item())
                train_info['dist_entropy'].append(total_entropy.item())
                train_info['entropy_cont'].append(entropy_cont.mean().item())
                train_info['adv_targ'].append(advantage.mean().item())
                train_info['ratio'].append(ratio.mean().item())

        # 6. æ›´æ–°å­¦ä¹ ç‡ã€æ¸…ç†å’Œè¿”å›
        # # åœ¨å®Œæˆæ•´ä¸ªæ‰¹æ¬¡æ•°æ®çš„å­¦ä¹ åï¼Œè®©è°ƒåº¦å™¨æ­¥è¿›ä¸€æ¬¡
        # self.Actor.actor_scheduler.step()
        # self.Critic.critic_scheduler.step()

        # 6. æ¸…ç†å’Œè¿”å›
        self.buffer.clear_memory()  # å®Œæˆä¸€è½®å­¦ä¹ åæ¸…ç©º Buffer
        # è®¡ç®—æ•´ä¸ª epoch çš„å¹³å‡æŒ‡æ ‡
        for key in train_info:
            train_info[key] = np.mean(train_info[key])

        self.save()  # ä¿å­˜æ¨¡å‹
        return train_info

    def prep_training_rl(self):
        """å°† Actor å’Œ Critic ç½‘ç»œè®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼ã€‚"""
        self.Actor.train()
        self.Critic.train()

    def prep_eval_rl(self):
        """å°† Actor å’Œ Critic ç½‘ç»œè®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼ˆä¾‹å¦‚ï¼Œå…³é—­ Dropoutï¼‰ã€‚"""
        self.Actor.eval()
        self.Critic.eval()

    def save(self, prefix=""):
        """ä¿å­˜ Actor å’Œ Critic æ¨¡å‹çš„æƒé‡ã€‚"""
        try:
            # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
            os.makedirs(self.run_save_dir, exist_ok=True)
            print(f"æ¨¡å‹å°†è¢«ä¿å­˜è‡³: {self.run_save_dir}")
        except Exception as e:
            print(f"åˆ›å»ºæ¨¡å‹æ–‡ä»¶å¤¹ {self.run_save_dir} å¤±è´¥: {e}")
            return
        # åˆ†åˆ«ä¿å­˜ Actor å’Œ Critic
        for net in ['Actor', 'Critic']:
            try:
                # æ„é€ æ–‡ä»¶å
                filename = f"{prefix}_{net}.pkl" if prefix else f"{net}.pkl"
                full_path = os.path.join(self.run_save_dir, filename)
                # ä¿å­˜æ¨¡å‹çš„çŠ¶æ€å­—å…¸
                torch.save(getattr(self, net).state_dict(), full_path)
                print(f"  - {filename} ä¿å­˜æˆåŠŸã€‚")
            except Exception as e:
                print(f"  - ä¿å­˜æ¨¡å‹ {net} åˆ° {full_path} æ—¶å‘ç”Ÿé”™è¯¯: {e}")